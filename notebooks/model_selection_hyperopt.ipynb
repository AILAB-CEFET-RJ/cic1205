{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#005097; border:0' role=\"tab\" aria-controls=\"home\"><center>Hyperparameter Optimization</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "In machine learning, a **hyperparameter** is a value that controls the learning process of a model, but it's not directly learned from the data itself. Hyperparameters are essentially dials or knobs that must be adjusted before running the learning algorithm. They define the learning algorithm's overall behavior and influence the resulting model's complexity and capacity. In contrast to the model **parameters** that are learned during training, the hyperparameters are set before the training of a model begins. \n",
    "\n",
    "Some examples of hyperparameters are given below.\n",
    "\n",
    "* the learning rate in logistic regression and in linear regression,\n",
    "* the height of trees in decision tree learning,\n",
    "* the value of $k$ in $k$-NN,\n",
    "* the value of $k$ in $k$-means,\n",
    "* the values of $\\epsilon$ and $\\operatorname{minPoints}$ in DBSCAN,\n",
    "* the regularization term in linear regression,\n",
    "* the polynomial degree in polynomial regression.\n",
    "\n",
    "\n",
    "Hyperparameters can be considered as configurations of a learning algorithm. In general, the ideal settings for the algorithm to generate a suitable model for one dataset may not be the same for another dataset.\n",
    "\n",
    "*Hyperparameter optimization* (aka *hyperparameter tuning*) is the procedure of defining appropriate values for the hyperparameters of a given learning algorithm. The general approach is to generate several combinations of hyperparameter values. Then, a model is trained for each combinatation. Finally, the best combination (according to some evaluation measure applied on the candidate models) is chosen.\n",
    "\n",
    "When the learning algorithm generates an estimator (either a classifier or a regressor), there are two main strategies to *automatically* tune its hyperparameters: *Grid Search* and *Random Search*. Both strategies generate several hyperparameter combinations. Then, for each combination, an estimator is trained a model (on the training set) and evaluateed (on the validation set). The difference between these two strategies is in the way the several combinations are generated, as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid search** first defines a grid of hyperparameter combinations. The amount of hyperparameters defines the number of dimensions in the grid. Then, each combination (a point in the grid) ​​is used to produce a model.\n",
    "\n",
    "The image below ([source](https://www.researchgate.net/publication/271771573_TuPAQ_An_Efficient_Planner_for_Large-scale_Predictive_Analytic_Queries/figures?lo=1)) illustrates the GS procedure for a learning algorithm with two hyperparameters. The star represents the combination of hyperparameters that gives the best model. as measured in the validation set. The red dot in picture on the left represents the first combination test by GS. The picture on the center represents the second combination. The picture on the right illustrates all the tested combinations in the 2-dimensional grid.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://www.researchgate.net/profile/Michael_Jordan13/publication/271771573/figure/fig4/AS:668513593217027@1536397469229/Illustration-of-naive-grid-search-for-a-single-model-family-with-two-hyperparameters_W640.jpg\">\n",
    "</p>\n",
    "\n",
    "**Random search** selects random combinations of hyperparameter values ​​(within preconfigured ranges of values for each hyperparameter) to train and evaluate the candidate models. Instead of trying all possible combinations of values, RS performs a pre-defined number of iterations, testing a random combination of hyperparameters ​​in each iteration.\n",
    "\n",
    "Both GS and RS can be very computationally expensive. For example, searching for 20 different values ​​for each of 4 hyperparameters would require 160,000 k-fold cross-validation runs. If $k = 10$ then 1,600,000 model adjustments and 1,600,000 evaluations should be performed\n",
    "\n",
    "The image below ([source](https://community.alteryx.com/t5/Data-Science/Hyperparameter-Tuning-Black-Magic/ba-p/449289)) illustrates the difference between GS and RS. Think of a learning algorithm with just two hyperparameters. This way, each combination of its hyperparameters is a pair of numbers. Suppose one of these hyperparameters (x-axis) has more influence than the other (y-axis) on the predictive performance of the generated models. The plot on the left shows several of these pairs organized in a grid. In total, there are thirty combination in this grid. The picture on the right shows other combinations of pairs; this times these combinations where randomly selected. Notice that RS has the potential to explore more promising combinations than GS.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://pvsmt99345.i.lithium.com/t5/image/serverpage/image-id/74545i97245FDAA10376E9/image-size/large?v=1.0&px=999\">\n",
    "</p>\n",
    "\n",
    "In Scikit-Learn, the classes [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) implement GS and RS, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class GridSearchCV\n",
    "\n",
    "Scikit-Learn provides the class `GridSearchCV` to perform grid search.\n",
    "\n",
    "The following code block presents an example of using the `GridSearchCV` class to find the ideal polynomial model for a data set. In this example, a three-dimensional grid of hyperparameters is explored:\n",
    "\n",
    "* the polynomial degree,\n",
    "* a boolean indicator that indicates whether the linear coefficient should be adjusted,\n",
    "* an indicator (boolean) indicating whether the data should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best cross-validation score: 0.96\n",
      "\n",
      "Classification Report on Test Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "svc = SVC()\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nClassification Report on Test Set:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class RandomizedSearchCV\n",
    "\n",
    "This allows you to explicitly control the number of parameter combinations that are attempted. The number of search iterations is defined based on time or resources. Scikit Learn offers the RandomizedSearchCV function for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "LinearSVC:\n",
      "Best cv accuracy: 0.9666666666666667\n",
      "Test set score:   0.9666666666666667\n",
      "Best parameters:  {'C': 0.703}\n",
      "\n",
      "time to run RandomizedSearchCV: 2.061651249998249\n",
      "########\n",
      "Fitting 3 folds for each of 1099 candidates, totalling 3297 fits\n",
      "LinearSVC:\n",
      "Best cv accuracy: 0.9666666666666667\n",
      "Test set score:   0.9666666666666667\n",
      "Best parameters:  {'C': 0.441}\n",
      "\n",
      "time to run GridSearchCV: 0.7237497499445453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def linear_SVC(x, y, param, kfold):\n",
    "    param_grid = {'C':param}\n",
    "    k = StratifiedKFold(n_splits=kfold)\n",
    "    grid = GridSearchCV(LinearSVC(dual=False), param_grid=param_grid, cv=k, n_jobs=4, verbose=1)\n",
    "\n",
    "    return grid.fit(x, y)\n",
    "\n",
    "def Linear_SVC_Rand(x, y, param, kfold, n):\n",
    "    param_grid = {'C':param}\n",
    "    k = StratifiedKFold(n_splits=kfold)\n",
    "    randsearch = RandomizedSearchCV(LinearSVC(dual=False), param_distributions=param_grid, cv=k, n_jobs=4,\n",
    "                                    verbose=1, n_iter=n)\n",
    "\n",
    "    return randsearch.fit(x, y)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "start = timer()\n",
    "param = [i/1000 for i in range(1,1000)]\n",
    "param1 = [i for i in range(1,101)]\n",
    "param.extend(param1)\n",
    "\n",
    "#progress = progressbar.bar.ProgressBar()\n",
    "clf = Linear_SVC_Rand(x=x_train, y=y_train, param=param, kfold=3, n=100)\n",
    "\n",
    "print('LinearSVC:')\n",
    "print('Best cv accuracy: {}' .format(clf.best_score_))\n",
    "print('Test set score:   {}' .format(clf.score(x_test, y_test)))\n",
    "print('Best parameters:  {}' .format(clf.best_params_))\n",
    "print()\n",
    "\n",
    "duration = timer() - start\n",
    "print('time to run RandomizedSearchCV: {}' .format(duration))\n",
    "\n",
    "\n",
    "print('########')\n",
    "\n",
    "#high C means more chance of overfitting\n",
    "\n",
    "start = timer()\n",
    "param = [i/1000 for i in range(1,1000)]\n",
    "param1 = [i for i in range(1,101)]\n",
    "param.extend(param1)\n",
    "\n",
    "clf = linear_SVC(x=x_train, y=y_train, param=param, kfold=3)\n",
    "\n",
    "print('LinearSVC:')\n",
    "print('Best cv accuracy: {}' .format(clf.best_score_))\n",
    "print('Test set score:   {}' .format(clf.score(x_test, y_test)))\n",
    "print('Best parameters:  {}' .format(clf.best_params_))\n",
    "print()\n",
    "\n",
    "duration = timer() - start\n",
    "print('time to run GridSearchCV: {}' .format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna\n",
    "\n",
    "Optuna is an open-source hyperparameter optimization framework that automates the process of tuning machine learning models by efficiently searching for the best hyperparameters. It supports both classical machine learning models and deep learning models, and its goal is to maximize or minimize a given **objective function**, such as model performance, by optimizing hyperparameters.\n",
    "\n",
    "In the Optuna optimization framework, a **study** is the main object that represents an optimization session. It encompasses the entire optimization process — including the definition of the objective function, the trials (each set of hyperparameters tested), and the history and results of those trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna selects the **sampling strategy** based on the **study configuration** and **hyperparameter types**. The choice depends on the search space, the number of trials, and whether prior knowledge is available.\n",
    "\n",
    "## Default Strategy: Tree-structured Parzen Estimator (TPE)\n",
    "- If no specific sampler is specified, Optuna **automatically** uses **TPE (Tree-structured Parzen Estimator)**.\n",
    "- TPE **models the probability distribution** of good and bad hyperparameter choices and chooses new trials accordingly.\n",
    "- Best for **non-convex search spaces** where grid/random search fails.\n",
    "\n",
    "Example of instantiating a study that uses TPE:\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "study = optuna.create_study(direction=\"maximize\")  # Uses TPE by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicitly Specifying a Sampler\n",
    "You can override the default and choose a specific sampler:\n",
    "\n",
    "| Sampler                  | When to Use?                                           | Example                                      |\n",
    "|--------------------------|------------------------------------------------------|----------------------------------------------|\n",
    "| **TPE (default)**        | Works well in most cases, adaptive Bayesian optimization | `optuna.samplers.TPESampler()`               |\n",
    "| **Random Search**        | Good for benchmarking, large search spaces          | `optuna.samplers.RandomSampler()`           |\n",
    "| **Grid Search**          | If you have limited trials and want exhaustive search | `optuna.samplers.GridSampler(search_space)` |\n",
    "| **CMA-ES**              | Good for continuous spaces, often used in reinforcement learning | `optuna.samplers.CmaEsSampler()` |\n",
    "\n",
    "Example of choosing a sampler:\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.RandomSampler()  # Choose random search\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Optuna Adapts to the Problem\n",
    "Optuna adjusts the search strategy based on:\n",
    "\n",
    "- Discrete vs. Continuous Parameters. If parameters are categorical (trial.suggest_categorical), TPE handles them well. If parameters are continuous (trial.suggest_float), CMA-ES or TPE works better.\n",
    "- Log-scaled vs. Linear Search Space. If log=True is used (e.g., learning_rate), Optuna adjusts the sampling distribution accordingly.\n",
    "- Early Pruning and Convergence. If trials are pruned early, TPE focuses on exploiting promising areas rather than random exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing the Sampling Strategy\n",
    "You can combine samplers or switch strategies mid-experiment:\n",
    "\n",
    "```python\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=10)  # Use random search for first 10 trials\n",
    "study = optuna.create_study(sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/aug_train.csv')\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(columns=['id', 'Response'])\n",
    "y = data['Response']\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n",
    "numerical_features = X.columns.difference(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-10 05:27:13,275] A new study created in memory with name: no-name-74b16ce9-45fa-4e35-ace1-193434f61b06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-10 05:27:26,358] Trial 0 finished with value: 0.3414115867516296 and parameters: {'n_estimators': 69, 'max_depth': 14, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:27:58,037] Trial 1 finished with value: 0.00027945307348764586 and parameters: {'n_estimators': 239, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:28:30,901] Trial 2 finished with value: 0.27134730746322805 and parameters: {'n_estimators': 191, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:29:12,136] Trial 3 finished with value: 0.06669601151876058 and parameters: {'n_estimators': 285, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:29:55,640] Trial 4 finished with value: 0.07699580221032808 and parameters: {'n_estimators': 295, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:30:11,876] Trial 5 finished with value: 0.29978941584161434 and parameters: {'n_estimators': 78, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:30:35,740] Trial 6 finished with value: 0.027620524825490492 and parameters: {'n_estimators': 185, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:30:51,978] Trial 7 finished with value: 0.0 and parameters: {'n_estimators': 136, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:31:28,946] Trial 8 finished with value: 0.09759159330955391 and parameters: {'n_estimators': 240, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:31:43,134] Trial 9 finished with value: 0.0018708469474963475 and parameters: {'n_estimators': 118, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.3414115867516296.\n",
      "[I 2025-04-10 05:32:01,149] A new study created in memory with name: no-name-c8ba26b2-0872-4ca6-96e0-cd45f39be0a0\n",
      "[I 2025-04-10 05:32:29,150] Trial 0 finished with value: 0.33587881542257625 and parameters: {'n_estimators': 151, 'max_depth': 16, 'min_samples_split': 9, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.33587881542257625.\n",
      "[I 2025-04-10 05:33:15,780] Trial 1 finished with value: 0.3731010723961914 and parameters: {'n_estimators': 253, 'max_depth': 18, 'min_samples_split': 6, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.3731010723961914.\n",
      "[I 2025-04-10 05:33:56,030] Trial 2 finished with value: 0.1079761693631978 and parameters: {'n_estimators': 262, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.3731010723961914.\n",
      "[I 2025-04-10 05:34:34,606] Trial 3 finished with value: 0.32961123664312475 and parameters: {'n_estimators': 217, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.3731010723961914.\n",
      "[I 2025-04-10 05:35:03,212] Trial 4 finished with value: 0.34316103355495836 and parameters: {'n_estimators': 156, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.3731010723961914.\n",
      "[I 2025-04-10 05:35:42,207] Trial 5 finished with value: 0.40691059414949987 and parameters: {'n_estimators': 198, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.40691059414949987.\n",
      "[I 2025-04-10 05:36:28,749] Trial 6 finished with value: 0.31242671749634304 and parameters: {'n_estimators': 269, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 10}. Best is trial 5 with value: 0.40691059414949987.\n",
      "[I 2025-04-10 05:36:46,831] Trial 7 finished with value: 0.0 and parameters: {'n_estimators': 177, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 5 with value: 0.40691059414949987.\n",
      "[I 2025-04-10 05:37:01,479] Trial 8 finished with value: 0.17134656404530588 and parameters: {'n_estimators': 93, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 6}. Best is trial 5 with value: 0.40691059414949987.\n",
      "[I 2025-04-10 05:37:35,237] Trial 9 finished with value: 0.08689771773788123 and parameters: {'n_estimators': 234, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 7}. Best is trial 5 with value: 0.40691059414949987.\n",
      "[I 2025-04-10 05:38:30,646] A new study created in memory with name: no-name-37b9b0fa-e347-4927-9603-8e320f8d3ea9\n",
      "[I 2025-04-10 05:38:57,535] Trial 0 finished with value: 0.0 and parameters: {'n_estimators': 292, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-04-10 05:39:09,097] Trial 1 finished with value: 0.407406237168574 and parameters: {'n_estimators': 62, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:39:47,823] Trial 2 finished with value: 0.10090574770101512 and parameters: {'n_estimators': 257, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:39:58,755] Trial 3 finished with value: 0.3963361655215408 and parameters: {'n_estimators': 59, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:40:44,319] Trial 4 finished with value: 0.36411448734911306 and parameters: {'n_estimators': 238, 'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:41:30,394] Trial 5 finished with value: 0.3982403295845239 and parameters: {'n_estimators': 249, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:42:13,284] Trial 6 finished with value: 0.1518176083059646 and parameters: {'n_estimators': 273, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:42:35,335] Trial 7 finished with value: 0.3821898677186142 and parameters: {'n_estimators': 112, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:43:16,039] Trial 8 finished with value: 0.33875864723788696 and parameters: {'n_estimators': 240, 'max_depth': 16, 'min_samples_split': 7, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:43:50,235] Trial 9 finished with value: 0.11672949904718517 and parameters: {'n_estimators': 210, 'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.407406237168574.\n",
      "[I 2025-04-10 05:44:05,179] A new study created in memory with name: no-name-92e62d08-b9a4-4689-86ff-3a05215380e1\n",
      "[I 2025-04-10 05:44:52,305] Trial 0 finished with value: 0.10220013849135696 and parameters: {'n_estimators': 291, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.10220013849135696.\n",
      "[I 2025-04-10 05:45:37,762] Trial 1 finished with value: 0.34705540511991756 and parameters: {'n_estimators': 249, 'max_depth': 16, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.34705540511991756.\n",
      "[I 2025-04-10 05:46:13,271] Trial 2 finished with value: 0.23409855082864073 and parameters: {'n_estimators': 204, 'max_depth': 13, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.34705540511991756.\n",
      "[I 2025-04-10 05:46:41,792] Trial 3 finished with value: 0.32777329994976484 and parameters: {'n_estimators': 155, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.34705540511991756.\n",
      "[I 2025-04-10 05:47:24,489] Trial 4 finished with value: 0.3816542109383128 and parameters: {'n_estimators': 233, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 9}. Best is trial 4 with value: 0.3816542109383128.\n",
      "[I 2025-04-10 05:48:12,926] Trial 5 finished with value: 0.38941843334286935 and parameters: {'n_estimators': 263, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 7}. Best is trial 5 with value: 0.38941843334286935.\n",
      "[I 2025-04-10 05:48:58,898] Trial 6 finished with value: 0.2982272565499166 and parameters: {'n_estimators': 255, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 6}. Best is trial 5 with value: 0.38941843334286935.\n",
      "[I 2025-04-10 05:49:15,310] Trial 7 finished with value: 0.0 and parameters: {'n_estimators': 252, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 5 with value: 0.38941843334286935.\n",
      "[I 2025-04-10 05:49:39,448] Trial 8 finished with value: 0.0 and parameters: {'n_estimators': 270, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.38941843334286935.\n",
      "[I 2025-04-10 05:50:06,033] Trial 9 finished with value: 0.09125152015696852 and parameters: {'n_estimators': 178, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.38941843334286935.\n",
      "[I 2025-04-10 05:51:07,421] A new study created in memory with name: no-name-3be9ecc4-e48a-48a5-9199-749e583763f6\n",
      "[I 2025-04-10 05:51:18,993] Trial 0 finished with value: 0.07329519960783262 and parameters: {'n_estimators': 64, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.07329519960783262.\n",
      "[I 2025-04-10 05:51:39,037] Trial 1 finished with value: 0.00019963427156395654 and parameters: {'n_estimators': 173, 'max_depth': 7, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.07329519960783262.\n",
      "[I 2025-04-10 05:52:11,057] Trial 2 finished with value: 0.12415258861293638 and parameters: {'n_estimators': 199, 'max_depth': 11, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.12415258861293638.\n",
      "[I 2025-04-10 05:53:04,923] Trial 3 finished with value: 0.4068149924839147 and parameters: {'n_estimators': 270, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.4068149924839147.\n",
      "[I 2025-04-10 05:53:19,362] Trial 4 finished with value: 0.0 and parameters: {'n_estimators': 175, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.4068149924839147.\n",
      "[I 2025-04-10 05:53:37,876] Trial 5 finished with value: 0.37862898218399527 and parameters: {'n_estimators': 91, 'max_depth': 17, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.4068149924839147.\n",
      "[I 2025-04-10 05:54:15,820] Trial 6 finished with value: 0.19519135313925082 and parameters: {'n_estimators': 237, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.4068149924839147.\n",
      "[I 2025-04-10 05:55:06,938] Trial 7 finished with value: 0.37358486615686354 and parameters: {'n_estimators': 288, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.4068149924839147.\n",
      "[I 2025-04-10 05:55:36,097] Trial 8 finished with value: 0.4165561459937632 and parameters: {'n_estimators': 143, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.4165561459937632.\n",
      "[I 2025-04-10 05:56:11,635] Trial 9 finished with value: 0.20359520434902412 and parameters: {'n_estimators': 218, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.4165561459937632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested CV F1 scores: [0.33155859440677016, 0.414046590175334, 0.4091045823724055, 0.3943604413567634, 0.4289385637395141]\n",
      "Mean F1 score: 0.39560175441015744\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(), categorical_features)\n",
    "])\n",
    "\n",
    "N_TRIALS = 10\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_scores = []\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=inner_cv, scoring='f1', n_jobs=-1)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    # Run Optuna for current outer fold\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "    # Train model with best params on full inner training set\n",
    "    best_params = study.best_params\n",
    "    final_model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(**best_params, random_state=42))\n",
    "    ])\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    outer_scores.append(score)\n",
    "\n",
    "print(\"Nested CV F1 scores:\", outer_scores)\n",
    "print(\"Mean F1 score:\", np.mean(outer_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on test set: 0.4440074991997805\n",
      "CPU times: user 48.3 s, sys: 75 ms, total: 48.4 s\n",
      "Wall time: 47.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "best_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        # max_depth=best_params['max_depth'],\n",
    "        min_samples_split=best_params['min_samples_split'],\n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train and evaluate the model\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score on test set:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on test set: 0.43990114580993034\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "default_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "default_model.fit(X_train, y_train)\n",
    "y_pred = default_model.predict(X_test)\n",
    "# Evaluate performance\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score on test set:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
