{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#005097; border:0' role=\"tab\" aria-controls=\"home\"><center>Hyperparameter Optimization with Optuna</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna is an open-source hyperparameter optimization framework that automates the process of tuning machine learning models by efficiently searching for the best hyperparameters. It supports both classical machine learning models and deep learning models, and its goal is to maximize or minimize a given objective function, such as model performance, by optimizing hyperparameters. Here's a step-by-step overview of how the Optuna framework works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna selects the **sampling strategy** based on the **study configuration** and **hyperparameter types**. The choice depends on the search space, the number of trials, and whether prior knowledge is available.\n",
    "\n",
    "## 1️⃣ Default Strategy: Tree-structured Parzen Estimator (TPE)\n",
    "- If no specific sampler is specified, Optuna **automatically** uses **TPE (Tree-structured Parzen Estimator)**.\n",
    "- TPE **models the probability distribution** of good and bad hyperparameter choices and chooses new trials accordingly.\n",
    "- Best for **non-convex search spaces** where grid/random search fails.\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "study = optuna.create_study(direction=\"maximize\")  # Uses TPE by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Explicitly Specifying a Sampler\n",
    "You can override the default and choose a specific sampler:\n",
    "\n",
    "| Sampler                  | When to Use?                                           | Example                                      |\n",
    "|--------------------------|------------------------------------------------------|----------------------------------------------|\n",
    "| **TPE (default)**        | Works well in most cases, adaptive Bayesian optimization | `optuna.samplers.TPESampler()`               |\n",
    "| **Random Search**        | Good for benchmarking, large search spaces          | `optuna.samplers.RandomSampler()`           |\n",
    "| **Grid Search**          | If you have limited trials and want exhaustive search | `optuna.samplers.GridSampler(search_space)` |\n",
    "| **CMA-ES**              | Good for continuous spaces, often used in reinforcement learning | `optuna.samplers.CmaEsSampler()` |\n",
    "\n",
    "Example of choosing a sampler:\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "sampler = optuna.samplers.RandomSampler()  # Choose random search\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ How Optuna Adapts to the Problem\n",
    "Optuna adjusts the search strategy based on:\n",
    "\n",
    "- Discrete vs. Continuous Parameters. If parameters are categorical (trial.suggest_categorical), TPE handles them well. If parameters are continuous (trial.suggest_float), CMA-ES or TPE works better.\n",
    "- Log-scaled vs. Linear Search Space. If log=True is used (e.g., learning_rate), Optuna adjusts the sampling distribution accordingly.\n",
    "- Early Pruning and Convergence. If trials are pruned early, TPE focuses on exploiting promising areas rather than random exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Customizing the Sampling Strategy\n",
    "You can combine samplers or switch strategies mid-experiment:\n",
    "\n",
    "```python\n",
    "sampler = optuna.samplers.TPESampler(n_startup_trials=10)  # Use random search for first 10 trials\n",
    "study = optuna.create_study(sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/aug_train.csv')\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(columns=['id', 'Response'])\n",
    "y = data['Response']\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n",
    "numerical_features = X.columns.difference(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-31 12:30:22,756] A new study created in memory with name: no-name-2493e23e-ab8c-4e74-95de-c813b3ad4ccf\n",
      "[I 2025-01-31 12:32:03,574] Trial 0 finished with value: 0.3762416412829145 and parameters: {'n_estimators': 258, 'max_depth': 17, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.3762416412829145.\n",
      "[I 2025-01-31 12:33:28,627] Trial 1 finished with value: 0.10200389577138821 and parameters: {'n_estimators': 259, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.3762416412829145.\n",
      "[I 2025-01-31 12:34:27,637] Trial 2 finished with value: 0.15861685616807358 and parameters: {'n_estimators': 165, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.3762416412829145.\n",
      "[I 2025-01-31 12:34:53,619] Trial 3 finished with value: 0.3981416554622904 and parameters: {'n_estimators': 64, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.3981416554622904.\n",
      "[I 2025-01-31 12:35:42,659] Trial 4 finished with value: 0.2457285439080859 and parameters: {'n_estimators': 133, 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 3 with value: 0.3981416554622904.\n",
      "[I 2025-01-31 12:36:42,604] Trial 5 finished with value: 0.08489186193279877 and parameters: {'n_estimators': 190, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.3981416554622904.\n",
      "[I 2025-01-31 12:37:29,098] Trial 6 finished with value: 0.0 and parameters: {'n_estimators': 214, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.3981416554622904.\n",
      "[I 2025-01-31 12:39:17,116] Trial 7 finished with value: 0.430662786094293 and parameters: {'n_estimators': 253, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 7 with value: 0.430662786094293.\n",
      "[I 2025-01-31 12:40:22,686] Trial 8 finished with value: 0.3350716885178316 and parameters: {'n_estimators': 172, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 7 with value: 0.430662786094293.\n",
      "[I 2025-01-31 12:41:06,834] Trial 9 finished with value: 0.09947706538776711 and parameters: {'n_estimators': 129, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 7 with value: 0.430662786094293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 253, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}\n",
      "CPU times: user 4.68 s, sys: 916 ms, total: 5.6 s\n",
      "Wall time: 10min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import optuna\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(), categorical_features)\n",
    "])\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=inner_cv, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "N_TRIALS = 10\n",
    "\n",
    "# Nested cross-validation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 253, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on test set: 0.4454172119680491\n",
      "CPU times: user 1min 38s, sys: 505 ms, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "best_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        # max_depth=best_params['max_depth'],\n",
    "        min_samples_split=best_params['min_samples_split'],\n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train and evaluate the model\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score on test set:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on test set: 0.43990114580993034\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "default_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "default_model.fit(X_train, y_train)\n",
    "y_pred = default_model.predict(X_test)\n",
    "# Evaluate performance\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score on test set:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
