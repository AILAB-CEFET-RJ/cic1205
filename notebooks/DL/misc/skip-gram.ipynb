{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "1. Data Preparation: \n",
    "        - The corpus is tokenized into words, and a vocabulary is built.\n",
    "        - Context-target pairs are generated using a window size of 2.\n",
    "\n",
    "2. Skip-gram Model: The SkipGramModel class defines a neural network with an embedding layer to learn word vectors and an output layer to predict context words.\n",
    "\n",
    "3. Training Loop: The model is trained using the context-target pairs. For each target word, it predicts its context words using the embeddings and updates them to minimize the prediction loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation\n",
    "\n",
    "We'll start by preparing a small corpus and generating context-target pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"we are learning nlp\",\n",
    "    \"nlp is fun\",\n",
    "    \"we love deep learning\",\n",
    "    \"deep learning is powerful\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = Counter()\n",
    "for sentence in tokenized_corpus:\n",
    "    for word in sentence:\n",
    "        vocabulary[word] += 1\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Generate context-target pairs\n",
    "def generate_skipgram_pairs(tokenized_corpus, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        sentence_len = len(sentence)\n",
    "        for idx, word in enumerate(sentence):\n",
    "            for neighbor in range(max(idx - window_size, 0), min(idx + window_size + 1, sentence_len)):\n",
    "                if neighbor != idx:\n",
    "                    pairs.append((word, sentence[neighbor]))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define the Skip-gram Model\n",
    "We'll create a simple neural network with an embedding layer and a linear layer to predict context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, target_word):\n",
    "        # Get the embedding of the target word\n",
    "        word_embed = self.embeddings(target_word)\n",
    "        # Calculate scores for all words in the vocabulary\n",
    "        output = self.output_layer(word_embed)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Training the Model\n",
    "Next, we'll train the Skip-gram model using the context-target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 69.3557\n",
      "Epoch 20/100, Loss: 63.8174\n",
      "Epoch 30/100, Loss: 61.0445\n",
      "Epoch 40/100, Loss: 59.2488\n",
      "Epoch 50/100, Loss: 57.9731\n",
      "Epoch 60/100, Loss: 57.0197\n",
      "Epoch 70/100, Loss: 56.2831\n",
      "Epoch 80/100, Loss: 55.7019\n",
      "Epoch 90/100, Loss: 55.2374\n",
      "Epoch 100/100, Loss: 54.8633\n",
      "Word: we, Embedding: [ 1.409335   -0.20863475  0.44019634 -0.47989354 -1.0884212  -0.25774264\n",
      "  1.3875313  -1.5579122  -0.3796208   1.0906088 ]\n",
      "Word: are, Embedding: [-0.9366768   0.8951899   0.04885015  1.4833986  -0.12077451  0.4680458\n",
      " -1.2635247   0.3657455   1.430025   -0.02664759]\n",
      "Word: learning, Embedding: [-0.09689061 -0.2076085   1.3457878   0.6046952  -0.4199099  -1.0195935\n",
      "  0.66363126  2.0995212  -0.13935253 -0.45863977]\n",
      "Word: nlp, Embedding: [ 2.5507257e+00  1.9211942e+00 -6.9080037e-01  2.9292544e-03\n",
      " -6.4891368e-02  4.6898937e-01 -6.4207202e-01 -6.9626361e-01\n",
      "  2.0395450e-03  1.3152207e-01]\n",
      "Word: is, Embedding: [-0.52267057 -0.5367874   0.49218756  1.5211189  -0.28172144  1.0779531\n",
      "  1.1760077  -0.94329804  1.6207098   1.4793022 ]\n",
      "Word: fun, Embedding: [-0.41329712  1.7235372  -1.3995035  -0.5401651   0.00970962  0.91153574\n",
      "  1.5317317   1.8411955   1.8502471  -0.83760875]\n",
      "Word: love, Embedding: [ 0.16847324  1.0751365   1.2165533   1.6101631  -1.2328509  -0.9078426\n",
      " -0.39820182 -0.4762101   0.2075876  -0.6273112 ]\n",
      "Word: deep, Embedding: [-0.18279111  0.18110014  1.8483984  -0.94464546 -0.53538847 -1.0200539\n",
      " -1.9641075  -0.50588816 -1.5195715  -1.3995026 ]\n",
      "Word: powerful, Embedding: [-0.41578183  0.93567014 -0.02603095 -0.7312846   0.6609421   0.3881986\n",
      " -0.4225797   1.1253655   0.8572216   0.993237  ]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare data for training\n",
    "def prepare_data(pairs, word_to_idx):\n",
    "    inputs = [word_to_idx[target] for target, context in pairs]\n",
    "    targets = [word_to_idx[context] for target, context in pairs]\n",
    "    return torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "\n",
    "inputs, targets = prepare_data(pairs, word_to_idx)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for input_word, target_word in zip(inputs, targets):\n",
    "        input_word = input_word.unsqueeze(0)\n",
    "        target_word = target_word.unsqueeze(0)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_word)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target_word)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Display learned embeddings\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"Word: {word}, Embedding: {model.embeddings.weight[idx].detach().numpy()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
