{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Data Preparation: \n",
    "- The corpus is tokenized into words, and a vocabulary is built.\n",
    "- Context-target pairs are generated using a window size of 2.\n",
    "\n",
    "2. Skip-gram Model: The SkipGramModel class defines a neural network with an embedding layer to learn word vectors and an output layer to predict context words.\n",
    "\n",
    "3. Training Loop: The model is trained using the context-target pairs. For each target word, it predicts its context words using the embeddings and updates them to minimize the prediction loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation\n",
    "\n",
    "This step prepares a small corpus and generates context-target pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"we are learning nlp\",\n",
    "    \"nlp is fun\",\n",
    "    \"we love deep learning\",\n",
    "    \"deep learning is powerful\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = Counter()\n",
    "for sentence in tokenized_corpus:\n",
    "    for word in sentence:\n",
    "        vocabulary[word] += 1\n",
    "\n",
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# Generate context-target pairs\n",
    "def generate_skipgram_pairs(tokenized_corpus, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        sentence_len = len(sentence)\n",
    "        for idx, word in enumerate(sentence):\n",
    "            for neighbor in range(max(idx - window_size, 0), min(idx + window_size + 1, sentence_len)):\n",
    "                if neighbor != idx:\n",
    "                    pairs.append((word, sentence[neighbor]))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Skip-gram Model\n",
    "\n",
    "This step creates a simple neural network with an embedding layer and a linear layer to predict context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, target_word):\n",
    "        # Get the embedding of the target word\n",
    "        word_embed = self.embeddings(target_word)\n",
    "        # Calculate scores for all words in the vocabulary\n",
    "        output = self.output_layer(word_embed)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training the Model\n",
    "\n",
    "This step trains the Skip-gram model using the context-target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 70.2636\n",
      "Epoch 20/100, Loss: 65.7007\n",
      "Epoch 30/100, Loss: 63.5598\n",
      "Epoch 40/100, Loss: 62.0821\n",
      "Epoch 50/100, Loss: 60.8883\n",
      "Epoch 60/100, Loss: 59.8661\n",
      "Epoch 70/100, Loss: 58.9722\n",
      "Epoch 80/100, Loss: 58.1843\n",
      "Epoch 90/100, Loss: 57.4870\n",
      "Epoch 100/100, Loss: 56.8684\n",
      "Word: we, Embedding: [-0.69457585  0.03122669  1.0407579  -0.17249197 -0.36623123 -1.0612456\n",
      "  0.35565415  0.60919106 -0.66884327  0.57878953]\n",
      "Word: are, Embedding: [-0.8537248   0.83230984  1.4542491  -0.03443678 -1.3387407  -0.09828038\n",
      " -0.6228846   0.8793163   0.9466001   0.28804305]\n",
      "Word: learning, Embedding: [ 0.64245325 -0.40728158 -0.6611613  -0.70274705  1.3546456  -1.6175278\n",
      " -0.38449246 -0.3137371   0.51765305  0.869688  ]\n",
      "Word: nlp, Embedding: [-1.4155036   1.2556876  -0.45933115  0.7096862   0.6858076   0.09723409\n",
      " -2.1826375  -0.08341801  1.3511233   0.847898  ]\n",
      "Word: is, Embedding: [ 0.88156915 -2.7319615   1.063199   -0.93589664 -0.5708867   0.0468301\n",
      " -0.2517836   1.1327626   0.41310048  1.3730305 ]\n",
      "Word: fun, Embedding: [ 0.9505713  -0.23318668 -1.8090616   1.4848984  -1.2117361  -0.37324357\n",
      "  1.9159995  -0.46246704  0.97969246  1.5425541 ]\n",
      "Word: love, Embedding: [ 0.85447156  0.27521968 -0.2349427   0.7870341   0.04272343  0.5055801\n",
      " -0.8908543   2.42005     1.1162674  -0.5343571 ]\n",
      "Word: deep, Embedding: [ 0.71161157  1.2156732  -0.11703045  2.208433   -0.04177136 -0.6428976\n",
      " -1.3857166   1.0640879  -1.4226292  -0.49012008]\n",
      "Word: powerful, Embedding: [-0.7998196  -1.8009299   0.02951743  1.5293261  -1.4505349  -1.2445319\n",
      " -1.0412458  -0.99535346 -1.078333    2.6293724 ]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare data for training\n",
    "def prepare_data(pairs, word_to_idx):\n",
    "    inputs = [word_to_idx[target] for target, context in pairs]\n",
    "    targets = [word_to_idx[context] for target, context in pairs]\n",
    "    return torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "\n",
    "inputs, targets = prepare_data(pairs, word_to_idx)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for input_word, target_word in zip(inputs, targets):\n",
    "        input_word = input_word.unsqueeze(0)\n",
    "        target_word = target_word.unsqueeze(0)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_word)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target_word)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Display learned embeddings\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"Word: {word}, Embedding: {model.embeddings.weight[idx].detach().numpy()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
