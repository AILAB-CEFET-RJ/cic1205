{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdution\n",
    "\n",
    "Batch normalization is a technique that improves the training of deep neural networks by normalizing the inputs to each layer. It helps stabilize learning, allows for higher learning rates, and can reduce the sensitivity to the initial weights. Below is an example using the synthetic dataset we previously generated, where we implement batch normalization in a neural network.\n",
    "\n",
    "\n",
    "The following equations describe the process of Batch Normalization:\n",
    "\n",
    "### Compute the Mean\n",
    "\n",
    "For each feature (across the batch), the mean is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "- $x_i$ is the input feature of the $i$-th example in the mini-batch.\n",
    "- $m$ is the number of examples in the mini-batch.\n",
    "\n",
    "### Compute the Variance\n",
    "\n",
    "The variance for each feature (across the batch) is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "\\end{equation}\n",
    "\n",
    "### Normalize the Input\n",
    "\n",
    "Each input feature is normalized using the computed mean and variance:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "- $\\epsilon$ is a small constant added to the variance to prevent division by zero.\n",
    "\n",
    "### Scale and Shift\n",
    "\n",
    "Finally, the normalized input is scaled and shifted using learnable parameters $\\gamma$ and $\\beta$:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "- $\\gamma$ and $\\beta$ are learned during training and allow the model to adjust the normalization as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [4., 5., 6.],\n",
      "        [4., 5., 6.],\n",
      "        [4., 5., 6.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "\n",
      "Output Tensor after BatchNorm1d and ReLU:\n",
      "tensor([[1.8708e+00, 1.8708e+00, 1.8708e+00],\n",
      "        [4.2488e-08, 0.0000e+00, 0.0000e+00],\n",
      "        [4.2488e-08, 0.0000e+00, 0.0000e+00],\n",
      "        [4.2488e-08, 0.0000e+00, 0.0000e+00],\n",
      "        [4.2488e-08, 0.0000e+00, 0.0000e+00],\n",
      "        [4.2488e-08, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple example tensor to simulate a batch of data with 3 features\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                             [4.0, 5.0, 6.0],\n",
    "                             [4.0, 5.0, 6.0],\n",
    "                             [4.0, 5.0, 6.0],\n",
    "                             [4.0, 5.0, 6.0],\n",
    "                             [4.0, 5.0, 6.0],\n",
    "                             [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Define a neural network with a single hidden layer using BatchNorm1d\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 3)          # Linear layer\n",
    "        self.bn1 = nn.BatchNorm1d(3)        # BatchNorm1d for 3 features\n",
    "        self.relu = nn.ReLU()               # ReLU activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Apply batch normalization after the linear layer\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "model = SimpleNet()\n",
    "\n",
    "# Forward pass through the network\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(\"Input Tensor:\")\n",
    "print(input_tensor)\n",
    "print(\"\\nOutput Tensor after BatchNorm1d and ReLU:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "def create_datasets(batch_size):\n",
    "\n",
    "    # percentage of training set to use as validation\n",
    "    valid_size = 0.2\n",
    "\n",
    "    # convert data to torch.FloatTensor\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # choose the training and test datasets\n",
    "    train_data = datasets.MNIST(root='data', \n",
    "                                train=True,\n",
    "                                download=True, \n",
    "                                transform=transform)\n",
    "\n",
    "    test_data = datasets.MNIST(root='data',\n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transform)\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # load training data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "    # load validation data in batches\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=valid_sampler,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "    # load test data in batches\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=0)\n",
    "    \n",
    "    return train_loader, test_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "train_loader, test_loader, valid_loader = create_datasets(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/750], Loss: 0.4536\n",
      "Epoch [1/10], Step [200/750], Loss: 0.3959\n",
      "Epoch [1/10], Step [300/750], Loss: 0.1229\n",
      "Epoch [1/10], Step [400/750], Loss: 0.2238\n",
      "Epoch [1/10], Step [500/750], Loss: 0.1535\n",
      "Epoch [1/10], Step [600/750], Loss: 0.1737\n",
      "Epoch [1/10], Step [700/750], Loss: 0.2120\n",
      "Epoch [2/10], Step [100/750], Loss: 0.0279\n",
      "Epoch [2/10], Step [200/750], Loss: 0.1405\n",
      "Epoch [2/10], Step [300/750], Loss: 0.0140\n",
      "Epoch [2/10], Step [400/750], Loss: 0.0823\n",
      "Epoch [2/10], Step [500/750], Loss: 0.0457\n",
      "Epoch [2/10], Step [600/750], Loss: 0.0279\n",
      "Epoch [2/10], Step [700/750], Loss: 0.0694\n",
      "Epoch [3/10], Step [100/750], Loss: 0.0092\n",
      "Epoch [3/10], Step [200/750], Loss: 0.0478\n",
      "Epoch [3/10], Step [300/750], Loss: 0.0518\n",
      "Epoch [3/10], Step [400/750], Loss: 0.2893\n",
      "Epoch [3/10], Step [500/750], Loss: 0.0238\n",
      "Epoch [3/10], Step [600/750], Loss: 0.0460\n",
      "Epoch [3/10], Step [700/750], Loss: 0.1030\n",
      "Epoch [4/10], Step [100/750], Loss: 0.0536\n",
      "Epoch [4/10], Step [200/750], Loss: 0.0299\n",
      "Epoch [4/10], Step [300/750], Loss: 0.0231\n",
      "Epoch [4/10], Step [400/750], Loss: 0.0177\n",
      "Epoch [4/10], Step [500/750], Loss: 0.0093\n",
      "Epoch [4/10], Step [600/750], Loss: 0.0232\n",
      "Epoch [4/10], Step [700/750], Loss: 0.0675\n",
      "Epoch [5/10], Step [100/750], Loss: 0.0378\n",
      "Epoch [5/10], Step [200/750], Loss: 0.0171\n",
      "Epoch [5/10], Step [300/750], Loss: 0.0128\n",
      "Epoch [5/10], Step [400/750], Loss: 0.0729\n",
      "Epoch [5/10], Step [500/750], Loss: 0.0336\n",
      "Epoch [5/10], Step [600/750], Loss: 0.0374\n",
      "Epoch [5/10], Step [700/750], Loss: 0.0123\n",
      "Epoch [6/10], Step [100/750], Loss: 0.0074\n",
      "Epoch [6/10], Step [200/750], Loss: 0.0078\n",
      "Epoch [6/10], Step [300/750], Loss: 0.0399\n",
      "Epoch [6/10], Step [400/750], Loss: 0.0404\n",
      "Epoch [6/10], Step [500/750], Loss: 0.0124\n",
      "Epoch [6/10], Step [600/750], Loss: 0.0173\n",
      "Epoch [6/10], Step [700/750], Loss: 0.0170\n",
      "Epoch [7/10], Step [100/750], Loss: 0.0605\n",
      "Epoch [7/10], Step [200/750], Loss: 0.0017\n",
      "Epoch [7/10], Step [300/750], Loss: 0.0316\n",
      "Epoch [7/10], Step [400/750], Loss: 0.0275\n",
      "Epoch [7/10], Step [500/750], Loss: 0.0198\n",
      "Epoch [7/10], Step [600/750], Loss: 0.0090\n",
      "Epoch [7/10], Step [700/750], Loss: 0.0098\n",
      "Epoch [8/10], Step [100/750], Loss: 0.0150\n",
      "Epoch [8/10], Step [200/750], Loss: 0.0561\n",
      "Epoch [8/10], Step [300/750], Loss: 0.0616\n",
      "Epoch [8/10], Step [400/750], Loss: 0.0380\n",
      "Epoch [8/10], Step [500/750], Loss: 0.0111\n",
      "Epoch [8/10], Step [600/750], Loss: 0.0263\n",
      "Epoch [8/10], Step [700/750], Loss: 0.0149\n",
      "Epoch [9/10], Step [100/750], Loss: 0.0013\n",
      "Epoch [9/10], Step [200/750], Loss: 0.0518\n",
      "Epoch [9/10], Step [300/750], Loss: 0.0055\n",
      "Epoch [9/10], Step [400/750], Loss: 0.0015\n",
      "Epoch [9/10], Step [500/750], Loss: 0.0156\n",
      "Epoch [9/10], Step [600/750], Loss: 0.0149\n",
      "Epoch [9/10], Step [700/750], Loss: 0.0054\n",
      "Epoch [10/10], Step [100/750], Loss: 0.0207\n",
      "Epoch [10/10], Step [200/750], Loss: 0.0071\n",
      "Epoch [10/10], Step [300/750], Loss: 0.0044\n",
      "Epoch [10/10], Step [400/750], Loss: 0.0061\n",
      "Epoch [10/10], Step [500/750], Loss: 0.0197\n",
      "Epoch [10/10], Step [600/750], Loss: 0.0102\n",
      "Epoch [10/10], Step [700/750], Loss: 0.0481\n",
      "Accuracy of the model on the 10000 test images: 98.23%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Neural Network with Batch Normalization\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.BatchNorm1d(256),  # Batch Normalization\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),  # Batch Normalization\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),   # Batch Normalization\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input tensor\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = NeuralNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the Model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
