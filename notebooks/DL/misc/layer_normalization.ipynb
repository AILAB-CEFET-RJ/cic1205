{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization Equations\n",
    "\n",
    "Layer Normalization is a technique used to normalize the activations of a neural network layer across the features for each individual data sample. Layer Normalization normalizes the activations across features for each data sample independently. The mean and variance are computed per sample and per feature, ensuring that each feature has zero mean and unit variance. The normalized output is then scaled and shifted using learnable parameters, allowing the network to maintain expressiveness while stabilizing the training process.\n",
    "\n",
    "The key equations used in Layer Normalization are as follows:\n",
    "\n",
    "## 1. Mean Calculation\n",
    "\n",
    "For a given layer with input features $x_i$, the mean $\\mu$ of the features is computed as:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $H$ is the number of features in the layer.\n",
    "- $x_i$ is the $i$-th feature of the input.\n",
    "\n",
    "## 2. Variance Calculation\n",
    "\n",
    "The variance $\\sigma^2$ of the features is calculated as:\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\sigma^2$ is the variance of the features.\n",
    "\n",
    "## 3. Normalization\n",
    "\n",
    "The normalized output $\\hat{x}_i$ for each feature $x_i$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mu$ is the mean of the features.\n",
    "- $\\sigma^2$ is the variance of the features.\n",
    "- $\\epsilon$ is a small constant added for numerical stability (e.g., $\\epsilon = 1 \\times 10^{-5}$).\n",
    "\n",
    "## 4. Scaling and Shifting\n",
    "\n",
    "After normalization, the output is scaled and shifted using learnable parameters $\\gamma$ and $\\beta$:\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\gamma$ is the scale parameter.\n",
    "- $\\beta$ is the shift parameter.\n",
    "- $\\hat{x}_i$ is the normalized feature.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
