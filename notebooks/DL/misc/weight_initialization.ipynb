{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "# Weight Initialization Techniques for Deep Networks\n",
    "\n",
    "Weight initialization is crucial in deep neural networks as it influences convergence speed and overall performance. Proper initialization helps prevent issues such as vanishing or exploding gradients. Choosing the right initialization technique is critical for effective training, particularly as networks become deeper and more complex.\n",
    "\n",
    "Below are the main techniques:\n",
    "\n",
    "## 1. Zero Initialization\n",
    "**Method:** Initialize all weights to zero.  \n",
    "**Problem:** If all weights are initialized to zero, each neuron in every layer performs the same computation during forward and backward propagation, leading to symmetry. As a result, the network fails to learn effectively because all neurons in each layer update their weights identically.\n",
    "\n",
    "## 2. Random Initialization\n",
    "**Method:** Initialize weights randomly using a uniform or normal distribution.  \n",
    "**Goal:** Break symmetry so neurons learn different features.  \n",
    "**Problem:** If weights are too large, activations can become very large, leading to exploding gradients. If weights are too small, activations can become very small, leading to vanishing gradients, making learning difficult.\n",
    "\n",
    "## 3. Xavier Initialization (Glorot Initialization)\n",
    "**Method:** Initialize weights using a distribution with zero mean and variance $\\text{Var}(W) = \\frac{1}{n_{\\text{in}}}$ or $\\text{Var}(W) = \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}$, where $n_{\\text{in}}$ is the number of input units and $n_{\\text{out}}$ is the number of output units.  \n",
    "**Goal:** Ensure that the variance of the outputs of each layer is approximately the same as the variance of its inputs, helping maintain the signal throughout the network, thus avoiding vanishing/exploding gradients.  \n",
    "**Best For:** Tanh and sigmoid activation functions.\n",
    "\n",
    "## 4. He Initialization (Kaiming Initialization)\n",
    "**Method:** Initialize weights using a distribution with variance $\\text{Var}(W) = \\frac{2}{n_{\\text{in}}}$.  \n",
    "**Goal:** Designed to work well with ReLU and its variants. This method accounts for the fact that ReLU neurons activate (i.e., are non-zero) only about 50% of the time, requiring larger variance.  \n",
    "**Best For:** ReLU and its variants (e.g., Leaky ReLU).\n",
    "\n",
    "## 5. LeCun Initialization\n",
    "**Method:** Similar to Xavier, but with variance $\\text{Var}(W) = \\frac{1}{n_{\\text{in}}}$.  \n",
    "**Goal:** Tailored for activation functions like SELU, designed to self-normalize.  \n",
    "**Best For:** SELU activation functions.\n",
    "\n",
    "## 6. Orthogonal Initialization\n",
    "**Method:** Initialize weight matrices using orthogonal matrices, achievable via Singular Value Decomposition (SVD) of a randomly generated matrix.  \n",
    "**Goal:** Maintain variance of activations across layers, crucial for very deep networks.  \n",
    "**Best For:** Recurrent Neural Networks (RNNs) and deep architectures where preserving the input signal is essential.\n",
    "\n",
    "## 7. Custom Initializations (Layer-Specific)\n",
    "**Method:** Some networks might benefit from custom initializations, depending on the architecture. For instance, initializing certain layers differently based on their role in the network (e.g., using smaller weights for deeper layers).  \n",
    "**Goal:** Fine-tune initialization for specific tasks or architectures.\n",
    "\n",
    "## Summary\n",
    "- **Zero Initialization:** Not recommended due to symmetry breaking issues.\n",
    "- **Random Initialization:** Requires careful selection of range/variance.\n",
    "- **Xavier Initialization:** Suitable for tanh and sigmoid activations.\n",
    "- **He Initialization:** Best for ReLU and its variants.\n",
    "- **LeCun Initialization:** Ideal for SELU activations.\n",
    "- **Orthogonal Initialization:** Useful for preserving variance across deep layers, especially in RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Neural Network model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a 3-layer neural network (already implemented for you). Here are the initialization methods you will experiment with:  \n",
    "- *Zeros initialization* --  setting `initialization = \"zeros\"` in the input argument.\n",
    "- *Random initialization* -- setting `initialization = \"random\"` in the input argument. This initializes the weights to large random values.  \n",
    "- *He initialization* -- setting `initialization = \"he\"` in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015. \n",
    "\n",
    "**Instructions**: Please quickly read over the code below, and run it. In the next part you will implement the three initialization methods that this `model()` calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Zero initialization\n",
    "\n",
    "There are two types of parameters to initialize in a neural network:\n",
    "- the weight matrices $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "- the bias vectors $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n",
    "\n",
    "**Exercise**: Implement the following function to initialize all parameters to zeros. You'll see later that this does not work well since it fails to \"break symmetry\", but lets try it anyway and see what happens. Use np.zeros((..,..)) with the correct shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is predicting 0 for every example. \n",
    "\n",
    "In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with $n^{[l]}=1$ for every layer, and the network is no more powerful than a linear classifier such as logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**What you should remember**:\n",
    ">- The weights $W^{[l]}$ should be initialized randomly to break symmetry. \n",
    ">- It is however okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7598, -0.2424,  1.5024, -0.0970,  1.4350,  0.8894, -0.1437,  0.1174,\n",
      "         -1.0093, -0.7497]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)  # Fully connected layer (input: 784, output: 128)\n",
    "        self.fc2 = nn.Linear(128, 64)   # Fully connected layer (input: 128, output: 64)\n",
    "        self.fc3 = nn.Linear(64, 10)    # Fully connected layer (input: 64, output: 10)\n",
    "\n",
    "        # Apply Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the output of the first layer\n",
    "        x = F.relu(self.fc2(x))  # Apply ReLU activation to the output of the second layer\n",
    "        x = self.fc3(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Create an instance of the network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Example input\n",
    "input_tensor = torch.randn(1, 784)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialization\n",
    "\n",
    "To break symmetry, lets intialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you will see what happens if the weights are intialized randomly, but to very large values. \n",
    "\n",
    "**Exercise**: Implement the following function to initialize your weights to large random values (scaled by \\*10) and your biases to zeros. Use `np.random.randn(..,..) * 10` for weights and `np.zeros((.., ..))` for biases. We are using a fixed `np.random.seed(..)` to make sure your \"random\" weights  match ours, so don't worry if running several times your code gives you always the same initial values for the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of the first layer: Parameter containing:\n",
      "tensor([[ 2.4738e-01,  1.0741e-01,  7.3833e-02,  1.7492e+00, -6.1805e-01,\n",
      "          8.2370e-01,  1.2449e+00,  4.2389e-01,  1.0911e+00,  1.1905e+00],\n",
      "        [ 5.0406e-01, -1.6414e+00, -1.8465e-02, -6.7157e-01, -6.1897e-01,\n",
      "         -1.3121e+00, -1.2527e+00,  5.7761e-01,  1.1591e+00,  1.3779e+00],\n",
      "        [-7.6910e-02, -2.9013e-01, -1.5574e-01, -1.4871e+00, -8.5508e-01,\n",
      "         -1.5233e+00,  1.1853e+00, -1.2545e+00,  5.4780e-01, -1.0406e+00],\n",
      "        [ 1.2977e+00,  1.0952e+00, -3.5565e-01,  4.4473e-01, -3.9502e-01,\n",
      "          3.4125e-01, -4.8433e-01,  1.4705e+00,  2.6437e-01, -8.9548e-02],\n",
      "        [ 1.6065e-01,  7.8690e-01, -1.8365e+00, -6.0202e-01, -1.8886e+00,\n",
      "         -4.2463e-01, -8.7001e-01, -1.3984e+00, -1.9412e+00, -1.4557e+00],\n",
      "        [-5.5558e-01, -1.3261e+00,  1.6025e+00,  4.0271e-02,  9.5612e-01,\n",
      "          1.1425e+00, -6.5658e-01,  1.5952e+00,  2.0885e-01,  4.6444e-01],\n",
      "        [ 1.8130e-01,  9.5424e-01,  5.6673e-01,  1.1100e+00,  1.1273e+00,\n",
      "          5.7793e-01, -3.2232e-01,  2.6246e-02, -6.1141e-02,  1.1893e+00],\n",
      "        [-1.5791e+00,  3.4532e-02, -1.0300e+00, -3.1477e-01,  3.9461e-01,\n",
      "          1.4310e+00,  5.3368e-01, -7.1478e-01, -2.0626e+00, -1.1074e-02],\n",
      "        [-8.3291e-01, -1.0970e-01, -3.7534e-01,  8.9846e-01, -7.1235e-02,\n",
      "         -4.9419e-01,  4.9923e-01,  5.0467e-01,  2.1007e-01,  1.2246e-01],\n",
      "        [-4.0965e-01,  1.2482e+00, -1.0411e+00, -6.1850e-01, -1.7935e+00,\n",
      "         -1.6130e+00, -2.1579e-01, -3.3923e-01, -1.7093e+00,  3.7100e-01],\n",
      "        [ 2.6348e-01,  1.1904e+00, -9.4395e-02, -3.8985e-01,  5.1852e-01,\n",
      "         -2.7530e-01, -1.1444e-01,  4.5647e-01,  7.8994e-01, -1.0183e+00],\n",
      "        [ 4.5189e-01,  1.6621e+00, -1.8268e-01,  3.7466e-01, -8.9947e-01,\n",
      "          7.1419e-01, -2.1535e-01, -3.4386e-01, -3.1204e-01, -1.2966e+00],\n",
      "        [ 3.4706e-01,  1.0805e+00, -1.2081e+00,  1.4263e+00,  3.1751e-02,\n",
      "          1.9651e+00,  5.3864e-01, -5.5870e-01,  5.8997e-01,  1.0485e+00],\n",
      "        [ 3.5680e-01, -5.5811e-01,  8.8833e-01, -2.1680e-01, -7.7095e-01,\n",
      "          4.4894e-01,  1.7851e+00,  8.3219e-01,  2.8622e+00,  1.0320e-01],\n",
      "        [ 3.3618e-02, -3.4958e-01, -1.3310e-02,  1.5409e+00,  8.4311e-01,\n",
      "          1.3132e+00,  1.9922e+00, -3.7401e-02, -9.3119e-01,  2.2138e+00],\n",
      "        [ 4.2177e-01, -6.8947e-01,  5.9296e-02, -2.5732e-01,  7.5382e-01,\n",
      "         -1.0459e-01, -2.7006e-01, -3.0811e-02,  1.1616e-01,  2.0334e-01],\n",
      "        [-2.3771e-01,  8.8045e-01, -8.1513e-01,  5.2015e-01, -9.5549e-02,\n",
      "         -4.2891e-01,  1.2811e+00,  2.8291e-01,  1.4649e+00,  2.0567e-02],\n",
      "        [ 9.5399e-01,  1.9047e-01, -2.1132e+00, -4.4722e-01,  2.1477e+00,\n",
      "          2.5557e-01, -3.5812e-01, -2.0976e+00, -2.6694e+00,  1.6857e-01],\n",
      "        [-4.4180e-01, -7.1506e-01, -1.0651e+00, -3.0728e-01, -2.4534e-01,\n",
      "          7.1309e-01, -9.4125e-01,  1.1682e+00,  1.0977e-01,  9.3907e-01],\n",
      "        [-6.1176e-01,  1.8639e+00,  3.0617e-01,  4.8021e-01,  4.1351e-01,\n",
      "         -6.1096e-02, -4.3426e-01,  6.7545e-01,  6.8891e-01, -2.6296e+00],\n",
      "        [ 1.3946e+00, -2.0793e-01, -6.6531e-01, -8.4020e-02,  5.8285e-01,\n",
      "         -1.2673e+00,  6.7076e-01,  8.3316e-01,  6.1703e-01, -7.7188e-02],\n",
      "        [-1.7811e+00,  3.1521e-01,  1.1801e+00,  4.2183e-01,  7.0433e-01,\n",
      "          1.8505e+00, -1.3496e+00,  1.3483e-01, -1.2323e+00,  4.8416e-01],\n",
      "        [-1.0380e+00, -1.6521e-01, -1.0058e+00,  5.0198e-01,  1.1502e+00,\n",
      "          8.1148e-01, -3.7654e-01, -6.4156e-01,  3.9546e-01,  9.2671e-01],\n",
      "        [-1.4556e+00, -1.0651e+00, -2.0302e-01, -1.9954e+00,  3.2089e-01,\n",
      "          4.8729e-03, -5.5286e-01, -1.2208e+00,  9.0681e-01,  5.3047e-01],\n",
      "        [-4.6690e-01, -1.6756e+00,  3.5891e-01,  1.0408e+00, -3.1393e-02,\n",
      "         -8.7048e-01, -1.5185e+00,  1.2590e+00, -6.8457e-01,  9.6232e-01],\n",
      "        [ 3.0661e-01,  7.9929e-01, -1.0848e+00, -8.2072e-01,  1.4292e+00,\n",
      "         -9.9783e-01,  2.2036e-01, -1.2712e+00,  2.3528e-03,  8.5075e-01],\n",
      "        [ 3.4060e-01, -4.0595e-02,  7.9758e-01, -1.3813e+00,  1.6122e-01,\n",
      "          5.4871e-01,  3.5272e-01,  8.3884e-01,  1.6321e+00,  1.4909e+00],\n",
      "        [ 2.6183e-01,  7.7656e-01,  1.1201e+00, -9.2302e-02, -6.4243e-02,\n",
      "         -9.5001e-01,  5.5078e-03, -1.4634e-01, -1.1272e+00, -4.3112e-01],\n",
      "        [ 7.4886e-01,  3.9124e-01,  1.3230e-01, -7.1625e-01, -7.8789e-01,\n",
      "          1.3604e+00, -1.8378e+00, -5.6419e-02,  1.0302e+00, -7.3664e-01],\n",
      "        [-1.0535e+00,  1.1011e-01, -5.6479e-01, -1.4064e+00,  8.4863e-01,\n",
      "         -2.0468e-01,  1.3271e+00, -3.5084e-01,  5.1139e-01, -2.2150e+00],\n",
      "        [-3.9868e-01, -7.9805e-01,  5.6563e-01, -1.0483e-01, -2.7723e-01,\n",
      "         -1.1167e+00, -3.2294e-01, -3.8191e-01, -8.3287e-01,  1.9305e+00],\n",
      "        [ 6.5595e-01,  9.4532e-01,  1.4535e+00,  1.5228e+00, -5.6237e-01,\n",
      "          3.4484e-01, -8.6579e-01,  1.2451e-01,  7.6369e-02, -4.1998e-01],\n",
      "        [-7.0648e-01,  2.5853e+00, -4.6047e-01,  7.6375e-01,  3.1918e-01,\n",
      "          2.6976e-01, -6.1271e-01,  8.4731e-01, -1.3964e+00, -1.2390e+00],\n",
      "        [-2.0818e-01, -9.8029e-01,  4.8217e-01, -6.8437e-01,  1.5144e+00,\n",
      "          2.8191e-01,  1.6396e+00,  6.9122e-01,  2.3347e-01,  2.6541e-01],\n",
      "        [ 6.9939e-02, -1.4379e+00,  1.8295e+00, -2.7598e-01,  5.9564e-01,\n",
      "         -7.3007e-01,  7.5050e-01,  8.4202e-01, -1.4666e+00,  7.9656e-01],\n",
      "        [ 3.5544e-01, -7.7518e-01, -1.3722e+00,  3.1674e-01, -1.3012e+00,\n",
      "          9.2225e-02, -1.4345e+00,  9.6567e-01, -6.9900e-01,  1.8729e+00],\n",
      "        [-2.0555e-01, -3.0511e-01, -7.7831e-01, -6.5514e-01,  2.0332e-01,\n",
      "          6.6020e-01, -1.5332e+00, -3.3127e-01,  1.5255e+00,  7.6274e-01],\n",
      "        [ 1.2276e-01,  2.1864e+00,  1.1226e+00,  1.1824e-01,  5.0059e-01,\n",
      "          2.8417e+00, -1.1189e+00,  2.0442e+00, -2.8719e-02,  9.4961e-01],\n",
      "        [-3.2168e-01, -1.3321e+00,  4.1012e-01, -1.0832e+00, -4.8638e-03,\n",
      "         -1.1902e+00,  7.5843e-01,  2.0467e-01,  8.1245e-01, -2.2074e+00],\n",
      "        [ 4.3277e-01, -9.7652e-01,  1.4224e+00,  9.5447e-01,  1.6736e+00,\n",
      "          2.4473e+00, -1.0106e-01,  6.4917e-01, -5.1405e-01, -4.4458e-01],\n",
      "        [ 6.5689e-01,  7.6919e-01,  7.4614e-01, -1.2954e+00, -1.5662e+00,\n",
      "         -7.7678e-01, -8.9244e-02,  5.0327e-01, -6.7346e-01, -9.1563e-01],\n",
      "        [ 9.2736e-02,  2.9183e-01, -3.7897e-01,  6.1799e-01,  1.2819e-01,\n",
      "         -1.7146e+00,  3.9608e-01,  2.4116e-01,  5.9910e-01,  1.6935e+00],\n",
      "        [-1.2969e+00, -2.8436e-01,  4.9862e-01,  1.6170e-01,  1.2585e+00,\n",
      "          1.7377e-01, -2.6989e-01, -5.0705e-01,  7.5693e-01, -1.8817e-01],\n",
      "        [ 4.8409e-01,  9.7669e-01,  5.0796e-02,  2.6300e-01, -1.3074e+00,\n",
      "         -1.3463e+00,  8.7378e-01,  4.7692e-01,  1.0597e+00,  6.0198e-02],\n",
      "        [-1.0933e+00,  1.4229e+00, -1.2531e+00, -2.0937e-01, -1.4754e+00,\n",
      "          3.1918e-01, -8.9479e-01, -1.0113e+00, -7.2481e-01,  2.9259e+00],\n",
      "        [-8.6333e-01, -1.3107e+00,  9.0631e-01,  1.3156e-01, -1.2063e+00,\n",
      "          9.3247e-01, -2.3607e-01,  2.3708e-01,  4.8812e-01, -1.2911e+00],\n",
      "        [ 3.7886e-02, -2.9971e-01, -4.8868e-01, -5.8298e-02, -5.6370e-01,\n",
      "          1.7282e+00, -2.7610e-01, -7.0143e-02, -1.3423e+00, -1.9321e+00],\n",
      "        [-7.1438e-01,  6.3590e-01,  6.9375e-01,  9.7339e-01,  4.5541e-01,\n",
      "         -4.2353e-01,  5.4354e-02, -6.7676e-02,  1.3777e-01, -5.8152e-01],\n",
      "        [ 1.9321e+00,  8.7562e-01, -8.6168e-01,  1.0532e+00,  1.4412e+00,\n",
      "         -8.5328e-01, -5.3037e-01, -5.2242e-01,  1.7575e+00, -3.4460e-01],\n",
      "        [-1.7364e+00,  3.0192e-01,  1.8118e-01,  7.0632e-01,  6.7476e-01,\n",
      "          1.6317e+00,  8.4233e-01, -1.1164e+00, -5.1896e-02,  1.5181e+00]],\n",
      "       requires_grad=True)\n",
      "Biases of the first layer: Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)  # Fully connected layer (10 -> 50)\n",
    "        self.fc2 = nn.Linear(50, 20)  # Fully connected layer (50 -> 20)\n",
    "        self.fc3 = nn.Linear(20, 1)   # Fully connected layer (20 -> 1)\n",
    "\n",
    "        # Apply random initialization to weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=1.0)  # Random normal initialization\n",
    "            nn.init.constant_(layer.bias, 0)  # Initialize biases to zero\n",
    "\n",
    "# Create an instance of the network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Print the initialized weights and biases of the first layer\n",
    "print(\"Weights of the first layer:\", model.fc1.weight)\n",
    "print(\"Biases of the first layer:\", model.fc1.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.2860\n",
      "Epoch 2/100, Loss: 0.2623\n",
      "Epoch 3/100, Loss: 0.2384\n",
      "Epoch 4/100, Loss: 0.2223\n",
      "Epoch 5/100, Loss: 0.2064\n",
      "Epoch 6/100, Loss: 0.1945\n",
      "Epoch 7/100, Loss: 0.1839\n",
      "Epoch 8/100, Loss: 0.1736\n",
      "Epoch 9/100, Loss: 0.1648\n",
      "Epoch 10/100, Loss: 0.1572\n",
      "Epoch 11/100, Loss: 0.1508\n",
      "Epoch 12/100, Loss: 0.1478\n",
      "Epoch 13/100, Loss: 0.1439\n",
      "Epoch 14/100, Loss: 0.1389\n",
      "Epoch 15/100, Loss: 0.1346\n",
      "Epoch 16/100, Loss: 0.1340\n",
      "Epoch 17/100, Loss: 0.1293\n",
      "Epoch 18/100, Loss: 0.1277\n",
      "Epoch 19/100, Loss: 0.1243\n",
      "Epoch 20/100, Loss: 0.1195\n",
      "Epoch 21/100, Loss: 0.1187\n",
      "Epoch 22/100, Loss: 0.1157\n",
      "Epoch 23/100, Loss: 0.1143\n",
      "Epoch 24/100, Loss: 0.1144\n",
      "Epoch 25/100, Loss: 0.1078\n",
      "Epoch 26/100, Loss: 0.1069\n",
      "Epoch 27/100, Loss: 0.1063\n",
      "Epoch 28/100, Loss: 0.1055\n",
      "Epoch 29/100, Loss: 0.1006\n",
      "Epoch 30/100, Loss: 0.1025\n",
      "Epoch 31/100, Loss: 0.1009\n",
      "Epoch 32/100, Loss: 0.1003\n",
      "Epoch 33/100, Loss: 0.0982\n",
      "Epoch 34/100, Loss: 0.0970\n",
      "Epoch 35/100, Loss: 0.0934\n",
      "Epoch 36/100, Loss: 0.0956\n",
      "Epoch 37/100, Loss: 0.0918\n",
      "Epoch 38/100, Loss: 0.0923\n",
      "Epoch 39/100, Loss: 0.0895\n",
      "Epoch 40/100, Loss: 0.0905\n",
      "Epoch 41/100, Loss: 0.0909\n",
      "Epoch 42/100, Loss: 0.0886\n",
      "Epoch 43/100, Loss: 0.0893\n",
      "Epoch 44/100, Loss: 0.0888\n",
      "Epoch 45/100, Loss: 0.0873\n",
      "Epoch 46/100, Loss: 0.0877\n",
      "Epoch 47/100, Loss: 0.0855\n",
      "Epoch 48/100, Loss: 0.0833\n",
      "Epoch 49/100, Loss: 0.0845\n",
      "Epoch 50/100, Loss: 0.0823\n",
      "Epoch 51/100, Loss: 0.0824\n",
      "Epoch 52/100, Loss: 0.0824\n",
      "Epoch 53/100, Loss: 0.0813\n",
      "Epoch 54/100, Loss: 0.0796\n",
      "Epoch 55/100, Loss: 0.0782\n",
      "Epoch 56/100, Loss: 0.0802\n",
      "Epoch 57/100, Loss: 0.0786\n",
      "Epoch 58/100, Loss: 0.0792\n",
      "Epoch 59/100, Loss: 0.0808\n",
      "Epoch 60/100, Loss: 0.0762\n",
      "Epoch 61/100, Loss: 0.0779\n",
      "Epoch 62/100, Loss: 0.0781\n",
      "Epoch 63/100, Loss: 0.0741\n",
      "Epoch 64/100, Loss: 0.0738\n",
      "Epoch 65/100, Loss: 0.0766\n",
      "Epoch 66/100, Loss: 0.0748\n",
      "Epoch 67/100, Loss: 0.0745\n",
      "Epoch 68/100, Loss: 0.0724\n",
      "Epoch 69/100, Loss: 0.0720\n",
      "Epoch 70/100, Loss: 0.0746\n",
      "Epoch 71/100, Loss: 0.0711\n",
      "Epoch 72/100, Loss: 0.0707\n",
      "Epoch 73/100, Loss: 0.0715\n",
      "Epoch 74/100, Loss: 0.0701\n",
      "Epoch 75/100, Loss: 0.0696\n",
      "Epoch 76/100, Loss: 0.0708\n",
      "Epoch 77/100, Loss: 0.0696\n",
      "Epoch 78/100, Loss: 0.0699\n",
      "Epoch 79/100, Loss: 0.0716\n",
      "Epoch 80/100, Loss: 0.0701\n",
      "Epoch 81/100, Loss: 0.0675\n",
      "Epoch 82/100, Loss: 0.0690\n",
      "Epoch 83/100, Loss: 0.0670\n",
      "Epoch 84/100, Loss: 0.0671\n",
      "Epoch 85/100, Loss: 0.0671\n",
      "Epoch 86/100, Loss: 0.0665\n",
      "Epoch 87/100, Loss: 0.0665\n",
      "Epoch 88/100, Loss: 0.0661\n",
      "Epoch 89/100, Loss: 0.0663\n",
      "Epoch 90/100, Loss: 0.0662\n",
      "Epoch 91/100, Loss: 0.0641\n",
      "Epoch 92/100, Loss: 0.0644\n",
      "Epoch 93/100, Loss: 0.0636\n",
      "Epoch 94/100, Loss: 0.0671\n",
      "Epoch 95/100, Loss: 0.0648\n",
      "Epoch 96/100, Loss: 0.0629\n",
      "Epoch 97/100, Loss: 0.0628\n",
      "Epoch 98/100, Loss: 0.0630\n",
      "Epoch 99/100, Loss: 0.0649\n",
      "Epoch 100/100, Loss: 0.0617\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for [[ 1. -1.]]: 0.4066\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Step 1: Generate a synthetic dataset\n",
    "# We'll create a simple binary classification dataset\n",
    "\n",
    "# Generate random data points\n",
    "X = torch.randn(1000, 2)  # 1000 samples, 2 features\n",
    "# Create labels (0 or 1) based on a simple linear function with noise\n",
    "y = (X[:, 0] + X[:, 1] > 0).float().reshape(-1, 1)\n",
    "\n",
    "# Step 2: Create a DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Step 3: Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary classification\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Step 4: Initialize weights with random values\n",
    "def initialize_weights_random(model):\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.normal_(layer.weight, mean=0.0, std=1.0)\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "initialize_weights_random(model)\n",
    "\n",
    "# Step 5: Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 6: Train the model\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Record and print the average loss for each epoch\n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(average_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}')\n",
    "\n",
    "# Step 7: Plot the training loss over epochs\n",
    "plt.plot(range(num_epochs), losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Test the model on a new synthetic sample\n",
    "with torch.no_grad():\n",
    "    test_sample = torch.tensor([[1.0, -1.0]])  # Example test input\n",
    "    prediction = model(test_sample)\n",
    "    print(f'Prediction for {test_sample.numpy()}: {prediction.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see \"inf\" as the cost after the iteration 0, this is because of numerical roundoff; a more numerically sophisticated implementation would fix this. But this isn't worth worrying about for our purposes. \n",
    "\n",
    "Anyway, it looks like you have broken symmetry, and this gives better results. than before. The model is no longer outputting all 0s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when $\\log(a^{[3]}) = \\log(0)$, the loss goes to infinity.\n",
    "- Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. \n",
    "- If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.\n",
    "\n",
    "> In summary:\n",
    ">- Initializing weights to very large random values does not work well. \n",
    ">- Hopefully intializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - He initialization\n",
    "\n",
    "Finally, try \"He Initialization\"; this is named for the first author of He et al., 2015. (If you have heard of \"Xavier initialization\", this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of `sqrt(1./layers_dims[l-1])` where He initialization would use `sqrt(2./layers_dims[l-1])`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(256, 128)  # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(128, 10)   # Output layer\n",
    "\n",
    "        # Apply He initialization to all layers\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "# Create the network instance\n",
    "model = SimpleNN()\n",
    "\n",
    "# Example input: a batch of images with 784 features (e.g., 28x28 pixels flattened)\n",
    "example_input = torch.randn(64, 784)\n",
    "\n",
    "# Forward pass through the network\n",
    "output = model(example_input)\n",
    "print(output.shape)  # Expected output shape: (64, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 - Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**What you should remember from this notebook**:\n",
    ">- Different initializations lead to different results\n",
    ">- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
    ">- Don't intialize to values that are too large\n",
    ">- He initialization works well for networks with ReLU activations. "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "XOESP",
   "launcher_item_id": "8IhFN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
