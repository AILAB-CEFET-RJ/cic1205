{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c74307",
   "metadata": {},
   "source": [
    "## Cell state versus hidden state\n",
    "\n",
    "The distinction between **hidden state** and **cell state** in an LSTM (Long Short-Term Memory) cell is central to why LSTMs are more effective than traditional RNNs at capturing **long-term dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Explanation:\n",
    "\n",
    "#### 1. **Cell State ($C_t$) = Internal Memory**\n",
    "\n",
    "* The **cell state** is like the \"long-term memory\" of the LSTM.\n",
    "* It flows along the sequence **mostly unchanged**, only modulated by the **forget** and **input gates**.\n",
    "* This makes it easier to retain information over long time spans.\n",
    "* Analogy: a notebook you carry and write in (add/delete info) as you go along a sequence.\n",
    "\n",
    "#### 2. **Hidden State ($h_t$) = Output at Time t**\n",
    "\n",
    "* The **hidden state** is the **output** that gets passed to:\n",
    "\n",
    "  * the **next time step**, and\n",
    "  * the **next layer** (if stacked).\n",
    "* It is computed by applying a `tanh` on the current cell state and multiplying it by the **output gate**.\n",
    "* Analogy: the filtered view of your notebook that you share with others at each step.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Both are Needed?\n",
    "\n",
    "1. **Hidden state** alone (like in vanilla RNNs) is often **too volatile**. It gets updated at every time step and forgets things quickly.\n",
    "2. **Cell state** allows for more **stable and controlled memory**:\n",
    "\n",
    "   * You decide (via gates) what to forget, what to remember, and what to expose.\n",
    "3. Having both lets the LSTM **decouple memory from output**:\n",
    "\n",
    "   * The model can **store** useful information without **exposing** it immediately.\n",
    "\n",
    "---\n",
    "\n",
    "### Textual Summary:\n",
    "\n",
    "| Component        | Symbol | Role                                                                                     |\n",
    "| ---------------- | ------ | ---------------------------------------------------------------------------------------- |\n",
    "| **Cell State**   | $C_t$  | **Memory** of the cell; stores long-term information across time.                        |\n",
    "| **Hidden State** | $h_t$  | **Output** of the cell; controls what is exposed to the rest of the network at time $t$. |\n",
    "\n",
    "---\n",
    "### Visual Summary:\n",
    "\n",
    "```\n",
    "           +--------------+     \n",
    "           |   Cell State |  <--- long-term memory (C_t)\n",
    "           +--------------+\n",
    "                 ↑   ↑\n",
    "         forget ⊙    ⊕  input\n",
    "                 ↓   ↓\n",
    "             +---------+\n",
    "             |   LSTM  |\n",
    "             +---------+\n",
    "                 ↓\n",
    "           tanh(C_t) ⊙ output gate\n",
    "                 ↓\n",
    "           Hidden State h_t  ---> output to next time step/layer\n",
    "```\n",
    "\n",
    "## Cell state versus hidden state\n",
    "\n",
    "In an LSTM:\n",
    "\n",
    "> **The hidden state $h_t$ is the short-term memory.**\n",
    "> **The cell state $C_t$ is the long-term memory.**\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition Behind the Separation\n",
    "\n",
    "| Component        | Symbol | Memory Type           | Purpose                                                                                        |\n",
    "| ---------------- | ------ | --------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **Hidden State** | $h_t$  | **Short-term** memory | Captures recent, transient information; also serves as the **output** of the LSTM at time $t$. |\n",
    "| **Cell State**   | $C_t$  | **Long-term** memory  | Captures persistent memory across time steps; modulated by gates.                              |\n",
    "\n",
    "---\n",
    "\n",
    "### Why does this matter?\n",
    "\n",
    "* **$h_t$** gets **fully recomputed** at every time step. It reflects what the LSTM \"wants to say now\" — it’s influenced by the **current input** and recent context.\n",
    "* **$C_t$** can persist information over **long sequences**, **even if it's not relevant to the immediate output**.\n",
    "\n",
    "---\n",
    "\n",
    "### Analogy\n",
    "\n",
    "Think of an LSTM cell as a **human reading a paragraph**:\n",
    "\n",
    "* **$C_t$** is your overall understanding of the story so far (the broader context you carry with you).\n",
    "* **$h_t$** is what you’re thinking about *right now*, maybe just the meaning of the current sentence.\n",
    "\n",
    "---\n",
    "\n",
    "### What about in vanilla RNNs?\n",
    "\n",
    "* In a simple RNN, there's **only $h_t$**, which tries to do **both jobs** (memory + output).\n",
    "* This is why vanilla RNNs struggle with **long-term dependencies** — they can't separate what's important to **remember** vs. what needs to be **output**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff0115",
   "metadata": {},
   "source": [
    "# Toy Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b046f2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat [h_{t-1}, x_t]         : [0.40, -0.20, 0.50, 0.10]\n",
      "f_t (forget gate)             : [0.80, 0.20]\n",
      "i_t (input gate)              : [0.60, 0.40]\n",
      "C̃_t (candidate vector)       : [0.70, -0.10]\n",
      "C_t (new cell state)          : [0.66, -0.04]\n",
      "o_t (output gate)             : [0.90, 0.30]\n",
      "h_t (new hidden state)        : [0.52, -0.01]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Inputs (as tensors)\n",
    "x_t = torch.tensor([0.5, 0.1])             # input at time t\n",
    "h_t_minus_1 = torch.tensor([0.4, -0.2])    # previous hidden state\n",
    "C_t_minus_1 = torch.tensor([0.3, 0.0])     # previous cell state\n",
    "\n",
    "# Concatenation of h_{t-1} and x_t\n",
    "concat = torch.cat([h_t_minus_1, x_t])     # shape: (4,)\n",
    "\n",
    "# Simulate fixed weights to reproduce the desired gate outputs\n",
    "# We manually force values by inverting the activation functions\n",
    "\n",
    "def simulate_gate(output_vals):\n",
    "    # Sigmoid inverse: logit(y) = log(y / (1 - y))\n",
    "    return torch.log(output_vals / (1 - output_vals))\n",
    "\n",
    "# Target gate values as in the HTML animation\n",
    "f_t = torch.sigmoid(simulate_gate(torch.tensor([0.8, 0.2])))\n",
    "i_t = torch.sigmoid(simulate_gate(torch.tensor([0.6, 0.4])))\n",
    "o_t = torch.sigmoid(simulate_gate(torch.tensor([0.9, 0.3])))\n",
    "\n",
    "# Candidate vector C̃ₜ (simulated via tanh inverse)\n",
    "c_tilde = torch.tanh(torch.atanh(torch.tensor([0.7, -0.1])))\n",
    "\n",
    "# Step 4: new cell state\n",
    "C_t = f_t * C_t_minus_1 + i_t * c_tilde\n",
    "\n",
    "# Step 6: new hidden state\n",
    "h_t = o_t * torch.tanh(C_t)\n",
    "\n",
    "# Print results with consistent formatting (2 decimal places)\n",
    "def format_vector(label, vec):\n",
    "    values = [f\"{v.item():.2f}\" for v in vec]\n",
    "    print(f\"{label:<30}: [{', '.join(values)}]\")\n",
    "\n",
    "format_vector(\"Concat [h_{t-1}, x_t]\", concat)\n",
    "format_vector(\"f_t (forget gate)\", f_t)\n",
    "format_vector(\"i_t (input gate)\", i_t)\n",
    "format_vector(\"C̃_t (candidate vector)\", c_tilde)\n",
    "format_vector(\"C_t (new cell state)\", C_t)\n",
    "format_vector(\"o_t (output gate)\", o_t)\n",
    "format_vector(\"h_t (new hidden state)\", h_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849852b",
   "metadata": {},
   "source": [
    "# Toy Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a8a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      " tensor([[ 1.5410, -0.2934, -2.1788],\n",
      "        [ 0.5684, -1.0845, -1.3986],\n",
      "        [ 0.4033,  0.8380, -0.7193],\n",
      "        [-0.4033, -0.5966,  0.1820]])\n",
      "\n",
      "Output at each time step:\n",
      " tensor([[-0.1491,  0.0378],\n",
      "        [-0.2774,  0.0892],\n",
      "        [-0.0008,  0.2280],\n",
      "        [ 0.0942,  0.1251]], grad_fn=<SqueezeBackward0>)\n",
      "\n",
      "Final hidden state:\n",
      " tensor([0.0942, 0.1251], grad_fn=<SqueezeBackward0>)\n",
      "\n",
      "Final cell state:\n",
      " tensor([0.2080, 0.3955], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Input dimensions\n",
    "seq_len = 4      # number of time steps\n",
    "input_dim = 3    # features per time step\n",
    "hidden_dim = 2   # LSTM hidden state size\n",
    "batch_size = 1   # for simplicity\n",
    "\n",
    "# Dummy input (sequence of vectors)\n",
    "x = torch.randn(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Define LSTM layer (1 layer, unidirectional)\n",
    "lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "# Initial hidden and cell states (h0, c0)\n",
    "h0 = torch.zeros(1, batch_size, hidden_dim)\n",
    "c0 = torch.zeros(1, batch_size, hidden_dim)\n",
    "\n",
    "# Forward pass\n",
    "output, (hn, cn) = lstm(x, (h0, c0))\n",
    "\n",
    "# Print outputs\n",
    "print(\"Input sequence:\\n\", x.squeeze())\n",
    "print(\"\\nOutput at each time step:\\n\", output.squeeze())\n",
    "print(\"\\nFinal hidden state:\\n\", hn.squeeze())\n",
    "print(\"\\nFinal cell state:\\n\", cn.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bdb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 5])\n",
      "Output shape (all time steps): torch.Size([2, 4, 8])\n",
      "Final hidden state shape: torch.Size([1, 2, 8])\n",
      "Final cell state shape: torch.Size([1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Config\n",
    "input_size = 5     # number of features per time step\n",
    "hidden_size = 8    # size of hidden state and cell state\n",
    "seq_len = 4        # length of the input sequence\n",
    "batch_size = 2     # number of sequences in a batch\n",
    "\n",
    "# Dummy input: shape = (batch_size, seq_len, input_size)\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "# Define LSTM layer\n",
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "# Initial hidden state and cell state (num_layers=1 by default)\n",
    "h0 = torch.zeros(1, batch_size, hidden_size)  # shape: (num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(1, batch_size, hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "output, (hn, cn) = lstm(x, (h0, c0))\n",
    "\n",
    "# Print shapes\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape (all time steps):\", output.shape)\n",
    "print(\"Final hidden state shape:\", hn.shape)\n",
    "print(\"Final cell state shape:\", cn.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
