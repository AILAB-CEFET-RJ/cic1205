{"cells":[{"cell_type":"markdown","metadata":{"id":"KMI6jcdY3vz-"},"source":["# Créditos\n","\n","- [WORD EMBEDDINGS: ENCODING LEXICAL SEMANTICS](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#an-example-n-gram-language-modeling)\n","\n","- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n","\n","- [The Attention Mechanism from Scratch](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)\n","\n","- [How to use TorchText for neural machine translation, plus hack to make it 5x faster](https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95)\n","\n","- [Attention Mechanism | Deep Learning](https://youtu.be/wj3ZYbKKUHI)"]},{"cell_type":"markdown","metadata":{},"source":["# Approaches to Text Representation\n","\n","## One-Hot Encodings\n","\n","A one-hot encoding represents each word as a vector of length equal to the size of the vocabulary. All the values in the vector are zero except for a single element that is set to one, indicating the position of the word in the vocabulary.\n","- Characteristics: These are sparse vectors, as most of the elements are zero. There is no inherent notion of similarity between different words, as each word is represented independently of others.\n","- Use Cases: One-hot encodings are simple and are often used in the early stages of NLP models or as part of categorical feature processing.\n","\n","## Bag-of-Words (BoW)\n","\n","The Bag-of-Words model represents a text (such as a sentence or document) as a vector of word frequencies or binary indicators. It captures the number of times each word appears in the text but ignores grammar and word order.\n","- Characteristics: BoW is also a sparse representation. It provides a simple method for feature extraction from text, often used in traditional NLP tasks like text classification and information retrieval.\n","- Limitations: It does not capture semantic meaning or the relationship between words.\n","\n","## TF-IDF (Term Frequency-Inverse Document Frequency)\n","\n","TF-IDF is a weighting scheme that reflects how important a word is to a document in a collection (corpus). It is calculated by multiplying the term frequency (TF) by the inverse document frequency (IDF).\n","- Characteristics: Like BoW, TF-IDF vectors are sparse and capture the importance of words, helping to reduce the weight of common words (like \"the\", \"is\", \"and\") that are less informative.\n","- Use Cases: TF-IDF is commonly used in document retrieval, text mining, and various text classification tasks.\n","\n","## N-gram Models\n","\n","N-grams represent a text as a sequence of 'n' consecutive words. For example, in a bigram model (n=2), the phrase \"natural language processing\" would be represented as (\"natural language\", \"language processing\").\n","- Characteristics: N-grams can capture some context by considering sequences of words. However, they are also sparse and become computationally expensive as 'n' increases.\n","- Use Cases: N-grams are used in language modeling, text generation, and other tasks where word order matters.\n","\n","## Character-Level Representations\n","\n","Instead of representing text at the word level, character-level models use sequences of characters. Each word or sentence is broken down into its constituent characters.\n","- Characteristics: These models can capture morphological information and handle misspellings or rare words better than word-level models.\n","- Use Cases: Character-level models are used in tasks like spell-checking, machine translation, and text normalization.\n","\n","## Sequence-to-Sequence Representations\n","\n","These are used primarily in models for tasks like machine translation, summarization, and other sequence generation tasks. The entire input sequence is mapped to an output sequence using encoder-decoder architectures.\n","- Characteristics: These models can handle variable-length input and output sequences and can capture more complex dependencies between words and sentences.\n","- Use Cases: Widely used in modern NLP applications that require generating text or transforming one sequence into another.\n","\n","## Distributed Representations\n","\n","A distributed representation is a way of encoding words, phrases, or other linguistic units as vectors of continuous values in a multi-dimensional space. Each dimension of this space captures some latent feature of the linguistic unit, and these vectors are often learned from large amounts of text data using machine learning techniques.\n","\n","Examples of Distributed Representations:\n","\n","- Word Embeddings: Methods like Word2Vec, GloVe, and FastText learn static word embeddings, where each word has a single vector representation.\n","\n","- Contextual Embeddings: Methods like BERT, ELMo, and GPT learn contextual embeddings, where the representation of a word depends on the **context** it appears in."]},{"cell_type":"markdown","metadata":{"id":"ehEVIS26xsU3"},"source":["# Camada nn.Embedding\n","\n","`nn.Embedding()` é uma camada de incorporação (*embedding layer*) no PyTorch, que recebe na entrada o índice (valor inteiro) de uma palavra e gera um vetor representativo da palavra.\n","\n","Como exemplo, suponha que um corpus tenha sido processado e, como resultado, foi gerado um dicionário (vocabulário) de 700 palavras. Suponha ainda que desejamos criar vetores de palavras de 100 dimensões. Por meio de uma camada `nn.Embedding()`, podemos fazer o mapeamento0 do índice de cada palavra no dicionário para seu vetor correspondente, conforme ilustrado na figura abaixo.\n","\n","<center><img src='https://i.imgur.com/mMFq0F9.png'></center>\n","\n","A instanciação da camada para o exemplo ilustrado na figura deve ser a seguinte:\n","\n","> `embeds = nn.Embedding(700, 100)`\n","\n","Internamente, `nn.Embedding` é um tipo particular de uma camada linear. Os parâmetros dessa camada são armazenados em uma matriz de dimensões $V \\times D$, sendo $V$ o número de palavras no dicionário e $D$ o tamanho de cada vetor de palavras. Essa camada apenas mapeia uma palavra (especificada por um índice) para o vetor de palavras correspondente, ou seja, a linha correspondente na matriz de parâmetros."]},{"cell_type":"markdown","metadata":{"id":"DlBSnfaatd-m"},"source":["Como exemplo, considere uma situação (fictícia) em que um vocabulário foi construído com duas palavras. Suponha também que foi decidido que a dimensionalidade dos vetores de palavras deve ser igual a cinco. O bloco de código a seguir ilustra o uso da camada nn.Embedding para produzir o vetor correspondente à primeira palavra do vocabulário."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6449,"status":"ok","timestamp":1647545757320,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"OUcl22XAQk2u"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1647545757322,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"U2B4tzSjQlV4","outputId":"4c88c9b4-b40a-4e8d-d656-3d72813879b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Lookup tensor:\n","tensor([0])\n","Resultado da camada de embedding:\n","tensor([[-0.5403,  1.4560,  0.3753,  1.2061,  0.9149]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["word_to_ix = {\"hello\": 0, \"world\": 1}\n","embeds = nn.Embedding(2, 5)\n","\n","lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n","print('Lookup tensor:')\n","print(lookup_tensor)\n","\n","hello_embed = embeds(lookup_tensor)\n","\n","print('Resultado da camada de embedding:')\n","print(hello_embed)"]},{"cell_type":"markdown","metadata":{"id":"RPfHN9cZ4VRH"},"source":["A seguir, um exemplo de que ilustra de que forma treinar um modelo de linguagem."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":680,"status":"ok","timestamp":1647292504115,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"-vjulTL-QcdW","outputId":"8c47e7df-1f4d-48e4-d514-9450efc84d23"},"outputs":[{"name":"stdout","output_type":"stream","text":["First 3 n-grams:\n"," [(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n","Losses:\n"," [524.9166340827942, 522.6140785217285, 520.3273525238037, 518.0552611351013, 515.7955350875854, 513.5485126972198, 511.3124771118164, 509.0867063999176, 506.87151551246643, 504.6662006378174]\n","Embedding vector for beauty:\n"," tensor([ 1.6651,  0.7111,  0.4536,  1.0112, -1.0816,  0.9382,  0.5687,  1.9885,\n","         1.7340,  1.2580], grad_fn=<SelectBackward0>)\n"]}],"source":["CONTEXT_SIZE = 2\n","EMBEDDING_DIM = 10\n","# We will use Shakespeare Sonnet 2\n","test_sentence = \"\"\"When forty winters shall besiege thy brow,\n","And dig deep trenches in thy beauty's field,\n","Thy youth's proud livery so gazed on now,\n","Will be a totter'd weed of small worth held:\n","Then being asked, where all thy beauty lies,\n","Where all the treasure of thy lusty days;\n","To say, within thine own deep sunken eyes,\n","Were an all-eating shame, and thriftless praise.\n","How much more praise deserv'd thy beauty's use,\n","If thou couldst answer 'This fair child of mine\n","Shall sum my count, and make my old excuse,'\n","Proving his beauty by succession thine!\n","This were to be new made when thou art old,\n","And see thy blood warm when thou feel'st it cold.\"\"\".split()\n","# we should tokenize the input, but we will ignore that for now\n","# build a list of tuples.\n","# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n","ngrams = [\n","    (\n","        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n","        test_sentence[i]\n","    )\n","    for i in range(CONTEXT_SIZE, len(test_sentence))\n","]\n","# Print the first 3, just so you can see what they look like.\n","print('First 3 n-grams:\\n', ngrams[:3])\n","\n","vocab = set(test_sentence)\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","\n","\n","class NGramLanguageModeler(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, context_size):\n","        super(NGramLanguageModeler, self).__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n","        self.linear2 = nn.Linear(128, vocab_size)\n","\n","    def forward(self, inputs):\n","        embeds = self.embeddings(inputs).view((1, -1))\n","        out = F.relu(self.linear1(embeds))\n","        out = self.linear2(out)\n","        log_probs = F.log_softmax(out, dim=1)\n","        return log_probs\n","\n","\n","losses = []\n","loss_function = nn.NLLLoss()\n","model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n","optimizer = optim.SGD(model.parameters(), lr=0.001)\n","\n","for epoch in range(10):\n","    total_loss = 0\n","    for context, target in ngrams:\n","\n","        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n","        # into integer indices and wrap them in tensors)\n","        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n","\n","        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n","        # new instance, you need to zero out the gradients from the old\n","        # instance\n","        model.zero_grad()\n","\n","        # Step 3. Run the forward pass, getting log probabilities over next\n","        # words\n","        log_probs = model(context_idxs)\n","\n","        # Step 4. Compute your loss function. (Again, Torch wants the target\n","        # word wrapped in a tensor)\n","        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n","\n","        # Step 5. Do the backward pass and update the gradient\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Get the Python number from a 1-element Tensor by calling tensor.item()\n","        total_loss += loss.item()\n","    losses.append(total_loss)\n","\n","print('Losses:\\n', losses)  # The loss decreased every iteration over the training data!\n","\n","# To get the embedding of a particular word, e.g. \"beauty\"\n","some_word = \"beauty\"\n","print('Embedding vector for %s:\\n %s' % (some_word, model.embeddings.weight[word_to_ix[some_word]]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1647293051327,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"3gkG2dkhURj1","outputId":"50cb0dc3-0619-458b-cead-9718a0c7669f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege'), (['besiege', 'shall'], 'thy'), (['thy', 'besiege'], 'brow,'), (['brow,', 'thy'], 'And'), (['And', 'brow,'], 'dig'), (['dig', 'And'], 'deep'), (['deep', 'dig'], 'trenches'), (['trenches', 'deep'], 'in'), (['in', 'trenches'], 'thy'), (['thy', 'in'], \"beauty's\"), ([\"beauty's\", 'thy'], 'field,'), (['field,', \"beauty's\"], 'Thy'), (['Thy', 'field,'], \"youth's\"), ([\"youth's\", 'Thy'], 'proud'), (['proud', \"youth's\"], 'livery'), (['livery', 'proud'], 'so'), (['so', 'livery'], 'gazed'), (['gazed', 'so'], 'on'), (['on', 'gazed'], 'now,'), (['now,', 'on'], 'Will'), (['Will', 'now,'], 'be'), (['be', 'Will'], 'a'), (['a', 'be'], \"totter'd\"), ([\"totter'd\", 'a'], 'weed'), (['weed', \"totter'd\"], 'of'), (['of', 'weed'], 'small'), (['small', 'of'], 'worth'), (['worth', 'small'], 'held:'), (['held:', 'worth'], 'Then'), (['Then', 'held:'], 'being'), (['being', 'Then'], 'asked,'), (['asked,', 'being'], 'where'), (['where', 'asked,'], 'all'), (['all', 'where'], 'thy'), (['thy', 'all'], 'beauty'), (['beauty', 'thy'], 'lies,'), (['lies,', 'beauty'], 'Where'), (['Where', 'lies,'], 'all'), (['all', 'Where'], 'the'), (['the', 'all'], 'treasure'), (['treasure', 'the'], 'of'), (['of', 'treasure'], 'thy'), (['thy', 'of'], 'lusty'), (['lusty', 'thy'], 'days;'), (['days;', 'lusty'], 'To'), (['To', 'days;'], 'say,'), (['say,', 'To'], 'within'), (['within', 'say,'], 'thine'), (['thine', 'within'], 'own'), (['own', 'thine'], 'deep'), (['deep', 'own'], 'sunken'), (['sunken', 'deep'], 'eyes,'), (['eyes,', 'sunken'], 'Were'), (['Were', 'eyes,'], 'an'), (['an', 'Were'], 'all-eating'), (['all-eating', 'an'], 'shame,'), (['shame,', 'all-eating'], 'and'), (['and', 'shame,'], 'thriftless'), (['thriftless', 'and'], 'praise.'), (['praise.', 'thriftless'], 'How'), (['How', 'praise.'], 'much'), (['much', 'How'], 'more'), (['more', 'much'], 'praise'), (['praise', 'more'], \"deserv'd\"), ([\"deserv'd\", 'praise'], 'thy'), (['thy', \"deserv'd\"], \"beauty's\"), ([\"beauty's\", 'thy'], 'use,'), (['use,', \"beauty's\"], 'If'), (['If', 'use,'], 'thou'), (['thou', 'If'], 'couldst'), (['couldst', 'thou'], 'answer'), (['answer', 'couldst'], \"'This\"), ([\"'This\", 'answer'], 'fair'), (['fair', \"'This\"], 'child'), (['child', 'fair'], 'of'), (['of', 'child'], 'mine'), (['mine', 'of'], 'Shall'), (['Shall', 'mine'], 'sum'), (['sum', 'Shall'], 'my'), (['my', 'sum'], 'count,'), (['count,', 'my'], 'and'), (['and', 'count,'], 'make'), (['make', 'and'], 'my'), (['my', 'make'], 'old'), (['old', 'my'], \"excuse,'\"), ([\"excuse,'\", 'old'], 'Proving'), (['Proving', \"excuse,'\"], 'his'), (['his', 'Proving'], 'beauty'), (['beauty', 'his'], 'by'), (['by', 'beauty'], 'succession'), (['succession', 'by'], 'thine!'), (['thine!', 'succession'], 'This'), (['This', 'thine!'], 'were'), (['were', 'This'], 'to'), (['to', 'were'], 'be'), (['be', 'to'], 'new'), (['new', 'be'], 'made'), (['made', 'new'], 'when'), (['when', 'made'], 'thou'), (['thou', 'when'], 'art'), (['art', 'thou'], 'old,'), (['old,', 'art'], 'And'), (['And', 'old,'], 'see'), (['see', 'And'], 'thy'), (['thy', 'see'], 'blood'), (['blood', 'thy'], 'warm'), (['warm', 'blood'], 'when'), (['when', 'warm'], 'thou'), (['thou', 'when'], \"feel'st\"), ([\"feel'st\", 'thou'], 'it'), (['it', \"feel'st\"], 'cold.')]\n"]}],"source":["print(ngrams)"]},{"cell_type":"markdown","metadata":{"id":"aJZR1ZgB5GiV"},"source":["## PyTorch tutorial\n","\n","[LANGUAGE MODELING WITH NN.TRANSFORMER AND TORCHTEXT](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN2YK3Lf3xE+u3t22CB8A+s","name":"DL11.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
