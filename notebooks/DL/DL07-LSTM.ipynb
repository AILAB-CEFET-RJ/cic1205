{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL07-LSTM.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN/eZ5oi50hEJ8I3MuvN+4k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Nberz5mo2rgf"},"source":["## LSTMs"]},{"cell_type":"markdown","metadata":{"id":"28K3b1p-2wMf"},"source":["## Demo 1 - torch.nn.LSTM\n","\n","No PytTorch, a classe [torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) prov√™ a implementa√ß√£o de uma c√©lula LSTM. Essa classe implementa as seguintes equa√ß√µes:\n","\n","![texto do link](https://i.imgur.com/A9reTrh.png)\n","\n","Da documenta√ß√£o do PyTorch:\n","\n","- `input_size` ‚Äì The number of expected features in the input x\n","- `hidden_size` ‚Äì The number of features in the hidden state h\n","- `num_layers` ‚Äì Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n","\n","Como exemplo, considere uma arquitetura LSTM em que foram definidos:\n","- $c_ùë° \\in \\mathbb{R}^2$\n","- $‚Ñé_ùë° \\in \\mathbb{R}^2$\n","- $ùë•_ùë° \\in \\mathbb{R}^3$\n","\n","![texto do link](https://i.imgur.com/Ifn6yVs.png)\n","\n","Cada camada sigmoid, tanh ou de estado oculto em uma c√©lula LSTM √© na verdade uma rede neural feed forward de uma √∫nica camada oculta, cujo n√∫mero de neur√¥nios √© definido pelo par√¢metro `hidden_size`. \n","\n","- Se definirmos `hidden_size = 2`, cada c√©lula LSTM ter√° redes neurais com 2 neur√¥nios em sua camada oculta.\n","\n","- Se definirmos `input_size = 3`, cada elemento da sequ√™ncia de entrada ser√° representado por um vetor de tr√™s dimens√µes."]},{"cell_type":"code","metadata":{"id":"ntwwLXLU2ybc","executionInfo":{"status":"ok","timestamp":1646352844499,"user_tz":180,"elapsed":2,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}}},"source":["import torch.nn as nn\n","hidden_size = 2\n","input_size = 3\n","lstm1 = nn.LSTM(input_size, hidden_size, num_layers=1)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zbtOE6Ifv652"},"source":["Uma c√©lula LSTM possui as seguintes propriedades (texto a seguir tamb√©m retirado da [documenta√ß√£o do PyTorch](https://pytorch.org/docs/stable/nn.html)):\n","\n","- weight_ih_l[k] ‚Äì the learnable input-hidden weights of the $\\text{k}^{th}$ ($W_{ii}|W_{if}|W_{ig}|W_{io}$), of shape (4$\\times$hidden_size, input_size) for $k = 0$.\n","\n","- weight_hh_l[k] ‚Äì the learnable hidden-hidden weights of the $\\text{k}^{th}k$ layer ($W_{hi}|W_{hf}|W_{hg}|W_{ho}$), of shape (4*hidden_size, hidden_size)\n"]},{"cell_type":"markdown","source":["Para a c√©lula LSTM apresentada nesta se√ß√£o, valem as seguintes dimens√µes:\n","\n","$$\n","W_{ii}, W_{if}, W_{ig}, W_{io} \\in \\mathbb{R}^{2 \\times 3}\n","$$"],"metadata":{"id":"3R2Uv0RHUjQE"}},{"cell_type":"code","metadata":{"id":"xQ-Qkp7Q3KJ3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646352849746,"user_tz":180,"elapsed":265,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}},"outputId":"49e4e34e-8d32-47bc-8761-81780c14bd2d"},"source":["print(lstm1.weight_ih_l0.size())\n","print(lstm1.weight_ih_l0)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 3])\n","Parameter containing:\n","tensor([[-0.1932, -0.5081,  0.0987],\n","        [-0.0793,  0.5171, -0.2966],\n","        [-0.1412,  0.5200, -0.3319],\n","        [ 0.4398, -0.1529, -0.0209],\n","        [ 0.5111, -0.3910,  0.1674],\n","        [ 0.0869,  0.4588,  0.3659],\n","        [-0.5566, -0.5922, -0.2575],\n","        [-0.4459,  0.6885, -0.0592]], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["Para a c√©lula LSTM apresentada nesta se√ß√£o, valem as seguintes dimens√µes:\n","\n","$$\n","W_{hi}, W_{hf}, W_{hg}, W_{ho} \\in \\mathbb{R}^{2 \\times 2}\n","$$"],"metadata":{"id":"e6F8h2A9UKR3"}},{"cell_type":"code","metadata":{"id":"2y2AiqXXrTSL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646352863764,"user_tz":180,"elapsed":259,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}},"outputId":"fa61845e-ed0a-47c1-aefb-2ef9c1974a20"},"source":["print(lstm1.weight_hh_l0.size())\n","print(lstm1.weight_hh_l0)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 2])\n","Parameter containing:\n","tensor([[-0.6337,  0.1353],\n","        [ 0.3896, -0.4928],\n","        [-0.2574,  0.5458],\n","        [ 0.4612,  0.3946],\n","        [-0.4717, -0.0333],\n","        [-0.4188, -0.2073],\n","        [-0.3588,  0.6900],\n","        [ 0.6157, -0.0471]], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["## Demo 2 - Previs√£o de s√©ries multivariadas\n","\n","**Cr√©ditos**: o exemplo desta se√ß√£o foi adaptado do encontrado em [Multivariate input LSTM in pytorch](https://stackoverflow.com/questions/56858924/multivariate-input-lstm-in-pytorch).\n","\n","Esta se√ß√£o apresenta um exemplo muito simples de treinamento de um modelo de rede LSTM para predi√ß√£o no contexto de uma s√©rie temporal multivariada.\n","\n","> Considere uma s√©rie temporal multivariada na qual h√° tr√™s observa√ß√µes (vari√°veis medidas) por passo de tempo. Dessas tr√™s vari√°veis, uma dela √© alvo (i.e., a que desejamos predizer). As outras duas ser√£o usadas como vari√°veis independentes. Considere tamb√©m a tarefa de ajustar um modelo de predi√ß√£o no qual s√£o usadas observa√ß√µes de tr√™s passos de tempo no passado para predizer o valor da vari√°vel de interesse no passo de tempo atual."],"metadata":{"id":"BVIQ0iBGwcmh"}},{"cell_type":"markdown","source":["### Prepara√ß√£o dos dados\n","\n","Vamos de in√≠cio gerar um conjunto de dados sint√©tico. Esse conjunto de dados simula uma s√©rie temporal multivariada na qual, a cada passo de tempo s√£o observadas (medidas) tr√™s vari√°veis."],"metadata":{"id":"eNgBgB1yaUsA"}},{"cell_type":"code","source":["import random\n","import numpy as np\n","import torch\n","\n","# multivariate data preparation\n","from numpy import array\n","from numpy import hstack\n"," \n","# split a multivariate sequence into samples\n","def split_sequences(sequences, n_steps):\n","    X, y = list(), list()\n","    for i in range(len(sequences)):\n","        # find the end of this pattern\n","        end_ix = i + n_steps\n","        # check if we are beyond the dataset\n","        if end_ix > len(sequences):\n","            break\n","        # gather input and output parts of the pattern\n","        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n","        X.append(seq_x)\n","        y.append(seq_y)\n","    return array(X), array(y)\n"," \n","# define input sequence\n","in_seq1 = array([x for x in range(0,100,10)])\n","in_seq2 = array([x for x in range(5,105,10)])\n","out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n","# convert to [rows, columns] structure\n","in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n","in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n","out_seq = out_seq.reshape((len(out_seq), 1))\n","# horizontally stack columns\n","dataset = hstack((in_seq1, in_seq2, out_seq))"],"metadata":{"id":"O41vCa75aSMk","executionInfo":{"status":"ok","timestamp":1646349249494,"user_tz":180,"elapsed":12,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["A s√©rie temporal multivariada gerada pelo c√≥digo acima √© apresentada a seguir. Repare que h√° 10 passos de tempo, em cada um dos quais, tr√™s observa√ß√µes s√£o realizadas."],"metadata":{"id":"XijtSCGMxvRK"}},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jn_yHP-k1GIZ","executionInfo":{"status":"ok","timestamp":1646349249494,"user_tz":180,"elapsed":11,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}},"outputId":"7e733587-c3bf-4ec3-ad15-9c7efd6567d8"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  0,   5,   5],\n","       [ 10,  15,  25],\n","       [ 20,  25,  45],\n","       [ 30,  35,  65],\n","       [ 40,  45,  85],\n","       [ 50,  55, 105],\n","       [ 60,  65, 125],\n","       [ 70,  75, 145],\n","       [ 80,  85, 165],\n","       [ 90,  95, 185]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["n_timesteps = 3 # this is number of timesteps"],"metadata":{"id":"krQop2zi0Zrj","executionInfo":{"status":"ok","timestamp":1646349249495,"user_tz":180,"elapsed":11,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# convert dataset into input/output\n","X, y = split_sequences(dataset, n_timesteps)\n","print(X.shape, y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IF-5KFrT0UhJ","executionInfo":{"status":"ok","timestamp":1646349249495,"user_tz":180,"elapsed":10,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}},"outputId":"4a95d9ce-2804-4902-a126-06b5f3fffdd5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(8, 3, 2) (8,)\n"]}]},{"cell_type":"code","source":["print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yg4sbhbf0cJl","executionInfo":{"status":"ok","timestamp":1646349249495,"user_tz":180,"elapsed":9,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}},"outputId":"156b388f-02be-4bba-b3ea-374fe6b04bba"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[ 0  5]\n","  [10 15]\n","  [20 25]]\n","\n"," [[10 15]\n","  [20 25]\n","  [30 35]]\n","\n"," [[20 25]\n","  [30 35]\n","  [40 45]]\n","\n"," [[30 35]\n","  [40 45]\n","  [50 55]]\n","\n"," [[40 45]\n","  [50 55]\n","  [60 65]]\n","\n"," [[50 55]\n","  [60 65]\n","  [70 75]]\n","\n"," [[60 65]\n","  [70 75]\n","  [80 85]]\n","\n"," [[70 75]\n","  [80 85]\n","  [90 95]]]\n"]}]},{"cell_type":"code","source":["print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAtdtm9F0dbE","executionInfo":{"status":"ok","timestamp":1646349249496,"user_tz":180,"elapsed":9,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}},"outputId":"a08d5b48-fe79-4b1c-d58f-016641a1ad1c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 45  65  85 105 125 145 165 185]\n"]}]},{"cell_type":"markdown","source":["### Constru√ß√£o do modelo\n","\n","Esta se√ß√£o apresenta a implemeta√ß√£o do modelo em PyTorch. Como de praxe, a classe correspondente √© derivada de `torch.nn.Module`."],"metadata":{"id":"hTUFX3lMaWiA"}},{"cell_type":"code","source":["class MV_LSTM(torch.nn.Module):\n","    def __init__(self,n_features,seq_length):\n","        super(MV_LSTM, self).__init__()\n","        self.n_features = n_features\n","        self.seq_len = seq_length\n","        self.n_hidden = 20 # number of hidden states\n","        self.n_layers = 1 # number of LSTM layers (stacked)\n","    \n","        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n","                                 hidden_size = self.n_hidden,\n","                                 num_layers = self.n_layers, \n","                                 batch_first = True)\n","        # according to pytorch docs LSTM output is \n","        # (batch_size,seq_len, num_directions * hidden_size)\n","        # when considering batch_first = True\n","        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n","        \n","    \n","    def init_hidden(self, batch_size):\n","        # even with batch_first = True this remains same as docs\n","        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n","        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n","        self.hidden = (hidden_state, cell_state)\n","    \n","    \n","    def forward(self, x):        \n","        batch_size, seq_len, _ = x.size()\n","        \n","        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n","        # lstm_out(with batch_first = True) is \n","        # (batch_size,seq_len,num_directions * hidden_size)\n","        # for following linear layer we want to keep batch_size dimension and merge rest       \n","        # .contiguous() -> solves tensor compatibility error\n","        x = lstm_out.contiguous().view(batch_size,-1)\n","        return self.l_linear(x)"],"metadata":{"id":"Y4ToGcmiaZt2","executionInfo":{"status":"ok","timestamp":1646349249496,"user_tz":180,"elapsed":8,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Configura√ß√£o"],"metadata":{"id":"iuuL2Y92afmO"}},{"cell_type":"code","source":["n_features = 2 # this is number of parallel inputs\n","\n","# create NN\n","mv_net = MV_LSTM(n_features, n_timesteps)\n","criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n","optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n","\n","train_episodes = 500\n","batch_size = 16"],"metadata":{"id":"vmw-WNSSaib1","executionInfo":{"status":"ok","timestamp":1646349249496,"user_tz":180,"elapsed":8,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### Treinamento"],"metadata":{"id":"LuSNtTxVak-a"}},{"cell_type":"code","source":["mv_net.train()\n","for t in range(train_episodes):\n","    for b in range(0,len(X),batch_size):\n","        inpt = X[b:b+batch_size,:,:]\n","        target = y[b:b+batch_size]    \n","        \n","        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n","        y_batch = torch.tensor(target,dtype=torch.float32)\n","    \n","        mv_net.init_hidden(x_batch.size(0))\n","\n","        output = mv_net(x_batch) \n","        loss = criterion(output.view(-1), y_batch)  \n","        \n","        loss.backward()\n","        optimizer.step()        \n","        optimizer.zero_grad() \n","    print('step : ' , t , 'loss : ' , loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMwc8JnWapOk","executionInfo":{"status":"ok","timestamp":1646349251667,"user_tz":180,"elapsed":2178,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"}},"outputId":"2b5d86f9-fe9c-4648-88cf-6ff13c6dad77"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["step :  0 loss :  15388.94921875\n","step :  1 loss :  15017.09765625\n","step :  2 loss :  14673.6796875\n","step :  3 loss :  14356.7880859375\n","step :  4 loss :  14033.16796875\n","step :  5 loss :  13554.828125\n","step :  6 loss :  12812.4365234375\n","step :  7 loss :  12329.8701171875\n","step :  8 loss :  11862.169921875\n","step :  9 loss :  11422.3623046875\n","step :  10 loss :  10983.6669921875\n","step :  11 loss :  10548.66015625\n","step :  12 loss :  10119.884765625\n","step :  13 loss :  9699.3212890625\n","step :  14 loss :  9288.4794921875\n","step :  15 loss :  8887.8359375\n","step :  16 loss :  8496.3037109375\n","step :  17 loss :  8102.515625\n","step :  18 loss :  7546.8466796875\n","step :  19 loss :  7158.310546875\n","step :  20 loss :  6808.65234375\n","step :  21 loss :  6470.984375\n","step :  22 loss :  6146.25634765625\n","step :  23 loss :  5833.763671875\n","step :  24 loss :  5506.349609375\n","step :  25 loss :  5056.294921875\n","step :  26 loss :  4779.8896484375\n","step :  27 loss :  4517.98779296875\n","step :  28 loss :  4270.06640625\n","step :  29 loss :  4037.13525390625\n","step :  30 loss :  3819.69580078125\n","step :  31 loss :  3617.933837890625\n","step :  32 loss :  3431.79931640625\n","step :  33 loss :  3261.0673828125\n","step :  34 loss :  3105.373291015625\n","step :  35 loss :  2964.23876953125\n","step :  36 loss :  2837.09521484375\n","step :  37 loss :  2723.2998046875\n","step :  38 loss :  2622.14892578125\n","step :  39 loss :  2532.89990234375\n","step :  40 loss :  2454.77294921875\n","step :  41 loss :  2386.96728515625\n","step :  42 loss :  2328.6669921875\n","step :  43 loss :  2279.04248046875\n","step :  44 loss :  2237.20703125\n","step :  45 loss :  2227.4931640625\n","step :  46 loss :  2174.30029296875\n","step :  47 loss :  2151.86181640625\n","step :  48 loss :  2134.210205078125\n","step :  49 loss :  2120.75732421875\n","step :  50 loss :  2110.89111328125\n","step :  51 loss :  2104.032470703125\n","step :  52 loss :  2099.6416015625\n","step :  53 loss :  2097.216552734375\n","step :  54 loss :  2096.2734375\n","step :  55 loss :  2096.234619140625\n","step :  56 loss :  2096.126220703125\n","step :  57 loss :  2093.421875\n","step :  58 loss :  2078.8876953125\n","step :  59 loss :  2030.19677734375\n","step :  60 loss :  1963.0789794921875\n","step :  61 loss :  1901.2586669921875\n","step :  62 loss :  2249.031005859375\n","step :  63 loss :  1655.837646484375\n","step :  64 loss :  1927.153564453125\n","step :  65 loss :  1918.0810546875\n","step :  66 loss :  1894.719482421875\n","step :  67 loss :  1849.696044921875\n","step :  68 loss :  1774.112548828125\n","step :  69 loss :  1704.58935546875\n","step :  70 loss :  2029.498046875\n","step :  71 loss :  1823.81640625\n","step :  72 loss :  1688.620361328125\n","step :  73 loss :  1747.543701171875\n","step :  74 loss :  1766.8746337890625\n","step :  75 loss :  1765.908935546875\n","step :  76 loss :  1706.9197998046875\n","step :  77 loss :  1936.450439453125\n","step :  78 loss :  1656.460693359375\n","step :  79 loss :  1688.760498046875\n","step :  80 loss :  1687.801025390625\n","step :  81 loss :  1617.33447265625\n","step :  82 loss :  1721.91455078125\n","step :  83 loss :  1621.080810546875\n","step :  84 loss :  1730.5107421875\n","step :  85 loss :  1742.1766357421875\n","step :  86 loss :  1769.143310546875\n","step :  87 loss :  1768.30078125\n","step :  88 loss :  1762.4066162109375\n","step :  89 loss :  1746.0146484375\n","step :  90 loss :  1684.1961669921875\n","step :  91 loss :  1981.895263671875\n","step :  92 loss :  1633.447998046875\n","step :  93 loss :  1673.190673828125\n","step :  94 loss :  1691.409912109375\n","step :  95 loss :  1694.239990234375\n","step :  96 loss :  1690.124267578125\n","step :  97 loss :  1684.024658203125\n","step :  98 loss :  1677.347412109375\n","step :  99 loss :  1670.2783203125\n","step :  100 loss :  1662.604736328125\n","step :  101 loss :  1654.0594482421875\n","step :  102 loss :  1644.6307373046875\n","step :  103 loss :  1634.819091796875\n","step :  104 loss :  1625.454345703125\n","step :  105 loss :  1617.0498046875\n","step :  106 loss :  1609.3399658203125\n","step :  107 loss :  1601.308349609375\n","step :  108 loss :  1591.236083984375\n","step :  109 loss :  1576.382568359375\n","step :  110 loss :  1551.0401611328125\n","step :  111 loss :  1500.4600830078125\n","step :  112 loss :  1437.718994140625\n","step :  113 loss :  1334.009033203125\n","step :  114 loss :  1653.819580078125\n","step :  115 loss :  1640.1480712890625\n","step :  116 loss :  1625.1351318359375\n","step :  117 loss :  1609.165771484375\n","step :  118 loss :  1587.60009765625\n","step :  119 loss :  1554.59326171875\n","step :  120 loss :  1507.14453125\n","step :  121 loss :  1460.402099609375\n","step :  122 loss :  1400.6417236328125\n","step :  123 loss :  1376.859130859375\n","step :  124 loss :  1446.4521484375\n","step :  125 loss :  1323.82373046875\n","step :  126 loss :  1306.859375\n","step :  127 loss :  1311.52099609375\n","step :  128 loss :  1306.3515625\n","step :  129 loss :  1280.503173828125\n","step :  130 loss :  1242.056884765625\n","step :  131 loss :  1197.2724609375\n","step :  132 loss :  1149.6337890625\n","step :  133 loss :  1124.587646484375\n","step :  134 loss :  1103.7088623046875\n","step :  135 loss :  1040.42724609375\n","step :  136 loss :  959.5320434570312\n","step :  137 loss :  906.4507446289062\n","step :  138 loss :  860.4983520507812\n","step :  139 loss :  915.711181640625\n","step :  140 loss :  804.189697265625\n","step :  141 loss :  836.9032592773438\n","step :  142 loss :  847.052490234375\n","step :  143 loss :  828.2620849609375\n","step :  144 loss :  802.150146484375\n","step :  145 loss :  782.572509765625\n","step :  146 loss :  751.9141235351562\n","step :  147 loss :  680.2766723632812\n","step :  148 loss :  610.1307373046875\n","step :  149 loss :  621.64892578125\n","step :  150 loss :  609.0863037109375\n","step :  151 loss :  586.7630615234375\n","step :  152 loss :  607.0469970703125\n","step :  153 loss :  606.05615234375\n","step :  154 loss :  586.2801513671875\n","step :  155 loss :  553.250732421875\n","step :  156 loss :  549.4560546875\n","step :  157 loss :  532.490966796875\n","step :  158 loss :  516.88330078125\n","step :  159 loss :  510.3660888671875\n","step :  160 loss :  491.5909118652344\n","step :  161 loss :  480.5653381347656\n","step :  162 loss :  479.64410400390625\n","step :  163 loss :  479.2093200683594\n","step :  164 loss :  456.7509765625\n","step :  165 loss :  449.81378173828125\n","step :  166 loss :  454.729248046875\n","step :  167 loss :  447.0002136230469\n","step :  168 loss :  434.1419982910156\n","step :  169 loss :  427.44940185546875\n","step :  170 loss :  423.258056640625\n","step :  171 loss :  414.57965087890625\n","step :  172 loss :  409.2603759765625\n","step :  173 loss :  407.3575439453125\n","step :  174 loss :  402.33056640625\n","step :  175 loss :  394.01898193359375\n","step :  176 loss :  387.4513244628906\n","step :  177 loss :  383.65478515625\n","step :  178 loss :  378.4219665527344\n","step :  179 loss :  373.42022705078125\n","step :  180 loss :  369.7373962402344\n","step :  181 loss :  364.9324951171875\n","step :  182 loss :  358.67626953125\n","step :  183 loss :  353.0311279296875\n","step :  184 loss :  349.02313232421875\n","step :  185 loss :  345.36090087890625\n","step :  186 loss :  339.84588623046875\n","step :  187 loss :  333.25421142578125\n","step :  188 loss :  324.17315673828125\n","step :  189 loss :  310.73651123046875\n","step :  190 loss :  297.7303771972656\n","step :  191 loss :  274.9453430175781\n","step :  192 loss :  226.49429321289062\n","step :  193 loss :  513.7142944335938\n","step :  194 loss :  480.0695495605469\n","step :  195 loss :  485.926025390625\n","step :  196 loss :  495.9635925292969\n","step :  197 loss :  493.481201171875\n","step :  198 loss :  473.90325927734375\n","step :  199 loss :  446.5927429199219\n","step :  200 loss :  436.1163024902344\n","step :  201 loss :  445.582763671875\n","step :  202 loss :  426.39031982421875\n","step :  203 loss :  402.70404052734375\n","step :  204 loss :  405.08502197265625\n","step :  205 loss :  401.17022705078125\n","step :  206 loss :  385.78570556640625\n","step :  207 loss :  365.3812255859375\n","step :  208 loss :  367.2973327636719\n","step :  209 loss :  359.95013427734375\n","step :  210 loss :  343.45849609375\n","step :  211 loss :  337.146484375\n","step :  212 loss :  335.6197814941406\n","step :  213 loss :  320.4307861328125\n","step :  214 loss :  314.54278564453125\n","step :  215 loss :  313.2733154296875\n","step :  216 loss :  303.32537841796875\n","step :  217 loss :  295.8422546386719\n","step :  218 loss :  294.15167236328125\n","step :  219 loss :  287.8428955078125\n","step :  220 loss :  281.515380859375\n","step :  221 loss :  279.383056640625\n","step :  222 loss :  273.5450439453125\n","step :  223 loss :  268.957763671875\n","step :  224 loss :  267.05499267578125\n","step :  225 loss :  262.7713928222656\n","step :  226 loss :  257.6089172363281\n","step :  227 loss :  255.48214721679688\n","step :  228 loss :  252.17221069335938\n","step :  229 loss :  246.93467712402344\n","step :  230 loss :  243.3665313720703\n","step :  231 loss :  238.42465209960938\n","step :  232 loss :  229.32691955566406\n","step :  233 loss :  218.28524780273438\n","step :  234 loss :  206.178466796875\n","step :  235 loss :  192.40548706054688\n","step :  236 loss :  168.96913146972656\n","step :  237 loss :  155.7784423828125\n","step :  238 loss :  242.29638671875\n","step :  239 loss :  200.41969299316406\n","step :  240 loss :  224.447265625\n","step :  241 loss :  210.9683837890625\n","step :  242 loss :  255.40708923339844\n","step :  243 loss :  231.94076538085938\n","step :  244 loss :  233.40061950683594\n","step :  245 loss :  244.8619842529297\n","step :  246 loss :  232.08425903320312\n","step :  247 loss :  225.38922119140625\n","step :  248 loss :  239.1119384765625\n","step :  249 loss :  223.01841735839844\n","step :  250 loss :  236.3563232421875\n","step :  251 loss :  229.58827209472656\n","step :  252 loss :  217.94447326660156\n","step :  253 loss :  231.539306640625\n","step :  254 loss :  215.8192138671875\n","step :  255 loss :  219.10275268554688\n","step :  256 loss :  217.87747192382812\n","step :  257 loss :  210.580810546875\n","step :  258 loss :  215.67425537109375\n","step :  259 loss :  208.24441528320312\n","step :  260 loss :  213.09912109375\n","step :  261 loss :  206.9866943359375\n","step :  262 loss :  208.83741760253906\n","step :  263 loss :  204.22979736328125\n","step :  264 loss :  206.40432739257812\n","step :  265 loss :  201.75445556640625\n","step :  266 loss :  205.49273681640625\n","step :  267 loss :  199.93072509765625\n","step :  268 loss :  201.077392578125\n","step :  269 loss :  197.9512481689453\n","step :  270 loss :  199.8365478515625\n","step :  271 loss :  196.2183837890625\n","step :  272 loss :  196.4539794921875\n","step :  273 loss :  193.8253631591797\n","step :  274 loss :  193.56503295898438\n","step :  275 loss :  192.65904235839844\n","step :  276 loss :  190.80801391601562\n","step :  277 loss :  191.3094482421875\n","step :  278 loss :  188.45973205566406\n","step :  279 loss :  189.12997436523438\n","step :  280 loss :  186.93992614746094\n","step :  281 loss :  186.3209228515625\n","step :  282 loss :  185.50576782226562\n","step :  283 loss :  183.67816162109375\n","step :  284 loss :  183.61074829101562\n","step :  285 loss :  181.7900390625\n","step :  286 loss :  181.193603515625\n","step :  287 loss :  180.3726348876953\n","step :  288 loss :  178.86402893066406\n","step :  289 loss :  178.42300415039062\n","step :  290 loss :  177.25057983398438\n","step :  291 loss :  176.1379852294922\n","step :  292 loss :  175.5540771484375\n","step :  293 loss :  174.35568237304688\n","step :  294 loss :  173.41128540039062\n","step :  295 loss :  172.7183837890625\n","step :  296 loss :  171.59805297851562\n","step :  297 loss :  170.67852783203125\n","step :  298 loss :  169.9551544189453\n","step :  299 loss :  168.93434143066406\n","step :  300 loss :  168.00018310546875\n","step :  301 loss :  167.26773071289062\n","step :  302 loss :  166.32342529296875\n","step :  303 loss :  165.3711700439453\n","step :  304 loss :  164.62594604492188\n","step :  305 loss :  163.75137329101562\n","step :  306 loss :  162.78756713867188\n","step :  307 loss :  162.0072021484375\n","step :  308 loss :  161.21377563476562\n","step :  309 loss :  160.28555297851562\n","step :  310 loss :  159.43362426757812\n","step :  311 loss :  158.66685485839844\n","step :  312 loss :  157.8379669189453\n","step :  313 loss :  156.95721435546875\n","step :  314 loss :  156.12835693359375\n","step :  315 loss :  155.355712890625\n","step :  316 loss :  154.54649353027344\n","step :  317 loss :  153.69227600097656\n","step :  318 loss :  152.87619018554688\n","step :  319 loss :  152.09829711914062\n","step :  320 loss :  151.30404663085938\n","step :  321 loss :  150.48291015625\n","step :  322 loss :  149.6659698486328\n","step :  323 loss :  148.88021850585938\n","step :  324 loss :  148.097412109375\n","step :  325 loss :  147.29055786132812\n","step :  326 loss :  146.4750213623047\n","step :  327 loss :  145.66661071777344\n","step :  328 loss :  144.8690185546875\n","step :  329 loss :  144.0654296875\n","step :  330 loss :  143.2410888671875\n","step :  331 loss :  142.400146484375\n","step :  332 loss :  141.54519653320312\n","step :  333 loss :  140.67770385742188\n","step :  334 loss :  139.78814697265625\n","step :  335 loss :  138.86036682128906\n","step :  336 loss :  137.8790283203125\n","step :  337 loss :  136.82369995117188\n","step :  338 loss :  135.67169189453125\n","step :  339 loss :  134.3858642578125\n","step :  340 loss :  132.9095458984375\n","step :  341 loss :  131.16017150878906\n","step :  342 loss :  129.02914428710938\n","step :  343 loss :  126.4056396484375\n","step :  344 loss :  123.24970245361328\n","step :  345 loss :  119.72657775878906\n","step :  346 loss :  116.25746154785156\n","step :  347 loss :  113.30450439453125\n","step :  348 loss :  111.05815124511719\n","step :  349 loss :  109.41960144042969\n","step :  350 loss :  108.18522644042969\n","step :  351 loss :  107.17625427246094\n","step :  352 loss :  106.26593017578125\n","step :  353 loss :  105.37501525878906\n","step :  354 loss :  104.4656982421875\n","step :  355 loss :  103.54623413085938\n","step :  356 loss :  102.6658935546875\n","step :  357 loss :  101.87025451660156\n","step :  358 loss :  101.13783264160156\n","step :  359 loss :  100.3606185913086\n","step :  360 loss :  99.41368865966797\n","step :  361 loss :  98.1359634399414\n","step :  362 loss :  96.46287536621094\n","step :  363 loss :  94.157958984375\n","step :  364 loss :  92.03834533691406\n","step :  365 loss :  88.74398803710938\n","step :  366 loss :  85.80809020996094\n","step :  367 loss :  80.15486145019531\n","step :  368 loss :  74.60466766357422\n","step :  369 loss :  73.41193389892578\n","step :  370 loss :  74.4587173461914\n","step :  371 loss :  72.4962387084961\n","step :  372 loss :  67.97835540771484\n","step :  373 loss :  66.69996643066406\n","step :  374 loss :  69.06621551513672\n","step :  375 loss :  69.3309326171875\n","step :  376 loss :  65.83558654785156\n","step :  377 loss :  62.52610778808594\n","step :  378 loss :  61.267974853515625\n","step :  379 loss :  61.98396682739258\n","step :  380 loss :  61.4847297668457\n","step :  381 loss :  59.95214080810547\n","step :  382 loss :  58.31343078613281\n","step :  383 loss :  57.7271728515625\n","step :  384 loss :  56.80461502075195\n","step :  385 loss :  55.45862579345703\n","step :  386 loss :  54.74304962158203\n","step :  387 loss :  54.63243865966797\n","step :  388 loss :  52.96796798706055\n","step :  389 loss :  52.3779296875\n","step :  390 loss :  52.20452117919922\n","step :  391 loss :  51.5682487487793\n","step :  392 loss :  50.68162536621094\n","step :  393 loss :  49.931983947753906\n","step :  394 loss :  49.587589263916016\n","step :  395 loss :  49.09654235839844\n","step :  396 loss :  48.58475112915039\n","step :  397 loss :  47.97734832763672\n","step :  398 loss :  47.46073913574219\n","step :  399 loss :  46.78559494018555\n","step :  400 loss :  46.15250778198242\n","step :  401 loss :  45.88161087036133\n","step :  402 loss :  45.34589385986328\n","step :  403 loss :  44.58660125732422\n","step :  404 loss :  44.27750778198242\n","step :  405 loss :  43.83897399902344\n","step :  406 loss :  43.34048080444336\n","step :  407 loss :  42.944976806640625\n","step :  408 loss :  42.323089599609375\n","step :  409 loss :  41.976463317871094\n","step :  410 loss :  41.66729736328125\n","step :  411 loss :  41.180206298828125\n","step :  412 loss :  40.790828704833984\n","step :  413 loss :  40.42150115966797\n","step :  414 loss :  40.090782165527344\n","step :  415 loss :  39.76715850830078\n","step :  416 loss :  39.366817474365234\n","step :  417 loss :  39.05543518066406\n","step :  418 loss :  38.73527526855469\n","step :  419 loss :  38.43038558959961\n","step :  420 loss :  38.124149322509766\n","step :  421 loss :  37.802433013916016\n","step :  422 loss :  37.55956268310547\n","step :  423 loss :  37.251319885253906\n","step :  424 loss :  36.95682144165039\n","step :  425 loss :  36.6644401550293\n","step :  426 loss :  36.40361785888672\n","step :  427 loss :  36.149993896484375\n","step :  428 loss :  35.88397979736328\n","step :  429 loss :  35.717071533203125\n","step :  430 loss :  35.79095458984375\n","step :  431 loss :  36.370330810546875\n","step :  432 loss :  38.94906234741211\n","step :  433 loss :  35.04141616821289\n","step :  434 loss :  34.61375045776367\n","step :  435 loss :  37.42736053466797\n","step :  436 loss :  38.18534851074219\n","step :  437 loss :  41.613521575927734\n","step :  438 loss :  36.678009033203125\n","step :  439 loss :  56.38648223876953\n","step :  440 loss :  69.51435089111328\n","step :  441 loss :  35.375511169433594\n","step :  442 loss :  124.54753875732422\n","step :  443 loss :  169.67529296875\n","step :  444 loss :  216.26272583007812\n","step :  445 loss :  196.3256072998047\n","step :  446 loss :  62.87554931640625\n","step :  447 loss :  91.49623107910156\n","step :  448 loss :  115.96853637695312\n","step :  449 loss :  74.2612533569336\n","step :  450 loss :  64.89852142333984\n","step :  451 loss :  70.70569610595703\n","step :  452 loss :  70.41584777832031\n","step :  453 loss :  49.833351135253906\n","step :  454 loss :  72.13915252685547\n","step :  455 loss :  64.21490478515625\n","step :  456 loss :  40.25544738769531\n","step :  457 loss :  40.42735290527344\n","step :  458 loss :  34.962894439697266\n","step :  459 loss :  39.33327102661133\n","step :  460 loss :  38.98529815673828\n","step :  461 loss :  36.013221740722656\n","step :  462 loss :  41.009220123291016\n","step :  463 loss :  36.33747863769531\n","step :  464 loss :  42.855777740478516\n","step :  465 loss :  35.42933654785156\n","step :  466 loss :  37.2182502746582\n","step :  467 loss :  31.624391555786133\n","step :  468 loss :  30.231952667236328\n","step :  469 loss :  29.86046600341797\n","step :  470 loss :  29.698442459106445\n","step :  471 loss :  30.141056060791016\n","step :  472 loss :  27.833858489990234\n","step :  473 loss :  30.00086212158203\n","step :  474 loss :  27.748916625976562\n","step :  475 loss :  27.361278533935547\n","step :  476 loss :  27.42145538330078\n","step :  477 loss :  26.745607376098633\n","step :  478 loss :  26.525188446044922\n","step :  479 loss :  27.080459594726562\n","step :  480 loss :  26.749313354492188\n","step :  481 loss :  25.9110107421875\n","step :  482 loss :  25.645469665527344\n","step :  483 loss :  25.251914978027344\n","step :  484 loss :  25.32394027709961\n","step :  485 loss :  25.180831909179688\n","step :  486 loss :  25.20085906982422\n","step :  487 loss :  24.92574691772461\n","step :  488 loss :  25.035799026489258\n","step :  489 loss :  24.547183990478516\n","step :  490 loss :  24.537166595458984\n","step :  491 loss :  24.27911376953125\n","step :  492 loss :  23.93885040283203\n","step :  493 loss :  23.871234893798828\n","step :  494 loss :  23.65041732788086\n","step :  495 loss :  23.578216552734375\n","step :  496 loss :  23.490520477294922\n","step :  497 loss :  23.484050750732422\n","step :  498 loss :  23.1968994140625\n","step :  499 loss :  23.086959838867188\n"]}]}]}