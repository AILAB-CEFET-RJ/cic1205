{"cells":[{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[],"source":["#pip install torch"]},{"cell_type":"markdown","metadata":{"id":"QbFUp2989zUH"},"source":["# PyTorch: conceitos básicos\n","\n","Em essência, uma rede neural artificial é uma coleção de funções aninhadas que são executadas em alguns dados de entrada. Essas funções são definidas por parâmetros (os pesos e vieses), que no PyTorch são armazenados em **tensores**.\n","\n","O PyTorch é uma biblioteca que nos permite manipular tensores. Tensor é um nome genérico usado para denotar vários objetos matemáticos: um único número, um vetor, uma matrix, etc. Em geral, vamos chamar de tensor qualquer array $n$-dimensional de números ($n \\geq 0$). \n","\n","Uma propriedade importante de tensores é que todos os seus números componentes pertencem ao mesmo tipo de dados."]},{"cell_type":"code","execution_count":207,"metadata":{"id":"T1QiLXov92YM"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","metadata":{"id":"KLr_9ee0Wyl7"},"source":["No exemplo abaixo, definimos nosso primeiro tensor. A expressão \"42.\" é uma abreviatura para 42.0. É usada para indicar ao Python (e ao PyTorch) que você deseja criar um número de ponto flutuante."]},{"cell_type":"code","execution_count":208,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":255,"status":"ok","timestamp":1638380071906,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"RlabyKUk98UC","outputId":"30a1bfdd-d092-4b1e-a469-0ff98a884892"},"outputs":[{"data":{"text/plain":["tensor(42.)"]},"execution_count":208,"metadata":{},"output_type":"execute_result"}],"source":["um_tensor = torch.tensor(42.)\n","um_tensor"]},{"cell_type":"markdown","metadata":{"id":"pFnyqEMZXDcF"},"source":["Podemos verificar isso consultando a propriedade dtype de nosso tensor."]},{"cell_type":"code","execution_count":209,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1638380075001,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"G2n22nKA_BBt","outputId":"e5971926-2976-4a56-c6df-83d6d58ffc64"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.float32\n"]}],"source":["print(um_tensor.dtype)"]},{"cell_type":"markdown","metadata":{"id":"YSxBYayoXcd4"},"source":["Vamos ver outros exemplos de criação de tensores com PyTorch."]},{"cell_type":"code","execution_count":210,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1638467847378,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"Msw58PiCSkQF","outputId":"ff4ad2bd-a037-4be3-ae1c-bd6d88302142"},"outputs":[{"data":{"text/plain":["array([[1, 2],\n","       [3, 4],\n","       [5, 6]])"]},"execution_count":210,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","np.array([[1,2], \n","          [3,4], \n","          [5, 6]])"]},{"cell_type":"code","execution_count":211,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":401,"status":"ok","timestamp":1638362149871,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"9Wxd9Mi4_le5","outputId":"e2f01316-dd7b-4f48-9ec9-f713c8c787c5"},"outputs":[],"source":["t = torch.tensor([[1., 2], \n","                  [3, 4], \n","                  [5, 6]])"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([5., 6.])"]},"execution_count":212,"metadata":{},"output_type":"execute_result"}],"source":["t[-1]"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(1.)"]},"execution_count":213,"metadata":{},"output_type":"execute_result"}],"source":["t[0, 0]"]},{"cell_type":"markdown","metadata":{"id":"q0Hdl7pOXhoi"},"source":["O próximo exemplo cria um tensor tri-dimensional."]},{"cell_type":"code","execution_count":214,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1638453476289,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"tkGj2zWI_7BY","outputId":"335b1f02-9a63-4916-ebe5-aefb967646db"},"outputs":[{"data":{"text/plain":["tensor([[[1, 2, 3],\n","         [4, 5, 6]],\n","\n","        [[1, 2, 3],\n","         [4, 5, 6]],\n","\n","        [[1, 2, 3],\n","         [4, 5, 6]]])"]},"execution_count":214,"metadata":{},"output_type":"execute_result"}],"source":["y = torch.tensor([\n","     [\n","       [1, 2, 3],\n","       [4, 5, 6]\n","     ],\n","     [\n","       [1, 2, 3],\n","       [4, 5, 6]\n","     ],\n","     [\n","       [1, 2, 3],\n","       [4, 5, 6]\n","     ]\n","   ])\n","\n","y"]},{"cell_type":"markdown","metadata":{"id":"e1QZRYi4CmE3"},"source":["Uma boa analogia para entender intuitivamente um tensor tri-dimensional é pensar em um ficheiro (veja a figura a seguir).\n","\n","<img src=\"https://cf.shopee.com.br/file/c71de8869cdb5f0938c4a14e32ca4f23\" alt=\"Drawing\" width=\"200\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"Y02P7bHFAu33"},"source":["Tensores podem apresentar um número arbitrário de dimensões. Além disso, as dimensões não precisam ter o mesmo tamanho. \n","\n","O tamanho de cada dimensão em um tensor pode ser consultado por meio da propriedade `shape`."]},{"cell_type":"code","execution_count":215,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1638453492728,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"LLOTHtvRA5RO","outputId":"e840345e-d365-4c78-e306-c24028dd3ddd"},"outputs":[{"data":{"text/plain":["torch.Size([3, 2, 3])"]},"execution_count":215,"metadata":{},"output_type":"execute_result"}],"source":["y.shape"]},{"cell_type":"markdown","metadata":{"id":"8q3JdbaeFI3-"},"source":["Repare que, dentro de cada dimensão de um tensor, deve haver consistência entre os elementos componentes. Como exemplo, a execução da célula de código a seguir deve causar um erro."]},{"cell_type":"code","execution_count":216,"metadata":{"id":"SKwA0wplAn4Y"},"outputs":[],"source":["# torch.tensor([[0, 1, 2], \n","#               [3, 4], \n","#               [5, 6]])"]},{"cell_type":"markdown","metadata":{"id":"rN8bCwUEGcS3"},"source":["## Operações sobre tensores\n","\n","Essa seção apresenta algumas formas de transformar tensores usando operações fornecidas pelo PyTorch. A documentação completa dessa funções pode ser encontrada aqui: https://pytorch.org/docs/stable/torch.html"]},{"cell_type":"markdown","metadata":{"id":"gBLPWdS4nP5q"},"source":["Podemos combinar tensores com as operações aritméticas usuais. Vejamos um exemplo:"]},{"cell_type":"code","execution_count":217,"metadata":{"id":"9sIDoWZoGegY"},"outputs":[],"source":["import torch \n","\n","w = torch.tensor(.3)\n","x = torch.tensor(1.)\n","b = torch.tensor(.5)"]},{"cell_type":"markdown","metadata":{"id":"iZB2EPanX_zj"},"source":["Multiplicação:"]},{"cell_type":"code","execution_count":218,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":255,"status":"ok","timestamp":1638468829897,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"CvIIRvNNGxyI","outputId":"1c0c2f1b-56c6-46c6-c0c1-6ad426cd40d0"},"outputs":[{"data":{"text/plain":["tensor(0.3000)"]},"execution_count":218,"metadata":{},"output_type":"execute_result"}],"source":["y = w * x \n","y"]},{"cell_type":"markdown","metadata":{"id":"D8Sy6XMRYBwy"},"source":["Multiplicação e adição:"]},{"cell_type":"code","execution_count":219,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1638365006670,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"w2S_d_GRG0xB","outputId":"c3928fa6-3cb4-4e60-a14d-52322853db59"},"outputs":[{"data":{"text/plain":["tensor(0.8000)"]},"execution_count":219,"metadata":{},"output_type":"execute_result"}],"source":["y = w * x + b\n","y"]},{"cell_type":"markdown","metadata":{"id":"QWa-PajOecqX"},"source":["Outra operação importante é a multiplicação de tensores. Veja o exemplo a seguir."]},{"cell_type":"code","execution_count":220,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1638468936352,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"ctDzf80JG84C","outputId":"3bbe5acb-91d8-4a6d-973b-6ce53e747a3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 17,  20,  23,  26],\n","        [ 47,  59,  71,  83],\n","        [ 77,  98, 119, 140],\n","        [ 77, 101, 125, 149],\n","        [ 53,  71,  89, 107]])\n"]}],"source":["import torch\n","\n","A = torch.tensor([[0, 1, 2], \n","                  [3, 4, 5], \n","                  [6, 7, 8], \n","                  [9, 7, 8], \n","                  [7, 6, 5]])\n","\n","B = torch.tensor([[0, 1, 2, 3], \n","                  [3, 4, 5, 6], \n","                  [7, 8, 9, 10]])\n","\n","C = A @ B\n","\n","print(C)"]},{"cell_type":"markdown","metadata":{"id":"LJorRQh24yn6"},"source":["Uma operação que nos será útil é a de transpor um tensor que representa uma matriz (i.e., um tensor 2D). Veja o exemplo a seguir."]},{"cell_type":"code","execution_count":221,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1638461091052,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"m74UdDXZ47M0","outputId":"edf4d876-7976-460c-bfe5-01cea2a9481d"},"outputs":[{"data":{"text/plain":["tensor([[ 0,  3,  7],\n","        [ 1,  4,  8],\n","        [ 2,  5,  9],\n","        [ 3,  6, 10]])"]},"execution_count":221,"metadata":{},"output_type":"execute_result"}],"source":["M = torch.tensor([[0, 1, 2, 3], \n","                  [3, 4, 5, 6], \n","                  [7, 8, 9, 10]])\n","M.t()"]},{"cell_type":"markdown","metadata":{},"source":["A seguir outro exemplo de multiplicação de tensores:"]},{"cell_type":"code","execution_count":222,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1638453255887,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"OmqucTt50FhP","outputId":"0140ad4d-b626-449a-cc61-9e70bd48c685"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 53,  71,  89, 107]])\n"]}],"source":["A = torch.tensor([[7, 6, 5]])\n","\n","B = torch.tensor([[0, 1, 2, 3], \n","                  [3, 4, 5, 6], \n","                  [7, 8, 9, 10]])\n","\n","C = A @ B\n","\n","print(C)"]},{"cell_type":"markdown","metadata":{"id":"4T9yoJGbrjuP"},"source":["Repare que os operadores '\\*' e '@' são diferentes. \n","\n","- '@': matrix multiplication\n","- '*': element wise multiplication"]},{"cell_type":"code","execution_count":223,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1638453258969,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"4BuKnzYkroJc","outputId":"9d4c8e45-1a6c-447a-bab2-d4ca201d0125"},"outputs":[{"data":{"text/plain":["tensor([[  0,   1,   4,   9],\n","        [  9,  16,  25,  36],\n","        [ 49,  64,  81, 100]])"]},"execution_count":223,"metadata":{},"output_type":"execute_result"}],"source":["B * B # Produto de Hadamard"]},{"cell_type":"markdown","metadata":{"id":"kQSU3Al3MRF-"},"source":["## Algumas propriedades e funções aplicáveis a tensores"]},{"cell_type":"markdown","metadata":{},"source":["### torch.empty\n","\n","A função torch.empty do PyTorch é usada para criar um tensor não inicializado com o formato especificado. Isso significa que o tensor criado conterá valores não definidos, que normalmente serão lixo de memória. Esta função é útil quando você deseja alocar memória para um tensor rapidamente e não precisa que os valores sejam inicializados."]},{"cell_type":"code","execution_count":224,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1812,"status":"ok","timestamp":1612281358311,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"68OrHRHeQuDx","outputId":"cafa99e0-d3c0-483c-b78f-e17777d5576b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1.1210e-44, 0.0000e+00, 0.0000e+00],\n","        [0.0000e+00, 0.0000e+00, 1.4013e-45]])\n","tensor([[[2.7174e-322,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, 2.6712e-314]],\n","\n","        [[2.6712e-314,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n","       dtype=torch.float64)\n","tensor([[0.0000e+00, 0.0000e+00, 9.1835e-41],\n","        [0.0000e+00, 0.0000e+00, 0.0000e+00]], requires_grad=True)\n"]}],"source":["import torch\n","\n","# Criar um tensor 2x3 não inicializado\n","tensor1 = torch.empty(2, 3)\n","print(tensor1)\n","\n","# Criar um tensor 2x3x4 não inicializado com tipo de dados float64\n","tensor2 = torch.empty(2, 3, 4, dtype=torch.float64)\n","print(tensor2)\n","\n","# Criar um tensor 2x3 não inicializado no dispositivo CUDA\n","#tensor3 = torch.empty(2, 3, device=torch.device('cuda'))\n","#print(tensor3)\n","\n","# Criar um tensor 2x3 com requires_grad=True\n","tensor4 = torch.empty(2, 3, requires_grad=True)\n","print(tensor4)"]},{"cell_type":"markdown","metadata":{},"source":["### torch.full\n","\n","A função torch.full no PyTorch é usada para criar um tensor de um determinado tamanho e preencher todas as suas entradas com um valor específico. Isso pode ser útil quando você precisa inicializar um tensor com valores constantes."]},{"cell_type":"code","execution_count":225,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1638469304578,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"keQa7EztMUtZ","outputId":"585e3196-23c3-47ae-9f93-19a26473f526"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[7, 7, 7],\n","        [7, 7, 7],\n","        [7, 7, 7]])\n"]}],"source":["import torch\n","\n","# Criar um tensor 3x3 preenchido com o valor 7\n","tensor = torch.full((3, 3), 7)\n","\n","print(tensor)"]},{"cell_type":"markdown","metadata":{},"source":["### torch.cat\n","\n","A função torch.cat no PyTorch é usada para concatenar (ou seja, juntar) dois ou mais tensores ao longo de uma dimensão especificada. Isso é útil quando você deseja combinar tensores em uma única matriz ao longo de um eixo específico."]},{"cell_type":"code","execution_count":226,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1638469445710,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"i9FC9LjDMtlv","outputId":"8e0ffd45-6bfc-4918-98e1-c021325f76b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4],\n","        [5, 6],\n","        [7, 8]])\n"]}],"source":["import torch\n","\n","tensor1 = torch.tensor([[1, 2], [3, 4]])\n","tensor2 = torch.tensor([[5, 6], [7, 8]])\n","\n","result = torch.cat((tensor1, tensor2), dim=0)\n","print(result)"]},{"cell_type":"code","execution_count":227,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":375,"status":"ok","timestamp":1638469448121,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"9BjJtJmWYpJu","outputId":"4b3f87da-4da5-4ede-d481-714e11164f68"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2, 5, 6],\n","        [3, 4, 7, 8]])\n"]}],"source":["import torch\n","\n","tensor1 = torch.tensor([[1, 2], [3, 4]])\n","tensor2 = torch.tensor([[5, 6], [7, 8]])\n","\n","result = torch.cat((tensor1, tensor2), dim=1)\n","print(result)"]},{"cell_type":"code","execution_count":228,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[ 1,  2],\n","         [ 3,  4]],\n","\n","        [[ 5,  6],\n","         [ 7,  8]],\n","\n","        [[ 9, 10],\n","         [11, 12]],\n","\n","        [[13, 14],\n","         [15, 16]]])\n"]}],"source":["import torch\n","\n","tensor1 = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n","tensor2 = torch.tensor([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n","\n","result = torch.cat((tensor1, tensor2), dim=0)\n","print(result)\n"]},{"cell_type":"markdown","metadata":{"id":"RWJ0ExrqmNZJ"},"source":["### torch.reshape\n","\n","A função torch.reshape em PyTorch é usada para alterar a forma (ou dimensão) de um tensor sem alterar os seus dados subjacentes. Essa função é muito útil quando você precisa modificar a estrutura de um tensor para atender aos requisitos de uma operação específica ou para se ajustar à arquitetura de um modelo."]},{"cell_type":"code","execution_count":229,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1638469457959,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"SrdpBi8PYzPV","outputId":"bcb883d8-7fe3-4d4d-c4c5-4759bc8f72c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original tensor: tensor([1, 2, 3, 4, 5, 6])\n","Remodelado para 2D: tensor([[1, 2, 3],\n","        [4, 5, 6]])\n"]}],"source":["# Exemplo 1: Remodelar um tensor 1D em um tensor 2D\n","\n","import torch\n","\n","# Tensor 1D\n","tensor_1d = torch.tensor([1, 2, 3, 4, 5, 6])\n","print(\"Original tensor:\", tensor_1d)\n","\n","# Remodelar para 2D\n","tensor_2d = torch.reshape(tensor_1d, (2, 3))\n","print(\"Remodelado para 2D:\", tensor_2d)\n"]},{"cell_type":"code","execution_count":230,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":233,"status":"ok","timestamp":1638469510598,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"7nBGYAOoN5mA","outputId":"499d583e-b8d3-42bc-9c80-589efc25e679"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original tensor: tensor([[1, 2, 3, 4],\n","        [5, 6, 7, 8]])\n","Remodelado para 3D: tensor([[[1, 2],\n","         [3, 4]],\n","\n","        [[5, 6],\n","         [7, 8]]])\n"]}],"source":["# Exemplo 2: Remodelar um tensor 2D em um tensor 3D usando -1 para inferir a dimensão\n","\n","import torch\n","\n","# Tensor 2D\n","tensor_2d = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n","print(\"Original tensor:\", tensor_2d)\n","\n","# Remodelar para 3D\n","tensor_3d = torch.reshape(tensor_2d, (2, 2, -1))\n","print(\"Remodelado para 3D:\", tensor_3d)\n"]},{"cell_type":"code","execution_count":231,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1638469644728,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"niXJ_mz5DIOH","outputId":"87c8e301-1eb2-4919-8043-ef5f87063067"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original tensor: tensor([[[ 1,  2,  3],\n","         [ 4,  5,  6]],\n","\n","        [[ 7,  8,  9],\n","         [10, 11, 12]]])\n","Remodelado para 2D: tensor([[ 1,  2,  3,  4],\n","        [ 5,  6,  7,  8],\n","        [ 9, 10, 11, 12]])\n"]}],"source":["# Exemplo 3: Remodelar um tensor com mais dimensões\n"," \n","import torch\n","\n","# Tensor 3D\n","tensor_3d = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n","print(\"Original tensor:\", tensor_3d)\n","\n","# Remodelar para 2D\n","tensor_2d = torch.reshape(tensor_3d, (3, 4))\n","print(\"Remodelado para 2D:\", tensor_2d)\n"]},{"cell_type":"markdown","metadata":{"id":"V1vtO7ummTy9"},"source":["A função rand permite gerar números aleatórios (de acordo com a distribuição de probabilidade uniforme) para iniciar os elementos de um tensor."]},{"cell_type":"code","execution_count":232,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1638469823843,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"gTr7pq8_IO1w","outputId":"1d22c4ed-30a4-4588-ad63-8c95b3f3f1ba"},"outputs":[{"data":{"text/plain":["tensor([[0.3048, 0.8190, 0.3231, 0.3510],\n","        [0.8366, 0.7135, 0.6034, 0.9260],\n","        [0.7461, 0.9074, 0.0860, 0.4717]])"]},"execution_count":232,"metadata":{},"output_type":"execute_result"}],"source":["torch.rand(3,4)"]},{"cell_type":"markdown","metadata":{"id":"3aoyqjWwmd7W"},"source":["### torch.randn\n","\n","A função torch.randn é usada para gerar um tensor com elementos amostrados a partir de uma distribuição normal padrão (com média 0 e desvio padrão 1). Ela é útil em várias situações, como na inicialização de pesos de uma rede neural, onde é comum inicializar os pesos com valores aleatórios para quebrar a simetria e permitir que o modelo aprenda de forma mais eficaz."]},{"cell_type":"code","execution_count":233,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1638469901406,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"XJSxfpmtIX_D","outputId":"c9c42c4c-036b-48d3-8eb0-3e7e728ec9ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.2990, -2.1737, -0.6602],\n","        [-0.4582, -0.7768, -1.4381],\n","        [-0.6215,  0.0591,  0.2684]])\n","tensor([[[-0.4530,  0.0205,  1.0887,  1.1185,  2.1169],\n","         [ 0.3952, -0.2988, -0.5113,  0.9304, -0.5463],\n","         [ 0.3408,  1.1294, -1.3243,  0.0242,  0.3475],\n","         [ 0.8148, -1.2598,  1.7421, -0.5017, -0.2645]],\n","\n","        [[-1.8287,  1.1441,  0.4110, -0.2794,  0.2674],\n","         [-1.3806, -0.4131, -0.0059,  0.3594,  0.3617],\n","         [-1.0445,  0.7477,  0.7793,  0.3057,  0.6959],\n","         [ 0.4535,  1.2476,  1.6912,  1.0357, -0.5796]]])\n"]}],"source":["import torch\n","\n","# Criar um tensor 3x3 com valores amostrados de uma distribuição normal padrão\n","tensor = torch.randn(3, 3)\n","print(tensor)\n","\n","# Criar um tensor 2x4x5 com valores amostrados de uma distribuição normal padrão, especificando o dispositivo como GPU\n","tensor_gpu = torch.randn(2, 4, 5)\n","print(tensor_gpu)"]},{"cell_type":"markdown","metadata":{"id":"sZ3EEmUga6tt"},"source":["### torch.sum\n","\n","A função torch.sum do PyTorch é usada para calcular a soma dos elementos de um tensor ao longo de determinadas dimensões."]},{"cell_type":"code","execution_count":234,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1638453263254,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"oyKyNfBRs4-r","outputId":"702cca6c-a881-40bb-f675-e5b9da7cdf3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(21)\n"]}],"source":["# Exemplo: Soma de todos os elementos de um tensor\n","\n","import torch\n","\n","# Tensor de exemplo\n","x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","\n","# Soma de todos os elementos\n","total_sum = torch.sum(x)\n","print(total_sum)  # Saída: tensor(21)\n"]},{"cell_type":"code","execution_count":235,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5, 7, 9])\n","tensor([ 6, 15])\n"]}],"source":["# Exemplo: Soma ao longo de uma dimensão específica\n","\n","import torch\n","\n","# Tensor de exemplo\n","x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","\n","# Soma ao longo da dimensão 0 (soma das colunas)\n","sum_dim0 = torch.sum(x, dim=0)\n","print(sum_dim0)\n","\n","# Soma ao longo da dimensão 1 (soma das linhas)\n","sum_dim1 = torch.sum(x, dim=1)\n","print(sum_dim1)\n"]},{"cell_type":"code","execution_count":236,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1638453276152,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"IdlGo9KHbCHr","outputId":"92962b9b-98c1-437f-fc07-ba9ae54bca2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[5, 7, 9]])\n","tensor([[ 6],\n","        [15]])\n"]}],"source":["# Exemplo: Mantendo a dimensão reduzida\n","\n","import torch\n","\n","# Tensor de exemplo\n","x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","\n","# Soma ao longo da dimensão 0 (soma das colunas) com keepdim=True\n","sum_dim0_keepdim = torch.sum(x, dim=0, keepdim=True)\n","print(sum_dim0_keepdim)  # Saída: tensor([[5, 7, 9]])\n","\n","# Soma ao longo da dimensão 1 (soma das linhas) com keepdim=True\n","sum_dim1_keepdim = torch.sum(x, dim=1, keepdim=True)\n","print(sum_dim1_keepdim)  # Saída: tensor([[ 6], [15]])\n"]},{"cell_type":"markdown","metadata":{"id":"xHTRbLUEck5l"},"source":["A função sum pode ser aplicada a tensores de maiores dimensões. Veja os exemplos a seguir para um tensor 3D. Veja também essas [animações](https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be) para ter um entendimento intuitivo sobre essas somas."]},{"cell_type":"code","execution_count":237,"metadata":{"id":"kp4j1UNvc8Jk"},"outputs":[],"source":["y = torch.tensor([\n","     [\n","       [1, 2, 3],\n","       [4, 5, 6]\n","     ],\n","     [\n","       [1, 2, 3],\n","       [4, 5, 6]\n","     ],\n","     [\n","       [1, 2, 3],\n","       [4, 5, 6]\n","     ]\n","   ])"]},{"cell_type":"code","execution_count":238,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1638454355568,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"ctGurrShfVP1","outputId":"51e9059e-88f2-46cb-9d75-d8919fbfd3fb"},"outputs":[{"data":{"text/plain":["torch.Size([3, 2, 3])"]},"execution_count":238,"metadata":{},"output_type":"execute_result"}],"source":["y.shape"]},{"cell_type":"code","execution_count":239,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1638453738445,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"FLJWZdp2crmJ","outputId":"012f685a-b09c-4342-dac8-3400274e7d0a"},"outputs":[{"data":{"text/plain":["tensor([[ 3,  6,  9],\n","        [12, 15, 18]])"]},"execution_count":239,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum(y, dim=0)"]},{"cell_type":"code","execution_count":240,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1638453739784,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"iRywTYJac3Oq","outputId":"19da4b90-bedb-47bb-fa2a-0bb80a190623"},"outputs":[{"data":{"text/plain":["tensor([[5, 7, 9],\n","        [5, 7, 9],\n","        [5, 7, 9]])"]},"execution_count":240,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum(y, dim=1)"]},{"cell_type":"code","execution_count":241,"metadata":{"id":"W9iuoChJc3dX"},"outputs":[{"data":{"text/plain":["tensor([[ 6, 15],\n","        [ 6, 15],\n","        [ 6, 15]])"]},"execution_count":241,"metadata":{},"output_type":"execute_result"}],"source":["torch.sum(y, dim=2)"]},{"cell_type":"markdown","metadata":{"id":"ih__0_a4bUkS"},"source":["Por vezes, vamos precisar simplesmente \"zerar\" os elementos em uma matriz. Veja o exemplo abaixo."]},{"cell_type":"code","execution_count":242,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1638453875086,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"Pu3xpkZ4dght","outputId":"31fef7cc-a247-4aa5-8fd4-f01d9eb289d9"},"outputs":[{"data":{"text/plain":["tensor([[ 0,  1,  2,  3],\n","        [ 3,  4,  5,  6],\n","        [ 7,  8,  9, 10]])"]},"execution_count":242,"metadata":{},"output_type":"execute_result"}],"source":["B"]},{"cell_type":"code","execution_count":243,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1638442110002,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"TntPGcJ9wjs8","outputId":"716e94ba-0c2a-4604-80d7-13351bf13213"},"outputs":[{"data":{"text/plain":["tensor([[0, 0, 0, 0],\n","        [0, 0, 0, 0],\n","        [0, 0, 0, 0]])"]},"execution_count":243,"metadata":{},"output_type":"execute_result"}],"source":["B.zero_()"]},{"cell_type":"markdown","metadata":{},"source":["### torch.from_numpy\n","\n","A função torch.from_numpy em PyTorch é usada para converter um array NumPy em um tensor PyTorch. Esta função cria um tensor PyTorch que compartilha os mesmos dados subjacentes que o array NumPy. Portanto, qualquer alteração feita no tensor será refletida no array original e vice-versa, desde que o array NumPy original não seja desalocado."]},{"cell_type":"code","execution_count":244,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Array NumPy: [1 2 3 4 5]\n","Tensor PyTorch: tensor([1, 2, 3, 4, 5])\n","Array NumPy após a modificação do tensor: [10  2  3  4  5]\n","Tensor PyTorch após modificação: tensor([10,  2,  3,  4,  5])\n"]}],"source":["import numpy as np\n","import torch\n","\n","# Criar um array NumPy\n","np_array = np.array([1, 2, 3, 4, 5])\n","\n","# Converter o array NumPy em um tensor PyTorch\n","tensor = torch.from_numpy(np_array)\n","\n","# Exibir o array e o tensor\n","print(\"Array NumPy:\", np_array)\n","print(\"Tensor PyTorch:\", tensor)\n","\n","# Modificar o tensor PyTorch\n","tensor[0] = 10\n","\n","# Exibir novamente para ver as mudanças\n","print(\"Array NumPy após a modificação do tensor:\", np_array)\n","print(\"Tensor PyTorch após modificação:\", tensor)\n"]},{"cell_type":"markdown","metadata":{"id":"HaaXsOXae7Le"},"source":["### `torch.backward`\n","\n","A função torch.backward() no PyTorch é uma parte essencial do processo de treinamento de redes neurais. Ela é usada para calcular os gradientes dos parâmetros do modelo em relação à função de perda. Esses gradientes são então usados para atualizar os pesos do modelo.\n","\n","O que torch.backward() faz:\n","- Cálculo dos Gradientes: Quando você chama loss.backward(), PyTorch realiza a retropropagação através do grafo computacional que foi construído durante a computação da perda. Ele calcula os gradientes da função de perda em relação a todos os parâmetros do modelo que possuem requires_grad=True.\n","\n","- Armazenamento dos Gradientes: Os gradientes calculados são armazenados nos atributos .grad dos tensores que têm requires_grad=True. Esses gradientes são usados pelo otimizador para ajustar os pesos do modelo durante o treinamento."]},{"cell_type":"code","execution_count":245,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":274,"status":"ok","timestamp":1638471670840,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"CJzJfK_6IfBN","outputId":"2bfa6876-42a1-44ee-d9ea-deb9d4630236"},"outputs":[{"data":{"text/plain":["tensor(0.6200, grad_fn=<AddBackward0>)"]},"execution_count":245,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.tensor(.3, )\n","w = torch.tensor(.4, requires_grad = True)\n","b = torch.tensor(.5, requires_grad = True)\n","\n","y = w * x + b\n","y"]},{"cell_type":"code","execution_count":246,"metadata":{"id":"RzOxtkcZH_Sp"},"outputs":[],"source":["y.backward()"]},{"cell_type":"code","execution_count":247,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1638471675027,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"ZqBDKSAJJc0Y","outputId":"1818605f-68aa-489f-9ff3-c5ac5b20af97"},"outputs":[{"name":"stdout","output_type":"stream","text":["dy/dw =  tensor(0.3000)\n","dy/dx =  None\n","dy/db =  tensor(1.)\n"]}],"source":["print('dy/dw = ', w.grad)\n","print('dy/dx = ', x.grad)\n","print('dy/db = ', b.grad)"]},{"cell_type":"markdown","metadata":{"id":"l8Wmery2le7f"},"source":["Como esperado, dy/dw tem o mesmo valor de x, e dy/db tem o valor 1. Observe que x.grad é None porque x não tem require_grad definido como True.\n","\n","O \"grad\" em w.grad é a abreviação de gradiente, que é outro termo para derivada. O termo gradiente é usado principalmente ao lidar com vetores e matrizes."]},{"cell_type":"markdown","metadata":{"id":"yK594pDjK71b"},"source":["O resulta acima podem ser entendidos se você considerar que $y$ é uma função de três outras variáveis, $w$, $x$ e $b$.\n","\n","$$\n","y = f(w, x, b) = w \\times x + b\n","$$\n","\n","Se calcularmos as derivadas parciais de $y$ com relação a $w$, $x$ e $b$, vamos encontrar o seguinte:\n","\n","\\begin{align}\n","\\frac{dy}{dw} &= w  \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n","\\frac{dy}{dx} &= x  \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n","\\frac{dy}{db} &= 1\n","\\end{align}\n"]},{"cell_type":"markdown","metadata":{},"source":["### torch.no_grad()\n","\n","A função torch.no_grad() em PyTorch é um contexto gerenciador (context manager) que desativa a computação do gradiente. Ela é usada para realizar operações que não precisam de cálculos de gradiente, como durante a inferência ou avaliação do modelo, para economizar memória e melhorar a eficiência computacional."]},{"cell_type":"markdown","metadata":{},"source":["# Regressão Linear com PyTorch\n","\n","Nesta seção, discutimos um dos algoritmos básicos do aprendizado de máquina: a regressão linear. Criaremos um modelo capaz de prever o perfil fisiológico de um indivíduo (variáveis alvo, variáveis dependentes), uma vez que se conhece o perfil de realização de exercícios (variáveis ​​de entrada, variáveis independentes, características). \n","\n","Vamos usar um conjunto de dados (*dataset*) bem simples denominado Linnerud. Esse conjunto de dados tem apenas 20 exemplos, 3 variáveis ​​independentes e 3 alvos. A descrição do conjunto de dados Linnerud é a seguinte: \n","\n","> “O conjunto de dados Linnerud é um conjunto de dados de regressão de múltiplas saídas. É composto por três variáveis sobre exercícios (matrix de dados, $X$) e três variáveis que medem característics ​​fisiológicas (matriz alvo, $y$) coletados de vinte homens de meia-idade em um clube de fitness:\n","\n","- variáveis relacionadas ao perfil de atividade física do indivíduo ($X$)- :  \"puxar ferro\" (Chins), abdominais (Situps) e saltos (Jumps).\n","\n","- variáveis relacionadas ao perfil fisiológico do indivíduo ($y$) - Peso (Weight), Cintura (Weist) e Pulso (Pulse).\n","\n","Mais informações sobre esse conjunto de dados podem ser encontradas nos links abaixo:\n","\n","- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_linnerud.html\n","\n","- https://scikit-learn.org/stable/datasets/toy_dataset.html#linnerrud-dataset\n","\n","- https://ai.plainenglish.io/an-exploration-into-sklearns-linnerrud-multioutput-dataset-4e0ad110c728"]},{"cell_type":"code","execution_count":248,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[  5. 162.  60.]\n"," [  2. 110.  60.]\n"," [ 12. 101. 101.]\n"," [ 12. 105.  37.]\n"," [ 13. 155.  58.]\n"," [  4. 101.  42.]\n"," [  8. 101.  38.]\n"," [  6. 125.  40.]\n"," [ 15. 200.  40.]\n"," [ 17. 251. 250.]\n"," [ 17. 120.  38.]\n"," [ 13. 210. 115.]\n"," [ 14. 215. 105.]\n"," [  1.  50.  50.]\n"," [  6.  70.  31.]\n"," [ 12. 210. 120.]\n"," [  4.  60.  25.]\n"," [ 11. 230.  80.]\n"," [ 15. 225.  73.]\n"," [  2. 110.  43.]]\n","(20, 3)\n","Target:\n","[[191.  36.  50.]\n"," [189.  37.  52.]\n"," [193.  38.  58.]\n"," [162.  35.  62.]\n"," [189.  35.  46.]\n"," [182.  36.  56.]\n"," [211.  38.  56.]\n"," [167.  34.  60.]\n"," [176.  31.  74.]\n"," [154.  33.  56.]\n"," [169.  34.  50.]\n"," [166.  33.  52.]\n"," [154.  34.  64.]\n"," [247.  46.  50.]\n"," [193.  36.  46.]\n"," [202.  37.  62.]\n"," [176.  37.  54.]\n"," [157.  32.  52.]\n"," [156.  33.  54.]\n"," [138.  33.  68.]]\n","(20, 3)\n","tensor([[  5., 162.,  60.],\n","        [  2., 110.,  60.],\n","        [ 12., 101., 101.],\n","        [ 12., 105.,  37.],\n","        [ 13., 155.,  58.],\n","        [  4., 101.,  42.],\n","        [  8., 101.,  38.],\n","        [  6., 125.,  40.],\n","        [ 15., 200.,  40.],\n","        [ 17., 251., 250.],\n","        [ 17., 120.,  38.],\n","        [ 13., 210., 115.],\n","        [ 14., 215., 105.],\n","        [  1.,  50.,  50.],\n","        [  6.,  70.,  31.],\n","        [ 12., 210., 120.],\n","        [  4.,  60.,  25.],\n","        [ 11., 230.,  80.],\n","        [ 15., 225.,  73.],\n","        [  2., 110.,  43.]], dtype=torch.float64)\n","tensor([[191.,  36.,  50.],\n","        [189.,  37.,  52.],\n","        [193.,  38.,  58.],\n","        [162.,  35.,  62.],\n","        [189.,  35.,  46.],\n","        [182.,  36.,  56.],\n","        [211.,  38.,  56.],\n","        [167.,  34.,  60.],\n","        [176.,  31.,  74.],\n","        [154.,  33.,  56.],\n","        [169.,  34.,  50.],\n","        [166.,  33.,  52.],\n","        [154.,  34.,  64.],\n","        [247.,  46.,  50.],\n","        [193.,  36.,  46.],\n","        [202.,  37.,  62.],\n","        [176.,  37.,  54.],\n","        [157.,  32.,  52.],\n","        [156.,  33.,  54.],\n","        [138.,  33.,  68.]], dtype=torch.float64)\n"]}],"source":["from sklearn.datasets import load_linnerud\n","X_, y_ = load_linnerud(return_X_y = True)\n","print(X_)\n","print(X_.shape)\n","\n","\n","print(\"Target:\")\n","print(y_)\n","print(y_.shape)\n","\n","import torch\n","X = torch.from_numpy(X_)\n","print(X)\n","y = torch.from_numpy(y_)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"fA1rIPWmra6g"},"source":["No aprendizado de máquina, o termo viés indutivo (*inductive bias*) se refere a um conjunto de suposições feitas por um algoritmo de aprendizagem a fim de realizar a indução, isto é, generalizar um conjunto finito de observação (dados de treinamento) em um modelo. Sem um viés desse tipo, a indução não seria possível, uma vez que as observações podem normalmente ser generalizadas de várias maneiras. Se o algoritmo tratasse todas essas possibilidades igualmente, ou seja, sem qualquer tendência no sentido de uma preferência por tipos específicos de generalização (refletindo o conhecimento prévio sobre a função alvo a ser aprendida), as previsões para novas situações não poderiam ser feitas.\n","\n","Em um modelo de regressão linear, a presuposição (ou o viés indutivo) é que cada variável alvo pode ser estimada como uma **soma ponderada** das variáveis ​​de entrada com a adição de alguma constante, conhecida como viés (bias). Para o conjunto de dados Linnerun, temos o seguinte:\n","\n","\\begin{align}\n"," \\text{Weight} &= w_{11} \\times \\text{Chins} + w_{12} \\times \\text{Situps} + w_{13} \\times \\text{Jumps} + b_{1} \\\\\n","\\text{Weist} &= w_{21} \\times \\text{Chins} + w_{22} \\times \\text{Situps} + w_{23} \\times \\text{Jumps} + b_2 \\\\\n","\\text{Pulse} &= w_{31} \\times \\text{Chins} + w_{32} \\times \\text{Situps} + w_{33} \\times \\text{Jumps} + b_3\n","\\end{align}\n","\n","As expressões acima podem ser representadas de forma matricial. Para entender isso, primeiro considere que podemos criar uma matriz para organizar em suas entradas todos os pesos, conforme ilustrado a seguir.\n","\n","$$\n","W = \n","\\begin{bmatrix}\n","w_{11} & w_{12} & w_{13}\\\\\n","w_{21} & w_{22} & w_{23}\\\\\n","w_{31} & w_{32} & w_{33}\n","\\end{bmatrix}\n","$$\n","\n","Podemos também organizar os valores de viés em um vetor: \n","$$\n","b = \\begin{bmatrix}\n","b_{1}\\\\\n","b_{2}\\\\\n","b_{3}\n","\\end{bmatrix}\n","$$\n","\n","Da mesma forma, cada exemplo $x$ do conjunto de dados, formado por valores das variáveis independentes $\\text{Chins}$, $\\text{Situps}$ e $\\text{Jumps}$, pode também ser modelado como um vetor no $\\Re^3$:\n","\n","$$\n","x = \\begin{bmatrix}\n","\\text{Chins}\\\\\n","\\text{Situps}\\\\\n","\\text{Jumps}\n","\\end{bmatrix}\n","$$\n","\n","O mesmo vale para as variáveis alvo:\n","\n","$$\n","y = \\begin{bmatrix}\n","\\text{Weight}\\\\\n","\\text{Weist}\\\\\n","\\text{Pulse}\n","\\end{bmatrix}\n","$$\n","\n","Agora, deve ficar claro que a seguinte expressão é uma identidade:\n","\n","$$\n","y = W \\times x + b \n","$$\n","\n","Na expressão matricial acima:\n","\n","- $y$ é a saída produzida pelo modelo, correspondente à predição da **variável alvo** (*target variable*)\n","- $x$ é entrada fornecida ao modelo (*feature vector*)\n","- $W$ é denominada a **matriz de pesos** (*weight matrix*)\n","- $b$ é denominado o **vetor viés** (*bias vector*)\n","- A $i$-ésima linha de $W$ e o $i$-ésimo elemento de $b$ são usados ​​para prever a $i$-ésima variável alvo ($1 \\leq i \\leq 3$)."]},{"cell_type":"markdown","metadata":{"id":"N1sZu6BNsvmC"},"source":["A parte de aprendizagem da regressão linear é descobrir um conjunto de parâmetros $w_{11}, w_{12}, \\ldots w_{23}, \\ldots, w_{33}, b_1, b_2, b_3$ usando os dados de treinamento, para fazer previsões precisas para novos dados. Os parâmetros aprendidos serão usados ​​para prever as características fisiológicas de novos indivíduos, um vez que se saiba seu perfil de realização de exercícios físicos.\n","\n","Vamos treinar nosso modelo ajustando ligeiramente os parâmetros várias vezes para fazer melhores previsões, usando uma técnica de otimização chamada **gradiente descendente**. Vamos começar importando o Numpy e o PyTorch."]},{"cell_type":"code","execution_count":249,"metadata":{"id":"L_tyxVRW2Iql"},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"code","execution_count":250,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1644523105287,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"5VoMtHieNyFk","outputId":"53873969-1fee-4ed6-ce8c-5ab2397048c5"},"outputs":[{"data":{"text/plain":["torch.float64"]},"execution_count":250,"metadata":{},"output_type":"execute_result"}],"source":["X.dtype"]},{"cell_type":"code","execution_count":251,"metadata":{"id":"_ba-i2yI2JyA"},"outputs":[],"source":["# X -> data matrix; \n","# y -> target vector, response vector.\n","X, y = X.float(), y.float()"]},{"cell_type":"markdown","metadata":{"id":"mAROi66E2SMt"},"source":["Os pesos e vieses, armazenados nas matrizes $W$ e $b$,  são inicializados como valores aleatórios. "]},{"cell_type":"code","execution_count":252,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":681,"status":"ok","timestamp":1639072350024,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"8t988pl62p2T","outputId":"65767c39-43de-416a-9358-48d986d5cbbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 1.1099,  0.8805,  0.1600],\n","        [-1.1625, -1.7516, -1.2653],\n","        [-0.6812, -1.8468, -0.5367]], requires_grad=True)\n","tensor([ 0.4356,  0.3716, -0.6117], requires_grad=True)\n"]}],"source":["W = torch.randn(3, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","print(W)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"OrErfh3i22IM"},"source":["Nosso modelo é simplesmente uma função que realiza uma multiplicação da matriz de dados $X$ e dos pesos $W$ (transpostos) e adiciona o vetor $b$ (replicado para cada observação)."]},{"cell_type":"code","execution_count":253,"metadata":{"id":"jdMunn7r2tDP"},"outputs":[],"source":["# implementação que usa vetorização (vectorization)\n","def model(x):\n","    return x @ W.t() + b"]},{"cell_type":"markdown","metadata":{"id":"rmS0Cq_z5iqS"},"source":["A matriz obtida ao passar os dados de entrada para o modelo é um conjunto de previsões para as variáveis alvo."]},{"cell_type":"code","execution_count":254,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1639079014959,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"fMjP_v8w_sFA","outputId":"13bb8ce7-fca8-464b-9d30-3b3f215c9ba4"},"outputs":[{"name":"stdout","output_type":"stream","text":["x = tensor([  5., 162.,  60.])\n","torch.Size([3])\n","y_pred = tensor([ 158.2184, -365.1204, -335.4095], grad_fn=<AddBackward0>)\n","torch.Size([3])\n"]}],"source":["print(f'x = {X[0]}')\n","print(X[0].shape)\n","\n","# Gera predições\n","y_pred = model(X[0])\n","print(f'y_pred = {y_pred}')\n","print(y_pred.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"aa6u_Kk42grh"},"source":["Compare o tensor `y_pred` acima com os valores corretos reproduzidos abaixo.\n","\n","Você pode ver uma grande diferença entre as previsões do modelo e os valores verdadeiros porque inicializamos nosso modelo com pesos e vieses aleatórios. Obviamente, não podemos esperar que um modelo inicializado aleatoriamente funcione."]},{"cell_type":"code","execution_count":255,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1638720528577,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"RFw-FojTBbsG","outputId":"942d586a-1658-4126-b21a-d187b1170786"},"outputs":[{"data":{"text/plain":["tensor([191.,  36.,  50.])"]},"execution_count":255,"metadata":{},"output_type":"execute_result"}],"source":["y[0]"]},{"cell_type":"markdown","metadata":{"id":"DMRyL-A8Bt6Y"},"source":["## Função de custo (*Loss function*)\n","\n","Para que possamos melhorar nosso modelo, precisamos de uma forma objetiva de avaliar o desempenho preditivo dele. Podemos comparar as previsões do modelo com os alvos reais usando o seguinte procedimento:\n","\n","1. Calcular a diferença entre as duas matrizes (`y_pred` e `y`).\n","2. Elevar ao quadrado cada elemento da matriz de diferença, para remover valores negativos.\n","3. Calcular a média dos elementos na matriz resultante.\n","\n","O resultado é um único número, conhecido como erro quadrático médio (MSE, *mean squared error*). Matematicamente, os passos acima se traduzem nas seguintes expressões, onde $n$ é a quantidade de variáveis alvo:\n","\n","\\begin{align}\n","\\text{diff} &= (y_\\text{true} - y_{\\text{pred}}) \\\\\n","S &= \\text{diff} \\odot \\text{diff} \\\\\n","\\operatorname{MSE} &= \\frac{1}{n} \\sum_{1 \\leq i,j \\leq n} S_{ij} \n","\\end{align}\n"]},{"cell_type":"code","execution_count":256,"metadata":{"id":"bWAr_V7KCTxH"},"outputs":[],"source":["# Definição da função de perda denominada MSE\n","def mse(y_true, y_pred):\n","    diff = y_true - y_pred\n","    return torch.sum(diff * diff) / diff.numel()"]},{"cell_type":"markdown","metadata":{"id":"iE41vU8H5eif"},"source":["Usando a função acima, vamos calcular o MSE para a versão atual de nosso modelo:"]},{"cell_type":"code","execution_count":265,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([20, 3])"]},"execution_count":265,"metadata":{},"output_type":"execute_result"}],"source":["y.shape"]},{"cell_type":"code","execution_count":257,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":421,"status":"ok","timestamp":1639074586394,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"RuCDMyX3CUeS","outputId":"f6ca6730-2898-458e-afac-84bc8fce499f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(104916.6328, grad_fn=<DivBackward0>)\n"]}],"source":["# Computa a função de custo\n","loss = mse(y, y_pred)\n","print(loss)"]},{"cell_type":"code","execution_count":258,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":546,"status":"ok","timestamp":1639074666642,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"dYkIy4qkdiM6","outputId":"15a86a98-eebe-4d84-aaf6-22b930407332"},"outputs":[{"data":{"text/plain":["323.90837101331607"]},"execution_count":258,"metadata":{},"output_type":"execute_result"}],"source":["import math\n","math.sqrt(loss)"]},{"cell_type":"markdown","metadata":{"id":"Z3viNOf7Clr5"},"source":["Podemos interpretar o valor produzido acima da seguinte forma: \n","\n","> Em média, cada elemento na previsão difere do alvo (valor verdadeiro) pela raiz quadrada do valor da função de custo. \n","\n","Objetivamente, esse resultado é muito ruim, considerando que os números que estamos tentando prever estão na faixa de 30-250. O resultado é chamado de perda porque indica o quão ruim o modelo é em prever as variáveis alvo. Representa a perda de informações no modelo: quanto menor a perda, melhor é o modelo."]},{"cell_type":"markdown","metadata":{"id":"0HrCwSCLDEgU"},"source":["## Cálculo dos gradientes\n","\n","Com PyTorch, podemos calcular automaticamente o gradiente ou derivada da função de custo com relação aos pesos e vieses. Isso porque eles foram definidos com `requires_grad` igual a `True`."]},{"cell_type":"markdown","metadata":{"id":"Ead3HsIrDYcx"},"source":["Os gradientes são armazenados na propriedade `.grad` dos respectivos tensores. Observe que as derivadas parciais da função de custo com relação a cada elemento da matriz de pesos $W$ podem ser organizados em outra matriz com as mesmas dimensões. Se denotarmos por $J$ a função de custo e por $dW$ essa outra matriz, temos:\n","\n","$$\n","dW = \n","\\begin{bmatrix}\n","\\frac{\\partial J}{\\partial w_{11}} & \\frac{\\partial J}{\\partial w_{12}} & \\frac{\\partial J}{\\partial w_{13}}\\\\\n","\\frac{\\partial J}{\\partial w_{21}} & \\frac{\\partial J}{\\partial w_{22}} & \\frac{\\partial J}{\\partial w_{23}}\\\\\n","\\frac{\\partial J}{\\partial w_{31}} & \\frac{\\partial J}{\\partial w_{32}} & \\frac{\\partial J}{\\partial w_{33}}\n","\\end{bmatrix}\n","$$\n","\n","$$\n","db = \n","\\begin{bmatrix}\n","\\frac{\\partial J}{\\partial b_{1}} \\\\\n","\\frac{\\partial J}{\\partial b_{2}} \\\\\n","\\frac{\\partial J}{\\partial b_{3}}\n","\\end{bmatrix}\n","$$\n","\n","Os matemáticos chamam essa matriz de [Jacobiana](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)."]},{"cell_type":"markdown","metadata":{"id":"YRDP7IWgD1S8"},"source":["A função de custo MSE é uma função quadrática dos pesos e vieses, e nosso objetivo é encontrar o conjunto de parâmetros onde a função de custo é mínima. Se traçarmos um gráfico da função de custo com qualquer parâmetro individual (peso ou viés), ele se parecerá com a figura mostrada no link a seguir: https://www.geogebra.org/m/j8jqxyrs.\n","\n","Um informação importante é que cada derivada parcial contida na Jacobiana $dW$ indica a taxa de variação da função de custo em uma direção específica, ou seja, a inclinação dessa função.\n","\n","Se um elemento de $dW$ for **positivo**, então:\n","- **aumentar** ligeiramente o valor do parâmetro correspondente **aumenta** o valor da função de custo;\n","- **diminuir** ligeiramente o valor do peso correspondente  **diminui** o valor da função de custo.\n","\n","Se um elemento de $dW$ for **negativo**, então:\n","- **aumentar** ligeiramente o valor do peso correspondente **diminui** o valor da função de custo;\n","- **diminuir** ligeiramente o valor do peso correspondente **aumenta** o valor da função de custo."]},{"cell_type":"markdown","metadata":{"id":"Pz3cUmjPEoEq"},"source":["O aumento ou diminuição na função $J$ causado pela mudança no valor um elemento em $W$ é proporcional a esse valor. Essa observação forma a base do algoritmo de otimização de **gradiente descendente** que usaremos para melhorar nosso modelo.\n","\n","> Podemos subtrair de cada elemento de $W$ uma pequena quantidade proporcional à derivada de $J$ com relação a esse elemento para reduzir ligeiramente o custo.\n","\n","A mesma explicação dada acima no contexto da matriz de pesos ($W$) pode ser dada no contexto do vetor de viéses ($b$)."]},{"cell_type":"code","execution_count":259,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":374,"status":"ok","timestamp":1639078413706,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"joiNXXdcE6nw","outputId":"dccfa1b7-6bbc-489c-8024-fd8af44e0421"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 1.1099,  0.8805,  0.1600],\n","        [-1.1625, -1.7516, -1.2653],\n","        [-0.6812, -1.8468, -0.5367]], requires_grad=True)\n","None\n","\n","tensor([ 0.4356,  0.3716, -0.6117], requires_grad=True)\n","None\n"]}],"source":["print(W)\n","print(W.grad)\n","print()\n","print(b)\n","print(b.grad)"]},{"cell_type":"code","execution_count":260,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"elapsed":418,"status":"error","timestamp":1639078486582,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"8FGtY5BfD0PF","outputId":"052e9b7e-aeb5-47bd-e020-d5cac6d6ffe4"},"outputs":[],"source":["loss.backward()\n","with torch.no_grad():\n","  W = W - W.grad * 1e-5\n","  b = b - b.grad * 1e-5"]},{"cell_type":"markdown","metadata":{"id":"RpVjv6ePXF60"},"source":["Agora, após a alteração dos parâmetros podemos computar novamente o valor da função de custo e comparar com o obtido anteriormente."]},{"cell_type":"code","execution_count":261,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1638722811158,"user":{"displayName":"Eduardo Bezerra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgtLlrVPd1NAOukzvdhFbWNfJNyiPGoaltpqHbN_A=s64","userId":"15338717559428153474"},"user_tz":180},"id":"BzzKkqUtFHei","outputId":"d2abc9a5-650b-4193-f546-256504952f67"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(73915.9062)\n"]}],"source":["y_pred = model(X)\n","loss = mse(y_pred, y)\n","print(loss)"]},{"cell_type":"code","execution_count":264,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(73915.9062)"]},"execution_count":264,"metadata":{},"output_type":"execute_result"}],"source":["loss"]},{"cell_type":"markdown","metadata":{"id":"4KErzcNHW6E4"},"source":["Compare o valor acima com o valor inicial da função de custo (i.e., o obtido com a configuração aleatória de valores dos parâmetros). Você deve perceber que de fato houve uma diminuição do valor da função de custo."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyORrFI77sy+A7hMq1hDZrpt","collapsed_sections":["HUgWHHy2-jAt","7UiSYucbBekb","9nuW6vS-k8Fd","fYFGuMIrKzve","rN8bCwUEGcS3","kQSU3Al3MRF-","VScLnD8fFbbi","HaaXsOXae7Le","UT8PKx981tac","Nf4peUg6n1Rg","DMRyL-A8Bt6Y","0HrCwSCLDEgU","5KyKqTE-FLb8"],"name":"DL01-fundamentos.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
