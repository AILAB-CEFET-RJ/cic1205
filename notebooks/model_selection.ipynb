{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wMwXl-eL6Sx"
   },
   "source": [
    "# Testing on the training data\n",
    "\n",
    "**DO NOT DO IT**, since it is methodologically wrong! You almost surely will get an biased (i.e. optimistic) estimate of the generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rg4PPnvSLe5Q",
    "outputId": "768a324a-8e0d-493b-9138-62d0eec1b0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=2, random_state=0)\n",
    "\n",
    "# X is our training data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# This will result in an overly optimistic estimation since we are using X again!\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "acc = accuracy_score(y, y_pred)\n",
    "print(f'Accuracy: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGluy49qLjeJ"
   },
   "source": [
    "## Two-way holdout\n",
    "\n",
    "The **two-way hold-out method** is a straightforward technique used in model selection where the available dataset is split into two separate subsets: one for **training** the model and another for **testing** its performance. Typically, the data is divided into a fixed ratio (e.g., 70% training and 30% testing), ensuring that the model learns patterns from the training set and is then evaluated on the unseen test set to estimate its generalization ability. This method helps prevent overfitting by providing an unbiased assessment of how the model performs on new data, but it can be sensitive to how the data is split, especially if the dataset is small or unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aj3TlkZWLkj9",
    "outputId": "ce08d511-e61f-41a4-d2db-623a64b51e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# test with unseen data\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4hoss4eL4IF"
   },
   "source": [
    "# k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-_Yf27B-L4pL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import timeit\n",
    "\n",
    "def do_cross_validation(clf, print_model=False, print_duration=False):\n",
    "    start = timeit.default_timer()\n",
    "    cv = cross_validate(clf, X, y, scoring='accuracy', cv=3)\n",
    "    scores = ' + '.join(f'{s:.2f}' for s in cv[\"test_score\"])\n",
    "    mean_ = cv[\"test_score\"].mean()\n",
    "    msg = f'Cross-validated accuracy: ({scores}) / 3 = {mean_:.2f}'\n",
    "\n",
    "    if print_model:\n",
    "        msg = f'\\nClassifier: {clf}\\n{msg}'\n",
    "        print(msg)\n",
    "\n",
    "    if print_duration:\n",
    "        duration = timeit.default_timer() - start\n",
    "        print(f\"Duration: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfrxHB93Y_PP",
    "outputId": "aadc11b0-d8d8-4ddb-c8a0-374a0ec6b4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: RandomForestClassifier(n_estimators=2, random_state=0)\n",
      "Cross-validated accuracy: (0.98 + 0.92 + 0.96) / 3 = 0.95\n",
      "\n",
      "Duration: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=2, random_state=0)\n",
    "do_cross_validation(clf, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_qkItbFOEPP"
   },
   "source": [
    "## Applying $k$-fold cross-validation for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AfExmrWAOHMc",
    "outputId": "c9133b2a-429e-430e-88f5-6b4937fc984d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default value for kernel:  rbf\n",
      "\n",
      "Classifier: SVC(random_state=0)\n",
      "Cross-validated accuracy: (0.96 + 0.98 + 0.94) / 3 = 0.96\n",
      "Duration: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "start = timeit.default_timer()\n",
    "svc = SVC(random_state=0)\n",
    "print('Default value for kernel: ', svc.kernel)\n",
    "do_cross_validation(svc, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz7sqaKQOYFo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoxscNzYObI6",
    "outputId": "fbffe700-569b-4886-90ae-6325c203a863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: SVC(kernel='linear', random_state=0)\n",
      "Cross-validated accuracy: (1.00 + 1.00 + 0.98) / 3 = 0.99\n",
      "\n",
      "\n",
      "Classifier: SVC(kernel='poly', random_state=0)\n",
      "Cross-validated accuracy: (0.98 + 0.94 + 0.98) / 3 = 0.97\n",
      "\n",
      "\n",
      "Classifier: RandomForestClassifier(n_estimators=2, random_state=0)\n",
      "Cross-validated accuracy: (0.98 + 0.92 + 0.96) / 3 = 0.95\n",
      "\n",
      "\n",
      "Classifier: RandomForestClassifier(n_estimators=5, random_state=0)\n",
      "Cross-validated accuracy: (0.98 + 0.94 + 0.94) / 3 = 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do_cross_validation(SVC(kernel='linear', random_state=0), print_model=True)\n",
    "do_cross_validation(SVC(kernel='poly', random_state=0), print_model=True)\n",
    "do_cross_validation(RandomForestClassifier(n_estimators=2, random_state=0), print_model=True)\n",
    "do_cross_validation(RandomForestClassifier(n_estimators=5, random_state=0), print_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YasAqT0fPeKe"
   },
   "source": [
    "# Nested cross-validation\n",
    "\n",
    "Nested cross-validation is a technique that provides an unbiased estimate of a model's generalization performance while also allowing for hyperparameter tuning. It involves two loops: \n",
    "- an outer loop, which splits the data into training and testing folds to assess model performance, \n",
    "- an inner loop, which further splits the training data for model selection and hyperparameter tuning (typically using cross-validation again). \n",
    "\n",
    "This structure ensures that the test data in the outer loop remains completely unseen during model selection, preventing information leakage and leading to a more reliable performance estimate, especially when comparing models or tuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GridSearchCV = Inner loop (model selection via cross-validation).\n",
    "- do_cross_validation() = Outer loop (model evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qggaX-dPhsQ",
    "outputId": "913308ae-92da-43d9-d69c-13dc928a99f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0.17338370000015857\n",
      "Classifier: GridSearchCV(estimator=RandomForestClassifier(random_state=0),\n",
      "             param_grid={'n_estimators': [2, 5]})\n",
      "Cross-validated accuracy: (0.98 + 0.92 + 0.96) / 3 = 0.95\n",
      "\n",
      "\n",
      "Duration: 0.055051600000297185\n",
      "Classifier: GridSearchCV(estimator=SVC(random_state=0),\n",
      "             param_grid={'kernel': ['linear', 'poly']})\n",
      "Cross-validated accuracy: (1.00 + 0.94 + 0.98) / 3 = 0.97\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# random forest inner loop\n",
    "clf_grid = GridSearchCV(RandomForestClassifier(random_state=0), param_grid={'n_estimators': [2, 5]})\n",
    "# random forest outer loop\n",
    "do_cross_validation(clf_grid, print_model=True, print_duration=True)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# svc inner loop\n",
    "svc_grid = GridSearchCV(SVC(random_state=0), param_grid={'kernel': ['linear', 'poly']})\n",
    "# svc outer loop\n",
    "do_cross_validation(svc_grid, print_model=True, print_duration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV - getting the final model\n",
    "\n",
    "Nested cross-validation itself doesn't directly produce a final model. Rather, it is a technique to get an unbiased estimated of the generalization error. \n",
    "\n",
    "There are three alternative approaches to produce the final model **after** using nested CV. \n",
    "1. The final model is produced by training on the entire dataset, and using the best hyperparameters found during the inner loop.\n",
    "2. The final model is produced using the algorithm selected in the inner loop, but performing an additional hyperparameter setting on the whole dataset.\n",
    "3. (Ensemble Model) The final model is built as an ensemble model by combining predictions from the multiple models trained in the inner loop.\n",
    "\n",
    "Approaches 1 and 2 are the most common ones. Both involve using the entire dataset to refit a model AFTER the generalization error has been estimated.\n",
    "\n",
    "Notice that in all of the three approaches described above, the estimate of the generalization error to be reported is the one resulting from the nested CV procedure. \n",
    "\n",
    "The neste CV method ensures a rigorous evaluation of model performance, with independent hyperparameter tuning in each outer fold, which is critical in avoiding overfitting and data leakage.\n",
    "\n",
    "The code blocks below provide examples of using the second approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV - Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: svc\n",
      "Accuracy in the 3 outer folds: [0.92814371 0.8978979  0.87387387].\n",
      "Average acc: 0.8999718281155408\n",
      "\n",
      "Model: rfc\n",
      "Accuracy in the 3 outer folds: [0.9491018  0.94594595 0.89489489].\n",
      "Average acc: 0.9299808790826756\n",
      "\n",
      "Average score across the outer folds:  {'svc': 0.8999718281155408, 'rfc': 0.9299808790826756}\n",
      "\n",
      "****************************************************************************************************\n",
      "Now we choose the best model and refit on the whole dataset\n",
      "****************************************************************************************************\n",
      "\n",
      "Best model: \n",
      "\tRandomForestClassifier()\n",
      "\n",
      "Estimation of its generalization error (accuracy):\n",
      "\t0.9299808790826756\n",
      "\n",
      "Best parameter choice for this model: \n",
      "\t{'max_depth': 500}\n",
      "(according to cross-validation `KFold(n_splits=3, random_state=None, shuffle=False)` on the whole dataset).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# `outer_cv` creates 3 folds for estimating generalization error\n",
    "outer_cv = KFold(3)\n",
    "\n",
    "# when we train on a certain fold, we use a second cross-validation\n",
    "# split in order to choose hyperparameters\n",
    "inner_cv = KFold(3)\n",
    "\n",
    "# create some regression data\n",
    "X, y = make_classification(n_samples=1000, n_features=10)\n",
    "\n",
    "# give shorthand names to models and use those as dictionary keys mapping\n",
    "# to models and parameter grids for that model\n",
    "models_and_parameters = {\n",
    "    'svc': (SVC(),\n",
    "            {'C': [0.01, 0.05, 0.1, 1]}),\n",
    "    'rfc': (RandomForestClassifier(),\n",
    "           {'max_depth': [5, 10, 50, 100, 200, 500]})}\n",
    "\n",
    "# we will collect the average of the scores on the 3 outer folds in this dictionary\n",
    "# with keys given by the names of the models in `models_and_parameters`\n",
    "average_scores_across_outer_folds_for_each_model = dict()\n",
    "\n",
    "# find the model with the best generalization error\n",
    "for name, (model, params) in models_and_parameters.items():\n",
    "    # this object is a regressor that also happens to choose\n",
    "    # its hyperparameters automatically using `inner_cv`\n",
    "    classifier_that_optimizes_its_hyperparams = GridSearchCV(\n",
    "        estimator=model, param_grid=params,\n",
    "        cv=inner_cv, scoring='accuracy')\n",
    "\n",
    "    # estimate generalization error on the 3-fold splits of the data\n",
    "    scores_across_outer_folds = cross_val_score(\n",
    "        classifier_that_optimizes_its_hyperparams,\n",
    "        X, y, cv=outer_cv, scoring='accuracy')\n",
    "\n",
    "    # get the mean accuracy across each of outer_cv's 3 folds\n",
    "    average_scores_across_outer_folds_for_each_model[name] = np.mean(scores_across_outer_folds)\n",
    "    error_summary = 'Model: {name}\\nAccuracy in the 3 outer folds: {scores}.\\nAverage acc: {avg}'\n",
    "    print(error_summary.format(\n",
    "        name=name, scores=scores_across_outer_folds,\n",
    "        avg=np.mean(scores_across_outer_folds)))\n",
    "    print()\n",
    "\n",
    "print('Average score across the outer folds: ',\n",
    "      average_scores_across_outer_folds_for_each_model)\n",
    "\n",
    "many_stars = '\\n' + '*' * 100 + '\\n'\n",
    "print(many_stars + 'Now we choose the best model and refit on the whole dataset' + many_stars)\n",
    "\n",
    "best_model_name, best_model_avg_score = max(\n",
    "    average_scores_across_outer_folds_for_each_model.items(),\n",
    "    key=(lambda name_averagescore: name_averagescore[1]))\n",
    "\n",
    "# get the best model and its associated parameter grid\n",
    "best_model, best_model_params = models_and_parameters[best_model_name]\n",
    "\n",
    "# Approach 1\n",
    "#best_model.fit(X, y)\n",
    "\n",
    "# Approach 2: # refit this best model on the whole dataset so that we can start\n",
    "# making predictions on other data, and now we have a reliable estimate of\n",
    "# this model's generalization error and we are confident this is the best model\n",
    "# among the ones we have tried\n",
    "final_classifier = GridSearchCV(best_model, best_model_params, cv=inner_cv)\n",
    "final_classifier.fit(X, y)\n",
    "\n",
    "print('Best model: \\n\\t{}'.format(best_model), end='\\n\\n')\n",
    "print('Estimation of its generalization error (accuracy):\\n\\t{}'.format(\n",
    "    best_model_avg_score), end='\\n\\n')\n",
    "print('Best parameter choice for this model: \\n\\t{params}'\n",
    "      '\\n(according to cross-validation `{cv}` on the whole dataset).'.format(\n",
    "      params=final_classifier.best_params_, cv=inner_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV - Regression example I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: svr\n",
      "MSE in the 3 outer folds: [-22100.00522677 -23269.74391012 -25705.88313907].\n",
      "Average error: -23691.877425318682\n",
      "\n",
      "Model: rfr\n",
      "MSE in the 3 outer folds: [-3751.12083033 -3976.42008502 -4629.63215186].\n",
      "Average error: -4119.05768906673\n",
      "\n",
      "Average score across the outer folds:  {'svr': -23691.877425318682, 'rfr': -4119.05768906673}\n",
      "\n",
      "****************************************************************************************************\n",
      "Now we choose the best model and refit on the whole dataset\n",
      "****************************************************************************************************\n",
      "\n",
      "Best model: \n",
      "\tRandomForestRegressor()\n",
      "\n",
      "Estimation of its generalization error (negative mean squared error):\n",
      "\t-4119.05768906673\n",
      "\n",
      "Best parameter choice for this model: \n",
      "\t{'max_depth': 50}\n",
      "(according to cross-validation `KFold(n_splits=3, random_state=None, shuffle=False)` on the whole dataset).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "\n",
    "# `outer_cv` creates 3 folds for estimating generalization error\n",
    "outer_cv = KFold(3)\n",
    "\n",
    "# when we train on a certain fold, we use a second cross-validation\n",
    "# split in order to choose hyperparameters\n",
    "inner_cv = KFold(3)\n",
    "\n",
    "# create some regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=10)\n",
    "\n",
    "# give shorthand names to models and use those as dictionary keys mapping\n",
    "# to models and parameter grids for that model\n",
    "models_and_parameters = {\n",
    "    'svr': (SVR(),\n",
    "            {'C': [0.01, 0.05, 0.1, 1]}),\n",
    "    'rfr': (RandomForestRegressor(),\n",
    "           {'max_depth': [5, 10, 50, 100, 200, 500]})}\n",
    "\n",
    "# we will collect the average of the scores on the 3 outer folds in this dictionary\n",
    "# with keys given by the names of the models in `models_and_parameters`\n",
    "average_scores_across_outer_folds_for_each_model = dict()\n",
    "\n",
    "# find the model with the best generalization error\n",
    "for name, (model, params) in models_and_parameters.items():\n",
    "    # this object is a regressor that also happens to choose\n",
    "    # its hyperparameters automatically using `inner_cv`\n",
    "    regressor_that_optimizes_its_hyperparams = GridSearchCV(estimator = model, \n",
    "                                                            param_grid = params,\n",
    "                                                            cv = inner_cv, \n",
    "                                                            scoring = 'neg_mean_squared_error')\n",
    "\n",
    "    # estimate generalization error on the 3-fold splits of the data\n",
    "    scores_across_outer_folds = cross_val_score(regressor_that_optimizes_its_hyperparams,\n",
    "                                                X, \n",
    "                                                y, \n",
    "                                                cv = outer_cv, \n",
    "                                                scoring='neg_mean_squared_error')\n",
    "\n",
    "    # get the mean MSE across each of outer_cv's 3 folds\n",
    "    average_scores_across_outer_folds_for_each_model[name] = np.mean(scores_across_outer_folds)\n",
    "    error_summary = 'Model: {name}\\nMSE in the 3 outer folds: {scores}.\\nAverage error: {avg}'\n",
    "    print(error_summary.format(\n",
    "        name=name, scores=scores_across_outer_folds,\n",
    "        avg=np.mean(scores_across_outer_folds)))\n",
    "    print()\n",
    "\n",
    "print('Average score across the outer folds: ',\n",
    "      average_scores_across_outer_folds_for_each_model)\n",
    "\n",
    "many_stars = '\\n' + '*' * 100 + '\\n'\n",
    "print(many_stars + 'Now we choose the best model and refit on the whole dataset' + many_stars)\n",
    "\n",
    "best_model_name, best_model_avg_score = max(\n",
    "    average_scores_across_outer_folds_for_each_model.items(),\n",
    "    key=(lambda name_averagescore: name_averagescore[1]))\n",
    "\n",
    "# get the best model and its associated parameter grid\n",
    "best_model, best_model_params = models_and_parameters[best_model_name]\n",
    "\n",
    "# now we refit this best model on the whole dataset so that we can start\n",
    "# making predictions on other data, and now we have a reliable estimate of\n",
    "# this model's generalization error and we are confident this is the best model\n",
    "# among the ones we have tried\n",
    "final_regressor = GridSearchCV(best_model, best_model_params, cv=inner_cv)\n",
    "final_regressor.fit(X, y)\n",
    "\n",
    "print('Best model: \\n\\t{}'.format(best_model), end='\\n\\n')\n",
    "print('Estimation of its generalization error (negative mean squared error):\\n\\t{}'.format(\n",
    "    best_model_avg_score), end='\\n\\n')\n",
    "print('Best parameter choice for this model: \\n\\t{params}'\n",
    "      '\\n(according to cross-validation `{cv}` on the whole dataset).'.format(\n",
    "      params=final_regressor.best_params_, cv=inner_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.5.2.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV - Regression example II\n",
    "\n",
    "In this second example of using nested cross-validation for a regression task, [GridSearchCV](https://scikit-learn.org/1.5/modules/grid_search.html) is applied for hyperparameter tuning in the inner cross-validation loop and `cross_val_score` with `KFold` for the outer loop. In this example, the Ridge regression learning algorithm is used. This algorithm has a regularization parameter `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested CV Mean Squared Error: 0.5306 ± 0.0217\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset (using California Housing dataset as an example)\n",
    "try:\n",
    "    data = fetch_california_housing(as_frame=True)\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "except KeyError:\n",
    "    print(\"Error loading dataset. Please ensure you have internet access and try again.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Define the parameter grid for the inner cross-validation (for Ridge Regression)\n",
    "param_grid = {\n",
    "    'ridge__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Outer cross-validation (5-fold)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# List to store outer CV scores\n",
    "outer_scores = []\n",
    "\n",
    "# Nested Cross-Validation\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    # Split data into training and test sets for the outer fold\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Define a pipeline to scale features and apply Ridge Regression\n",
    "    pipeline = make_pipeline(StandardScaler(), Ridge())\n",
    "    \n",
    "    # Inner cross-validation with GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Fit the model on the training data of the outer fold\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Use the best model to predict the outer test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate the Mean Squared Error on the outer test set\n",
    "    outer_score = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Append the outer score\n",
    "    outer_scores.append(outer_score)\n",
    "\n",
    "# Display the average and standard deviation of outer CV scores\n",
    "print(f\"Nested CV Mean Squared Error: {np.mean(outer_scores):.4f} ± {np.std(outer_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running nested CV, we obtain an unbiased estimate of model performance with the optimal hyperparameters. However, nested CV doesn't directly provide a final, production-ready model. To train the final model with the best hyperparameters for production, follow these steps:\n",
    "\n",
    "1. Identify the best hyperparameters: Check which hyperparameters performed best on average across the inner folds of nested CV. You can retrieve these from the GridSearchCV object if needed or simply use the hyperparameter configuration that provided the lowest mean score in your nested CV loop.\n",
    "\n",
    "2. Train the final model on the entire dataset: Use the entire dataset (without splitting) to fit the model using the optimal hyperparameters. This will ensure the model has learned from all available data, maximizing its predictive power.\n",
    "\n",
    "3. Save the model: After training the final model, save it to disk for future use in production.\n",
    "\n",
    "Here's the Python code to build the final model using the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained and saved as 'final_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib  # For saving the model\n",
    "\n",
    "# Step 1: Define the best hyperparameters\n",
    "best_alpha = 1  # Replace with the best 'alpha' found in nested CV\n",
    "\n",
    "# Step 2: Train the final model on the entire dataset\n",
    "pipeline = make_pipeline(StandardScaler(), Ridge(alpha=best_alpha))\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(pipeline, \"final_model.joblib\")\n",
    "print(\"Final model trained and saved as 'final_model.joblib'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code provided above, notice the following:\n",
    "\n",
    "- Best Hyperparameters: Set `best_alpha` to the optimal value found during nested CV. This may vary depending on the dataset, so replace 1 with the actual optimal value obtained.\n",
    "\n",
    "- Training on Full Dataset: Training on the entire dataset with the chosen hyperparameters provides the final model that has learned from all data points.\n",
    "\n",
    "- Saving the Model: `joblib` is useful for saving Scikit-Learn models efficiently. The saved model can then be loaded in a production environment for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing between 2-way holdout and 3-way holdout\n",
    "\n",
    "When choosing the split proportions in two-way holdout and three-way holdout methods, several aspects influence the optimal choice. These aspects include dataset size, model complexity, and the need for robust model evaluation. Choosing split proportions that fit the data characteristics, model complexity, and evaluation priorities ensures robust model selection and reliable performance estimation. The discussino below provides a breakdown of considerations for each method.\n",
    "\n",
    "## Two-Way Holdout Method\n",
    "In the two-way holdout method, the dataset is split into two parts: typically a training set and a test set.\n",
    "\n",
    "**Split Proportions**\n",
    "Common choices are:\n",
    "- 80/20: 80% training, 20% testing.\n",
    "- 70/30: 70% training, 30% testing.\n",
    "- 90/10: 90% training, 10% testing (often used for very large datasets).\n",
    "\n",
    "**Aspects to Consider**\n",
    "\n",
    "1. **Dataset Size:**\n",
    "    - Small Datasets: A larger portion (e.g., 80–90%) should be dedicated to training to allow the model to learn enough patterns from limited data. However, this limits the amount of data left for testing, potentially making performance estimates less reliable.\n",
    "    - Large Datasets: You can afford to allocate a larger portion to testing (e.g., 30%) since there is usually enough data in the training set to achieve stable model training.\n",
    "\n",
    "2. **Model Complexity:**\n",
    "    - Simple Models (e.g., linear regression): Simple models with fewer parameters may need less training data to achieve a stable fit. In this case, a split like 70/30 may work well.\n",
    "    - Complex Models (e.g., deep learning): These models require substantial data to generalize well, so an 80/20 or 90/10 split is often more appropriate, especially if the dataset is small.\n",
    "\n",
    "3. **Evaluation Goals:**\n",
    "\n",
    "    - If generalization is a top priority (e.g., avoiding overfitting in a complex model), a larger test set (e.g., 70/30) provides a more reliable estimate of performance.\n",
    "    - If the primary focus is model performance with limited data, using a smaller test set (e.g., 90/10) allows more training data, which can enhance model performance but may slightly reduce confidence in the test results.\n",
    "\n",
    "## Three-Way Holdout Method\n",
    "In the three-way holdout method, the dataset is split into three parts: a training set, a validation set, and a test set.\n",
    "\n",
    "**Split Proportions**\n",
    "\n",
    "Typical splits include:\n",
    "- 60/20/20: 60% training, 20% validation, 20% test.\n",
    "- 70/15/15: 70% training, 15% validation, 15% test.\n",
    "- 80/10/10: 80% training, 10% validation, 10% test.\n",
    "\n",
    "**Aspects to Consider**\n",
    "\n",
    "1. **Dataset Size:**\n",
    "    - Small Datasets: A 60/20/20 split might be used to ensure adequate data in the validation and test sets for reliable model evaluation, but it reduces training data. Alternatively, a 70/15/15 split may strike a better balance, giving slightly more data for training.\n",
    "    - Large Datasets: With more data, an 80/10/10 split can allocate a larger training set to boost model performance while still leaving enough data in validation and test sets for reliable evaluation.\n",
    "\n",
    "2. **Purpose of Validation Set:**\n",
    "    - Hyperparameter Tuning: The validation set is crucial for hyperparameter tuning and model selection. If fine-tuning the model is critical, having 15–20% of the data as validation ensures more stable results, especially if the dataset is large.\n",
    "    - Early Stopping in Training: If using the validation set for early stopping (common in deep learning), a larger training set is beneficial (e.g., 70/15/15) to provide stability in validation metrics while maximizing training data.\n",
    "\n",
    "3. **Final Evaluation Goals:**\n",
    "    - Minimizing Overfitting: A larger test set (e.g., 20%) helps give a more robust final evaluation of model generalization, especially if it will be used in critical applications where overfitting could be harmful.\n",
    "    - Ensuring Training Robustness: In cases where training stability is prioritized (e.g., complex models with a large number of parameters), an 80/10/10 split can maximize training data while still providing reasonable validation and test insights.\n",
    "\n",
    "## General Recommendations\n",
    "- Small Datasets: 70/30 for two-way, 70/15/15 for three-way splits, as more training data is often necessary.\n",
    "- Medium to Large Datasets: 80/20 or 90/10 for two-way, and 80/10/10 for three-way splits. With larger datasets, holding out more data for validation and testing becomes feasible.\n",
    "- Complex Models and Hyperparameter Tuning: A three-way split with a larger training set and moderate validation set (e.g., 70/15/15) helps in fine-tuning while balancing generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the right evaluation metric \n",
    "\n",
    "We saw that model selection encompasses *model evaluation*, which is the process of assessing how well a model generalizes. Each evaluation metric has strengths and weaknesses. Therefore, understanding the context and priorities of the task is essential for selecting the best evaluation metric to be used during model selection.\n",
    "\n",
    "## Classification\n",
    "\n",
    "When evaluating binary classification models, it’s common to encounter situations where certain types of predictions carry different levels of importance or risk. Understanding these scenarios is crucial for choosing and tuning the appropriate model to meet real-world needs. \n",
    "\n",
    "Concretely, there are four main scenarios that can happen and that can influence choice of evaluation metric during model selection: **bad positives**, **harmless negatives**, **unbalanced classes**, and **unequal costs for predictions**. By choosing the appropriate evaluation metric, thresholds, and techniques during model seleciton, practitioners can tailor models to meet the specific needs and constraints of each application.\n",
    "\n",
    "1. **Bad Positives**. Bad positives refer to instances where the model predicts a positive outcome (1) incorrectly. This type of error is often called a **false positive**. For example, in medical diagnosis, a false positive for a severe disease could cause unnecessary anxiety for a patient and lead to costly, invasive follow-up tests.\n",
    "> When false positives are highly detrimental, precision should be prioritized, meaning the model should avoid predicting positives unless it’s very confident.\n",
    "\n",
    "2. **Harmless Negatives**. Harmless negatives refer to cases where predicting a negative outcome (0) incorrectly (i.e., a **false negative**) has a minimal or acceptable impact. For example, in spam detection, marking a legitimate email as spam (false negative) can be harmless if users are able to recover it from a spam folder.\n",
    "> When false negatives are acceptable or pose minimal risk, the model may prioritize detecting positives more confidently, sometimes sacrificing sensitivity to capture more positives accurately.\n",
    "\n",
    "3. **Unbalanced Classes** Unbalanced classes occur when the number of examples in each class differs significantly, with one class being much more frequent than the other. For example, in fraud detection, fraudulent transactions (positive class) are typically much rarer than legitimate transactions (negative class).\n",
    "> Standard accuracy as a metric may be misleading in such cases because the model could achieve high accuracy by mostly predicting the majority class. Techniques such as **resampling** (oversampling the minority class or undersampling the majority class) and using metrics like **F1 score** or **AUC-ROC** are more effective for evaluating performance in these cases.\n",
    "\n",
    "4. **Unequal Costs for Predictions**. In many real-world applications, the cost of different types of errors (false positives vs. false negatives) varies. This refers to unequal costs of predictions. For example, in a credit risk assessment, incorrectly granting a loan to a risky applicant (false positive) might have a high financial cost, while denying a loan to a low-risk applicant (false negative) might incur only a minor loss in potential business.\n",
    "  > For cases with unequal costs, it’s essential to balance the prediction threshold to account for these cost disparities. **Cost-sensitive learning** and setting a **custom decision threshold** are common strategies, and models may be evaluated using **weighted cost metrics** that reflect the different impacts of each error type.\n",
    "\n",
    "## Regression\n",
    "\n",
    "When choosing an evaluation metric for a regression task, it’s essential to consider the specific goals and characteristics of the problem. Here are some key aspects to guide the choice of an appropriate evaluation metric:\n",
    "\n",
    "1. **Type and Scale of Errors**. If large errors are more detrimental than smaller ones, metrics like **Mean Squared Error (MSE)** or **Root Mean Squared Error (RMSE)**, which penalize larger errors more, may be suitable. For cases where each error should be weighted equally (regardless of size), **Mean Absolute Error (MAE)** is often a good choice, as it gives a linear penalty to errors.\n",
    "\n",
    "2. **Interpretability and Communication**. **RMSE** and **MAE** are in the same units as the target variable, making them interpretable and easy to communicate. [Mean Absolute Percentage Error](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) (**MAPE**) provides an error as a percentage of actual values, which can be useful for interpreting results across different scales or for communicating model performance to stakeholders.\n",
    "\n",
    "3. **Sensitivity to Outliers**. If the dataset contains outliers, **MAE** might be a better choice because it is less sensitive to extreme values than **MSE** or **RMSE**, which emphasize large errors due to their squared terms.\n",
    "\n",
    "4. **Balance Between Positive and Negative Errors**. In cases where overestimations and underestimations have different costs, an asymmetric error metric (like **Quantile Loss**) may be appropriate, as it allows you to weigh over- and under-predictions differently.\n",
    "\n",
    "5. **Model Selection and Comparison**. Metrics like **RMSE** and **MAE** are popular because they provide consistent rankings across models and datasets. However, **Adjusted R-squared** can be more suitable when comparing models with different numbers of predictors, as it adjusts for model complexity.\n",
    "\n",
    "6. **Variance of Errors**. If the distribution of errors is a concern (e.g., for quality control in industrial processes), **R-squared (Coefficient of Determination)** may be useful, as it measures the proportion of variance in the target variable explained by the model.\n",
    "\n",
    "7. **Goal of the Application**. In applications like medical prediction or finance, high error tolerance may be unacceptable, making **RMSE** a better choice to keep larger errors in check. For other applications with more flexibility, **MAE** or **MAPE** may suffice.\n",
    "\n",
    "The table below presents a summary of the most common regression metrics.\n",
    "\n",
    "| Metric                | Description                                           | Use Case Example                                  |\n",
    "|-----------------------|-------------------------------------------------------|---------------------------------------------------|\n",
    "| **MAE**               | Average of absolute errors                            | Suitable for interpretable, low-outlier tasks     |\n",
    "| **MSE**               | Average of squared errors                             | Useful when penalizing large errors               |\n",
    "| **RMSE**              | Square root of MSE, interpretable in original units   | Sensitive to large errors, often used in finance  |\n",
    "| **R-squared**         | Proportion of explained variance                      | Ideal for comparing different models              |\n",
    "| **MAPE**              | Mean absolute percentage error                        | Useful when predicting across multiple scales     |\n",
    "| **Quantile Loss**     | Asymmetric error measure based on quantiles           | Good for applications needing over-/under-bias    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Model selection done right: A gentle introduction to nested cross-validation](https://ploomber.io/blog/nested-cv/).\n",
    "\n",
    "2. [Which is the final model from Nested Cross Validation: Accuracy or Frequency?](https://datascience.stackexchange.com/questions/116311/which-is-the-final-model-from-nested-cross-validation-accuracy-or-frequency)\n",
    "\n",
    "3. [What is the correct procedure for nested cross-validation?](https://stackoverflow.com/questions/64238730/what-is-the-correct-procedure-for-nested-cross-validation)\n",
    "\n",
    "4. [Nested Cross Validation (Cynthia Rudin)](https://youtu.be/az60jS7MQhU?list=PLNeXFnYrCJneoY_rKtWJy833YiMrCRi5f)\n",
    "\n",
    "5. [Nested cross-validation and selecting the best regression model - is this the right SKLearn process?](https://datascience.stackexchange.com/questions/13185/nested-cross-validation-and-selecting-the-best-regression-model-is-this-the-ri)\n",
    "\n",
    "6. [Model evaluation, model selection, and algorithm selection in machine learning](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html)\n",
    "\n",
    "7. [Nested cross validation for model selection](https://stats.stackexchange.com/questions/65128/nested-cross-validation-for-model-selection/65158#65158)\n",
    "\n",
    "8. [Training on the full dataset after cross-validation?](https://stats.stackexchange.com/questions/11602/training-on-the-full-dataset-after-cross-validation)\n",
    "\n",
    "9. [How to choose a predictive model after k-fold cross-validation?](https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation)\n",
    "\n",
    "10. [How to Train a Final Machine Learning Model](https://machinelearningmastery.com/train-final-machine-learning-model/)\n",
    "\n",
    "11. [How to get from evaluation to final model](https://mindfulmodeler.substack.com/p/how-to-get-from-evaluation-to-final)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
