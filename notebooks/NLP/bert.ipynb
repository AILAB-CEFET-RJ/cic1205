{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebezerra/anaconda3/envs/cic1205/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "Hidden States Shape: torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "# Load a pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define BERT configuration\n",
    "config = BertConfig(\n",
    "    hidden_size=768,  # Size of the hidden layers\n",
    "    num_hidden_layers=12,  # Number of transformer blocks\n",
    "    num_attention_heads=12,  # Number of attention heads\n",
    "    intermediate_size=3072,  # Size of the feed-forward layer\n",
    ")\n",
    "\n",
    "# Construct BERT model from config\n",
    "model = BertModel(config)\n",
    "\n",
    "# Display model architecture\n",
    "print(model)\n",
    "\n",
    "# Example text input\n",
    "text = \"BERT is a powerful transformer model.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass through BERT\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract hidden states\n",
    "hidden_states = outputs.last_hidden_state\n",
    "print(\"Hidden States Shape:\", hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden states tensor extracted from BERT in the previous example can be used in several practical applications. Below are some key use cases along with code snippets demonstrating how to leverage this tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT Hidden States in Practical Applications\n",
    "\n",
    "The **hidden states tensor** extracted from BERT can be used in several practical applications. Below are some key use cases along with **code snippets** demonstrating how to leverage this tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Text Classification**\n",
    "The hidden states from BERT can be used as **features** for a classification task.\n",
    "\n",
    "Applications:\n",
    "- Sentiment analysis (positive/negative review classification)\n",
    "- Spam detection\n",
    "- Document categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3253, -0.3554]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### **Example: Sentiment Analysis**\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode input text\n",
    "text = \"BERT is an amazing transformer model!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "hidden_states = outputs.last_hidden_state  # Shape: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "# Use the [CLS] token representation for classification\n",
    "cls_embedding = hidden_states[:, 0, :]  # Extract first token embedding\n",
    "\n",
    "# Example: Pass through a classification head\n",
    "classifier = nn.Linear(768, 2)  # Binary classification (e.g., positive/negative)\n",
    "logits = classifier(cls_embedding)\n",
    "print(logits)  # Output classification scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Named Entity Recognition (NER)**\n",
    "BERT’s hidden states can be used for **token-level classification** tasks like identifying names, dates, or organizations.\n",
    "\n",
    "Applications:\n",
    "- Extracting named entities (e.g., names, locations, organizations)\n",
    "- Legal and financial document analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulating a classifier for each token\n",
    "ner_classifier = nn.Linear(768, 5)  # Assume 5 entity classes (Person, Org, Date, etc.)\n",
    "\n",
    "# Pass hidden states through classifier\n",
    "token_logits = ner_classifier(hidden_states)\n",
    "token_probs = F.softmax(token_logits, dim=-1)\n",
    "\n",
    "print(token_probs.shape)  # Shape: [batch_size, seq_length, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Semantic Similarity / Sentence Embeddings**\n",
    "BERT embeddings can be used to compare how **similar two sentences are**.\n",
    "\n",
    "Applications:\n",
    "- Document similarity search\n",
    "- Duplicate question detection (e.g., Quora, StackOverflow)\n",
    "- Recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.9290674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "text1 = \"BERT is a transformer model.\"\n",
    "text2 = \"BERT is used in NLP tasks.\"\n",
    "\n",
    "inputs1 = tokenizer(text1, return_tensors=\"pt\")\n",
    "inputs2 = tokenizer(text2, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    output1 = model(**inputs1).last_hidden_state[:, 0, :]\n",
    "    output2 = model(**inputs2).last_hidden_state[:, 0, :]\n",
    "\n",
    "# Compute similarity\n",
    "similarity = cosine_similarity(output1.numpy(), output2.numpy())\n",
    "print(\"Similarity Score:\", similarity[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Question Answering**\n",
    "BERT’s hidden states can help **extract answers from text**.\n",
    "\n",
    "Applications:\n",
    "- Chatbots and virtual assistants\n",
    "- Automated document Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "qa_model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "question = \"What is BERT used for?\"\n",
    "context = \"BERT is a deep learning model used in natural language processing.\"\n",
    "\n",
    "# Encode question and context\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = qa_model(**inputs)\n",
    "\n",
    "start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "# Get the most likely start and end positions\n",
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "\n",
    "# Decode answer\n",
    "answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "| Application | How Hidden States Are Used |\n",
    "|-------------|---------------------------|\n",
    "| **Text Classification** | Use `[CLS]` token embedding as a feature for classification |\n",
    "| **Named Entity Recognition** | Use per-token hidden states to classify words into categories |\n",
    "| **Semantic Similarity** | Compare sentence embeddings using cosine similarity |\n",
    "| **Question Answering** | Identify answer spans using start/end token logits |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
