{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebezerra/anaconda3/envs/cic1205/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entity Recognition Results:\n",
      "\n",
      "[CLS]           -> O\n",
      "El              -> I-PER\n",
      "##on            -> I-PER\n",
      "Mu              -> I-PER\n",
      "##sk            -> I-PER\n",
      "is              -> O\n",
      "the             -> O\n",
      "CEO             -> O\n",
      "of              -> O\n",
      "Te              -> I-ORG\n",
      "##sla           -> I-ORG\n",
      "and             -> O\n",
      "Space           -> I-ORG\n",
      "##X             -> I-ORG\n",
      ",               -> O\n",
      "and             -> O\n",
      "he              -> O\n",
      "lives           -> O\n",
      "in              -> O\n",
      "California      -> I-LOC\n",
      ".               -> O\n",
      "[SEP]           -> O\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "def load_ner_model():\n",
    "    \"\"\"Load pre-trained BERT model for Named Entity Recognition.\"\"\"\n",
    "    model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForTokenClassification.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def predict_ner(text, tokenizer, model):\n",
    "    \"\"\"Predict named entities in the given text.\"\"\"\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs).logits\n",
    "    \n",
    "    predictions = torch.argmax(outputs, dim=2).numpy()[0]\n",
    "    id2label = model.config.id2label\n",
    "    \n",
    "    entities = [(token, id2label[predictions[i]]) for i, token in enumerate(tokens)]\n",
    "    return entities\n",
    "\n",
    "def display_results(entities):\n",
    "    \"\"\"Display the predicted entities in a readable format.\"\"\"\n",
    "    print(\"\\nNamed Entity Recognition Results:\\n\")\n",
    "    for token, label in entities:\n",
    "        print(f\"{token:15} -> {label}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"Elon Musk is the CEO of Tesla and SpaceX, and he lives in California.\"\n",
    "    tokenizer, model = load_ner_model()\n",
    "    entities = predict_ner(text, tokenizer, model)\n",
    "    display_results(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">Named Entity Recognition</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "M\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    us\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       "k\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "     is \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       "t\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    he\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CEO \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       "o\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    f \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "T\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    esl\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "a\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "     an\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "d\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "     S\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "p\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ac\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       "e\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    X, an\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       "d\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "     he\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    lives\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    in \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       "C\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    a\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "l\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ifo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "r\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ni\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "a\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    .\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">O</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "def visualize_ner(text, entities):\n",
    "    \"\"\"Visualize named entity recognition using spaCy's displaCy.\"\"\"\n",
    "    ents = []\n",
    "    start = 0\n",
    "    for token, label in entities:\n",
    "        ents.append({\"start\": start, \"end\": start + len(token), \"label\": label})\n",
    "        start += len(token) + 1  # Adjust for space\n",
    "\n",
    "    doc = {\"text\": text, \"ents\": ents, \"title\": \"Named Entity Recognition\"}\n",
    "    displacy.render(doc, style=\"ent\", manual=True, jupyter=True)\n",
    "\n",
    "# Example usage in Jupyter Notebook\n",
    "visualize_ner(text, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99209243,\n",
       "  'index': 1,\n",
       "  'word': 'El',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.8453797,\n",
       "  'index': 2,\n",
       "  'word': '##on',\n",
       "  'start': 2,\n",
       "  'end': 4},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99850273,\n",
       "  'index': 3,\n",
       "  'word': 'Mu',\n",
       "  'start': 5,\n",
       "  'end': 7},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9700687,\n",
       "  'index': 4,\n",
       "  'word': '##sk',\n",
       "  'start': 7,\n",
       "  'end': 9},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.99847037,\n",
       "  'index': 9,\n",
       "  'word': 'Te',\n",
       "  'start': 24,\n",
       "  'end': 26},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9957432,\n",
       "  'index': 10,\n",
       "  'word': '##sla',\n",
       "  'start': 26,\n",
       "  'end': 29},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.7222855,\n",
       "  'index': 11,\n",
       "  'word': 'and',\n",
       "  'start': 30,\n",
       "  'end': 33},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.97088474,\n",
       "  'index': 12,\n",
       "  'word': 'Space',\n",
       "  'start': 34,\n",
       "  'end': 39},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9989818,\n",
       "  'index': 13,\n",
       "  'word': '##X',\n",
       "  'start': 39,\n",
       "  'end': 40},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9996055,\n",
       "  'index': 19,\n",
       "  'word': 'California',\n",
       "  'start': 58,\n",
       "  'end': 68}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer) # Named Entity Recognition\n",
    "\n",
    "data = \"Elon Musk is the CEO of Tesla and SpaceX, and he lives in California.\"\n",
    "results = ner(data)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 'O'),\n",
       " ('El', 'B-PER'),\n",
       " ('##on', 'B-PER'),\n",
       " ('Mu', 'I-PER'),\n",
       " ('##sk', 'I-PER'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('CEO', 'O'),\n",
       " ('of', 'O'),\n",
       " ('Te', 'B-ORG'),\n",
       " ('##sla', 'I-ORG'),\n",
       " ('and', 'I-ORG'),\n",
       " ('Space', 'B-ORG'),\n",
       " ('##X', 'I-ORG'),\n",
       " (',', 'O'),\n",
       " ('and', 'O'),\n",
       " ('he', 'O'),\n",
       " ('lives', 'O'),\n",
       " ('in', 'O'),\n",
       " ('California', 'B-LOC'),\n",
       " ('.', 'O'),\n",
       " ('[SEP]', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = predict_ner(data, tokenizer, model)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">Named Entity Recognition</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    El\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       "o\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    n Mu\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
       "</mark>\n",
       "s\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    k \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       "i\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    s th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
       "</mark>\n",
       "e\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "     C\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       "E\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    O of \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       "T\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    esl\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       "a\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "     and \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
       "</mark>\n",
       "S\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pac\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
       "</mark>\n",
       "e\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    X, and he \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
       "</mark>\n",
       "lives in California.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_ner(data, [(entity[\"word\"], entity[\"entity\"]) for entity in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ebezerra/anaconda3/envs/cic1205/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Training: 100%|██████████| 1/1 [00:04<00:00,  4.73s/it]\n",
      "Training: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split \n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "# Assuming a predefined set of entity types\n",
    "entity_types = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "# Set num_labels\n",
    "num_labels = len(entity_types)\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "# Define batch_size\n",
    "batch_size = 32  # Adjust as needed\n",
    "# Define learning rate\n",
    "learning_rate = 5e-5  # Adjust as needed\n",
    "# Sample data from CoNLL-2003 (Replace this with your dataset) \n",
    "train_dataset_sample = [\n",
    "    {\"text\": \"John works at Google in New York.\", \"labels\": {\"entities\": [(0, 4, \"PERSON\"), (17, 22, \"ORG\"), (26, 34, \"GPE\")]}},\n",
    "    {\"text\": \"Apple Inc. is a technology company.\", \"labels\": {\"entities\": [(0, 10, \"ORG\")]}},     # Add more samples as needed\n",
    "]\n",
    "def tokenize_and_format_data(dataset, tokenizer):\n",
    "    tokenized_data = []\n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"]\n",
    "        entities = sample[\"labels\"][\"entities\"]\n",
    "        # Tokenize the input text using the BERT tokenizer\n",
    "\n",
    "        tokens =  tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "        # Initialize labels for each token as 'O' (Outside)\n",
    "        labels = ['O'] * len(tokens)\n",
    "        # Update labels for entity spans\n",
    "        for start, end, entity_type in entities:\n",
    "            # Tokenize the prefix to get the correct offset\n",
    "            prefix_tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text[:start])))\n",
    "            start_token = len(prefix_tokens)\n",
    "            # Tokenize the entity to get its length\n",
    "            entity_tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text[start:end])))\n",
    "            end_token = start_token + len(entity_tokens) - 1\n",
    "            labels[start_token] = f\"B-{entity_type}\"\n",
    "\n",
    "    for i in range(start_token + 1, end_token +1):\n",
    "        labels[i] = f\"I-{entity_type}\"\n",
    "\n",
    "        # Convert tokens and labels to input IDs and label IDs\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        label_ids = [entity_types.index(label) for label in labels]\n",
    "        # Pad input_ids and label_ids to the maximum sequence length\n",
    "        padding_length = tokenizer.model_max_length - len(input_ids)\n",
    "        input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        label_ids += [entity_types.index('O')] * padding_length\n",
    "    tokenized_data.append({'input_ids': input_ids, 'labels': label_ids\n",
    "        })\n",
    "    # Convert tokenized data to PyTorch dataset\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor([item['input_ids'] for item in tokenized_data]),               \n",
    "        torch.tensor([item['labels'] for item in tokenized_data])\n",
    "    )\n",
    "    return dataset\n",
    "# Prepare data for fine-tuning\n",
    "train_data = tokenize_and_format_data(train_dataset_sample, tokenizer) \n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "# Fine-tune the model\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate) \n",
    "num_epochs = 15  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        # Unpack the tuple\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss =  outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "# Save the fine-tuned model for later use model.save_pretrained('fine_tuned_ner_model')\n",
    "model.save_pretrained('fine_tuned_ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 'LABEL_0'),\n",
       " ('el', 'LABEL_0'),\n",
       " ('##on', 'LABEL_0'),\n",
       " ('mu', 'LABEL_0'),\n",
       " ('##sk', 'LABEL_0'),\n",
       " ('is', 'LABEL_0'),\n",
       " ('the', 'LABEL_0'),\n",
       " ('ceo', 'LABEL_0'),\n",
       " ('of', 'LABEL_0'),\n",
       " ('tesla', 'LABEL_0'),\n",
       " ('and', 'LABEL_0'),\n",
       " ('space', 'LABEL_0'),\n",
       " ('##x', 'LABEL_0'),\n",
       " (',', 'LABEL_2'),\n",
       " ('and', 'LABEL_0'),\n",
       " ('he', 'LABEL_0'),\n",
       " ('lives', 'LABEL_0'),\n",
       " ('in', 'LABEL_0'),\n",
       " ('california', 'LABEL_0'),\n",
       " ('.', 'LABEL_6'),\n",
       " ('[SEP]', 'LABEL_4')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"Elon Musk is the CEO of Tesla and SpaceX, and he lives in California.\"\n",
    "\n",
    "entities = predict_ner(data, tokenizer, model)\n",
    "entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cic1205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
